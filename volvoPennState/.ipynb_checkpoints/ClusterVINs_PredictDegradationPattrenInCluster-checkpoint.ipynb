{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12db406-1f0b-444e-877d-f25644a77421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.23.0.9-3.el8.x86_64\n",
      "env: PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.23.0.9-3.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin\n"
     ]
    }
   ],
   "source": [
    "%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.23.0.9-3.el8.x86_64\n",
    "%env PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.23.0.9-3.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf050045-88a5-4f60-953e-858fad5224f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark as psk\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import time as t \n",
    "from datetime import date, datetime, timedelta\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import reduce  \n",
    "from math import modf\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from delta import * \n",
    "from delta.tables import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from IPython import get_ipython\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e27ba5a0-9595-41c4-93f3-1cff75c55c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_test_file_path = '/storage/home/yqf5148/work/volvoPennState/data/dataset/aggregated_test_df.csv'\n",
    "aggregated_test_df = pd.read_csv(aggregated_test_file_path)\n",
    "\n",
    "# # Define the feature columns\n",
    "# degradation_feature_columns = list(range(591, 721))  # Columns 591 to 721 as features\n",
    "# X = aggregated_test_df.iloc[:, degradation_feature_columns]\n",
    "# # display(X)\n",
    "# # Print the contents of col[722] in train_data\n",
    "# # column_contents = aggregated_test_df.iloc[:, 720]\n",
    "# # print(column_contents)\n",
    "\n",
    "# Separate features and target\n",
    "clustered_aggregated_train_df = aggregated_train_df\n",
    "\n",
    "# Exclude VIN column (column[0]) for clustering. Column[591] is the last KOLA feature in dataframe. Col[592] to col[719] are calculated features, and col[720] and col[721] are target columns. \n",
    "X = clustered_aggregated_train_df.iloc[:, 1:591]  \n",
    "\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('clusterer', KMeans(n_clusters=optimal_clusters, random_state=42))])  # You can change the number of clusters\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline.fit(X)\n",
    "\n",
    "# Get cluster labels\n",
    "cluster_labels = pipeline.named_steps['clusterer'].labels_\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "clustered_aggregated_test_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Save the dataframe with cluster labels\n",
    "clustered_aggregated_test_df.to_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/clustered_aggregated_test_df.csv', index = None, mode = 'w', header=False)\n",
    "\n",
    "print(\"Clustering completed. The clustered data is saved to 'clustered_aggregated_test_df.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa4a44-afe8-4ed0-ae20-bd7fe8fb274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "clustered_aggregated_train_file_path = '/storage/home/yqf5148/work/volvoPennState/data/dataset/clustered_aggregated_train_df.csv'\n",
    "clustered_aggregated_test_file_path = '/storage/home/yqf5148/work/volvoPennState/data/dataset/clustered_aggregated_test_df.csv'\n",
    "\n",
    "clustered_aggregated_train_df = pd.read_csv(clustered_aggregated_train_file_path)\n",
    "clustered_aggregated_test_df = pd.read_csv(clustered_aggregated_test_file_path)\n",
    "\n",
    "print(\"Clustered aggregated train and test data per VIN:\")\n",
    "print(\"Clustered aggregated train data shape:\", clustered_aggregated_train_df.shape)\n",
    "print(\"Clustered aggregated test data shape:\", clustered_aggregated_test_df.shape)\n",
    "\n",
    "\n",
    "# Print the contents of col[722] in train_data\n",
    "# column_722_contents = clustered_aggregated_train_df.iloc[:, 721]\n",
    "# print(column_722_contents)\n",
    "\n",
    "# Combine train and test data\n",
    "df = pd.concat([clustered_aggregated_train_df, clustered_aggregated_test_df], ignore_index=True)\n",
    "\n",
    "# Define the column ranges\n",
    "first_feature_columns = list(range(591, 717, 2))  # col[591], col[593], ..., col[717]\n",
    "second_feature_columns = list(range(592, 718, 2))  # col[592], col[594], ..., col[718]\n",
    "target_1 = 719\n",
    "target_2 = 720\n",
    "cluster_col = 721\n",
    "\n",
    "# Update the indices to reflect the correct column positions\n",
    "clusters = df.iloc[:, cluster_col].unique()\n",
    "\n",
    "\n",
    "# Extract features for first target from train and test datasets\n",
    "X_train_first = clustered_aggregated_train_df.iloc[:, first_feature_columns]\n",
    "X_test_first = clustered_aggregated_test_df.iloc[:, first_feature_columns]\n",
    "\n",
    "# Extract features for second target from train and test datasets\n",
    "X_train_second = clustered_aggregated_train_df.iloc[:, second_feature_columns]\n",
    "X_test_second = clustered_aggregated_test_df.iloc[:, second_feature_columns]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_first_scaled = scaler.fit_transform(X_train_first)\n",
    "X_test_first_scaled = scaler.transform(X_test_first)\n",
    "X_train_second_scaled = scaler.fit_transform(X_train_second)\n",
    "X_test_second_scaled = scaler.transform(X_test_second)\n",
    "\n",
    "# Determine the optimal number of clusters based on the train dataset\n",
    "kmeans_first = KMeans(n_clusters=len(train_df[cluster_col].unique()), random_state=0)\n",
    "kmeans_second = KMeans(n_clusters=len(train_df[cluster_col].unique()), random_state=0)\n",
    "\n",
    "# Train the clustering models\n",
    "kmeans_first.fit(X_train_first_scaled)\n",
    "kmeans_second.fit(X_train_second_scaled)\n",
    "\n",
    "# Predict clusters for the test dataset\n",
    "predicted_clusters_first = kmeans_first.predict(X_test_first_scaled)\n",
    "predicted_clusters_second = kmeans_second.predict(X_test_second_scaled)\n",
    "\n",
    "# Compare predicted clusters with actual clusters\n",
    "actual_clusters = test_df.iloc[:, cluster_col]\n",
    "\n",
    "comparison_first = pd.DataFrame({\n",
    "    'VIN': test_df['VIN'],\n",
    "    'Actual Cluster': actual_clusters,\n",
    "    'Predicted Cluster First Target': predicted_clusters_first\n",
    "})\n",
    "\n",
    "comparison_second = pd.DataFrame({\n",
    "    'VIN': test_df['VIN'],\n",
    "    'Actual Cluster': actual_clusters,\n",
    "    'Predicted Cluster Second Target': predicted_clusters_second\n",
    "})\n",
    "\n",
    "# Print comparison results\n",
    "print(\"Comparison for First Target:\")\n",
    "print(comparison_first.head(20))  # Display first 20 rows for brevity\n",
    "\n",
    "print(\"\\nComparison for Second Target:\")\n",
    "print(comparison_second.head(20))  # Display first 20 rows for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88511705-aca2-48a6-a3a4-d0219c967564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a715db-bdc0-4558-88c2-037dbbe6479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column ranges for the plots\n",
    "calendar_day_col = 592\n",
    "features_odd = list(range(593, 720, 2))  # Odd indexed columns\n",
    "features_even = list(range(594, 721, 2))  # Even indexed columns\n",
    "target_first_15d = 721\n",
    "target_second_15d = 722\n",
    "\n",
    "# List of unique VINs\n",
    "train_vins = train_data.iloc[:, 0].unique()\n",
    "\n",
    "# Plotting degradation patterns for each VIN\n",
    "for vin in train_vins:\n",
    "    vin_data = train_data[train_data.iloc[:, 0] == vin]\n",
    "    vin_data = vin_data.sort_values(by=vin_data.columns[calendar_day_col])\n",
    "    \n",
    "    # First diagram: Odd indexed features with target\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for feature in features_odd:\n",
    "        plt.plot(vin_data.iloc[:, calendar_day_col], vin_data.iloc[:, feature], label=vin_data.columns[feature])\n",
    "    plt.plot(vin_data.iloc[:, calendar_day_col], vin_data.iloc[:, target_first_15d], label=vin_data.columns[target_first_15d], linewidth=2, color='black')\n",
    "    \n",
    "    plt.title(f'Degradation Pattern for VIN {vin} (Odd Indexed Features with Target)')\n",
    "    plt.xlabel('Calendar Day')\n",
    "    plt.ylabel('Feature Value')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Second diagram: Even indexed features with target\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for feature in features_even:\n",
    "        plt.plot(vin_data.iloc[:, calendar_day_col], vin_data.iloc[:, feature], label=vin_data.columns[feature])\n",
    "    plt.plot(vin_data.iloc[:, calendar_day_col], vin_data.iloc[:, target_second_15d], label=vin_data.columns[target_second_15d], linewidth=2, color='black')\n",
    "    \n",
    "    plt.title(f'Degradation Pattern for VIN {vin} (Even Indexed Features with Target)')\n",
    "    plt.xlabel('Calendar Day')\n",
    "    plt.ylabel('Feature Value')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8427933-eadf-42f8-92cb-1de778036d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f9f9b4-7745-440a-89ff-18c2dda53b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env-for-jupyter5-29",
   "language": "python",
   "name": "pyspark-env-for-jupyter5-29"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
