{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17a01be-2e2c-4fee-a050-6d6cec63df17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ps: /storage/icds/RISE/sw8/anaconda/anaconda3/envs/tensorflow/lib/libuuid.so.1: no version information available (required by /usr/lib64/libblkid.so.1)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/10 00:26:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/10 00:26:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/storage/work/yqf5148/volvoPennState/Jobs/outputs/PopulationWithChassisId.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 56\u001b[0m\n\u001b[1;32m     46\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m     47\u001b[0m        \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[2]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     48\u001b[0m        \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     49\u001b[0m        \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.maxResultSize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20g\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     50\u001b[0m        \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100g\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     51\u001b[0m        \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     53\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset spark.sql.legacy.timeParserPolicy=LEGACY\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \u001b[38;5;66;03m#To resolve the error for p1075_38 to_timestamp formating: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '1/2/2019 20:40:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 56\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPopulationWithChassisId.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     57\u001b[0m           \u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpopulation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m df_population \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/anaconda3/envs/volvopennstate-env/lib/python3.12/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/work/anaconda3/envs/volvopennstate-env/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/work/anaconda3/envs/volvopennstate-env/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/storage/work/yqf5148/volvoPennState/Jobs/outputs/PopulationWithChassisId.csv."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark as psk\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import time as t \n",
    "from datetime import date, datetime, timedelta\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import reduce  \n",
    "from math import modf\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from delta import * \n",
    "from delta.tables import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "conf = (SparkConf().set(\"spark.driver.maxResultSize\", \"4g\"))\n",
    "\n",
    "# Create new context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# sc = SparkContext(\"local\", \"Simple App\")\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "       .master(\"local[2]\") \\\n",
    "       .appName(\"test\") \\\n",
    "       .config(\"spark.driver.maxResultSize\", \"20g\")\\\n",
    "       .config(\"spark.driver.memory\", \"100g\")\\\n",
    "       .getOrCreate()\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")    #To resolve the error for p1075_38 to_timestamp formating: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '1/2/2019 20:40:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/PopulationWithChassisId.csv\") \\\n",
    "          .createOrReplaceTempView(\"population\")\n",
    "\n",
    "df_population = spark.sql(\"SELECT * FROM population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bf7c1-f72a-4d7e-b67f-b5aa8f8a7220",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_features = [    \n",
    "          \"calendar_day\", \n",
    "          \"f_1_dtc38_1th_15d\", \n",
    "          \"f_1_dtc38_2nd_15d\", \n",
    "          \"f_2_dtc38_1th_15d\", \n",
    "          \"f_2_dtc38_2nd_15d\", \n",
    "          \"f_3_dtc38_1th_15d\", \n",
    "          \"f_3_dtc38_2nd_15d\", \n",
    "          \"f_4_dtc38_1th_15d\", \n",
    "          \"f_4_dtc38_2nd_15d\", \n",
    "          \"f_5_dtc38_1th_15d\", \n",
    "          \"f_5_dtc38_2nd_15d\", \n",
    "          \"f_6_dtc38_1th_15d\", \n",
    "          \"f_6_dtc38_2nd_15d\", \n",
    "          \"f_7_dtc38_1th_15d\", \n",
    "          \"f_7_dtc38_2nd_15d\", \n",
    "          \"f_8_dtc38_1th_15d\", \n",
    "          \"f_8_dtc38_2nd_15d\", \n",
    "\n",
    "          \"f_1_dtc75_1th_15d\", \n",
    "          \"f_1_dtc75_2nd_15d\", \n",
    "          \"f_2_dtc75_1th_15d\", \n",
    "          \"f_2_dtc75_2nd_15d\", \n",
    "          \"f_3_dtc75_1th_15d\", \n",
    "          \"f_3_dtc75_2nd_15d\", \n",
    "          \"f_4_dtc75_1th_15d\", \n",
    "          \"f_4_dtc75_2nd_15d\", \n",
    "          \"f_5_dtc75_1th_15d\", \n",
    "          \"f_5_dtc75_2nd_15d\", \n",
    "          \"f_6_dtc75_1th_15d\", \n",
    "          \"f_6_dtc75_2nd_15d\", \n",
    "          \"f_7_dtc75_1th_15d\", \n",
    "          \"f_7_dtc75_2nd_15d\", \n",
    "          \"f_8_dtc75_1th_15d\", \n",
    "          \"f_8_dtc75_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc77_1th_15d\", \n",
    "          \"f_1_dtc77_2nd_15d\", \n",
    "          \"f_2_dtc77_1th_15d\", \n",
    "          \"f_2_dtc77_2nd_15d\", \n",
    "          \"f_3_dtc77_1th_15d\", \n",
    "          \"f_3_dtc77_2nd_15d\", \n",
    "          \"f_4_dtc77_1th_15d\", \n",
    "          \"f_4_dtc77_2nd_15d\", \n",
    "          \"f_5_dtc77_1th_15d\", \n",
    "          \"f_5_dtc77_2nd_15d\", \n",
    "          \"f_6_dtc77_1th_15d\", \n",
    "          \"f_6_dtc77_2nd_15d\", \n",
    "          \"f_7_dtc77_1th_15d\", \n",
    "          \"f_7_dtc77_2nd_15d\", \n",
    "          \"f_8_dtc77_1th_15d\", \n",
    "          \"f_8_dtc77_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc86_1th_15d\", \n",
    "          \"f_1_dtc86_2nd_15d\", \n",
    "          \"f_2_dtc86_1th_15d\", \n",
    "          \"f_2_dtc86_2nd_15d\", \n",
    "          \"f_3_dtc86_1th_15d\", \n",
    "          \"f_3_dtc86_2nd_15d\", \n",
    "          \"f_4_dtc86_1th_15d\", \n",
    "          \"f_4_dtc86_2nd_15d\", \n",
    "          \"f_5_dtc86_1th_15d\", \n",
    "          \"f_5_dtc86_2nd_15d\", \n",
    "          \"f_6_dtc86_1th_15d\", \n",
    "          \"f_6_dtc86_2nd_15d\", \n",
    "          \"f_7_dtc86_1th_15d\", \n",
    "          \"f_7_dtc86_2nd_15d\", \n",
    "          \"f_8_dtc86_1th_15d\", \n",
    "          \"f_8_dtc86_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc92_1th_15d\", \n",
    "          \"f_1_dtc92_2nd_15d\", \n",
    "          \"f_2_dtc92_1th_15d\", \n",
    "          \"f_2_dtc92_2nd_15d\", \n",
    "          \"f_3_dtc92_1th_15d\", \n",
    "          \"f_3_dtc92_2nd_15d\", \n",
    "          \"f_4_dtc92_1th_15d\", \n",
    "          \"f_4_dtc92_2nd_15d\", \n",
    "          \"f_5_dtc92_1th_15d\", \n",
    "          \"f_5_dtc92_2nd_15d\", \n",
    "          \"f_6_dtc92_1th_15d\", \n",
    "          \"f_6_dtc92_2nd_15d\", \n",
    "          \"f_7_dtc92_1th_15d\", \n",
    "          \"f_7_dtc92_2nd_15d\", \n",
    "          \"f_8_dtc92_1th_15d\", \n",
    "          \"f_8_dtc92_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc94_1th_15d\", \n",
    "          \"f_1_dtc94_2nd_15d\", \n",
    "          \"f_2_dtc94_1th_15d\", \n",
    "          \"f_2_dtc94_2nd_15d\", \n",
    "          \"f_3_dtc94_1th_15d\", \n",
    "          \"f_3_dtc94_2nd_15d\", \n",
    "          \"f_4_dtc94_1th_15d\", \n",
    "          \"f_4_dtc94_2nd_15d\", \n",
    "          \"f_5_dtc94_1th_15d\", \n",
    "          \"f_5_dtc94_2nd_15d\", \n",
    "          \"f_6_dtc94_1th_15d\", \n",
    "          \"f_6_dtc94_2nd_15d\", \n",
    "          \"f_7_dtc94_1th_15d\", \n",
    "          \"f_7_dtc94_2nd_15d\", \n",
    "          \"f_8_dtc94_1th_15d\", \n",
    "          \"f_8_dtc94_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc0401_1th_15d\", \n",
    "          \"f_1_dtc0401_2nd_15d\", \n",
    "          \"f_2_dtc0401_1th_15d\", \n",
    "          \"f_2_dtc0401_2nd_15d\", \n",
    "          \"f_3_dtc0401_1th_15d\", \n",
    "          \"f_3_dtc0401_2nd_15d\", \n",
    "          \"f_4_dtc0401_1th_15d\", \n",
    "          \"f_4_dtc0401_2nd_15d\", \n",
    "          \"f_5_dtc0401_1th_15d\", \n",
    "          \"f_5_dtc0401_2nd_15d\", \n",
    "          \"f_6_dtc0401_1th_15d\", \n",
    "          \"f_6_dtc0401_2nd_15d\", \n",
    "          \"f_7_dtc0401_1th_15d\", \n",
    "          \"f_7_dtc0401_2nd_15d\", \n",
    "          \"f_8_dtc0401_1th_15d\", \n",
    "          \"f_8_dtc0401_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc2457_1th_15d\", \n",
    "          \"f_1_dtc2457_2nd_15d\", \n",
    "          \"f_2_dtc2457_1th_15d\", \n",
    "          \"f_2_dtc2457_2nd_15d\", \n",
    "          \"f_3_dtc2457_1th_15d\", \n",
    "          \"f_3_dtc2457_2nd_15d\", \n",
    "          \"f_4_dtc2457_1th_15d\", \n",
    "          \"f_4_dtc2457_2nd_15d\", \n",
    "          \"f_5_dtc2457_1th_15d\", \n",
    "          \"f_5_dtc2457_2nd_15d\", \n",
    "          \"f_6_dtc2457_1th_15d\", \n",
    "          \"f_6_dtc2457_2nd_15d\", \n",
    "          \"f_7_dtc2457_1th_15d\", \n",
    "          \"f_7_dtc2457_2nd_15d\", \n",
    "          \"f_8_dtc2457_1th_15d\", \n",
    "          \"f_8_dtc2457_2nd_15d\",\n",
    "\n",
    "          \"if_parts_replaced_in_1th_15d\", \n",
    "          \"if_parts_replaced_in_2nd_15d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa59e0-9a98-41e3-a35a-3424c627a088",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Normalization Using Z-score Method\n",
    "The z-score method (often called standardization) transforms the info into distribution with a mean of 0 and a typical deviation of 1. \n",
    "Each standardized value is computed by subtracting the mean of the corresponding feature then dividing by the quality deviation.\n",
    "\n",
    "One standard is to include an epsilon variable that prevents divide by zero. \n",
    "Epsilon is an intentional error added to calculations to prevent creating NaN or Inf.\n",
    "Fact about z-score[column] = 0: mSince the standard deviation is calculated by taking \n",
    "the sum of the squared deviations from the mean, a zero standard deviation can only be possible \n",
    "when all the values of a variable are the same (all equal to the mean). \n",
    "In this case, those variables have no discriminative power so they can be removed from the analysis. \n",
    "They cannot improve any classification, clustering or regression task. \n",
    "Many implementations will do it for you or throw an error about a matrix calculation.\n",
    "'''\n",
    "def normalize_numerical_data(numeric_columns):\n",
    "   #first 2 columns, ENGIN_HP and calendar_day are not normalized and neither the latest two columns (target calumns), parts_are_replaced columns\n",
    "    number_of_numeric_features = len(numeric_columns.columns)-2\n",
    "    epsilon=1e-100\n",
    "    for column in numeric_columns.iloc[:,2:number_of_numeric_features].columns:\n",
    "        numeric_columns[column] = (numeric_columns[column] -\n",
    "                               numeric_columns[column].mean()) / (numeric_columns[column].std()+epsilon)\n",
    "\n",
    "    return numeric_columns\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "This converts a bytes string to a float between 0.0 and 1.0. If we are using a unicode string (e.g., in python 3), then we need to encode it. \n",
    "This should give the same result on any machine and any version of python (tested on python 2.7 and 3.5).\n",
    "\n",
    "Note: the & 0xffffffff is here to guarantee an unsigned int result. \n",
    "It is needed because depending on the python version crc32(b) may return a signed or unsigned int.:\n",
    "'''\n",
    "def bytes_to_float(s):\n",
    "    return float(crc32(b) & 0xffffffff) / 2**32\n",
    "\n",
    "\n",
    "def str_to_float(s, encoding=\"utf-8\"):\n",
    "    return bytes_to_float(s.astype('str').encode(encoding))\n",
    "\n",
    "\n",
    "convert_str_to_float_udf = udf(str_to_float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e0425-5df8-465f-a86f-b5d686578c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_from_population = ['VIN','ENGINE_SIZE','ENGINE_HP','VEH_TYPE']+[s for s in df_population.columns if 'KOLA' in s]\n",
    "all_columns_names = selected_features_from_population + calculated_features\n",
    "resultedData = pd.read_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/resultedData.csv', sep=',', names=all_columns_names, header=None, error_bad_lines=False, index_col=False, dtype='unicode')\n",
    "# display(resultedData.loc[resultedData['calendar_day'].astype('int32')==375].iloc[: , 1:5])\n",
    "display(resultedData.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b756407-e22e-42ca-a526-18dbb0fae4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf08c1-1520-4cde-b005-e048074444c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env-for-jupyter",
   "language": "python",
   "name": "pyspark-env-for-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
