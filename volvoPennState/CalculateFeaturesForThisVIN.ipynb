{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d27272c-55f1-48ed-b3bd-445ce73244d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
      "env: PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin\n"
     ]
    }
   ],
   "source": [
    "%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
    "%env PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "302060b9-b23b-4dab-8fe9-4d92a71d0ecb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ps: /storage/icds/RISE/sw8/anaconda/anaconda3/envs/tensorflow/lib/libuuid.so.1: no version information available (required by /usr/lib64/libblkid.so.1)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/19 23:23:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/19 23:23:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/19 23:23:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/11/19 23:23:24 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/11/19 23:23:24 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/11/19 23:23:24 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark as psk\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import time as t \n",
    "from datetime import date, datetime, timedelta\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import reduce  \n",
    "from math import modf\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from delta import * \n",
    "from delta.tables import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "conf = (SparkConf().set(\"spark.driver.maxResultSize\", \"4g\"))\n",
    "\n",
    "# Create new context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# sc = SparkContext(\"local\", \"Simple App\")\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "       .master(\"local[2]\") \\\n",
    "       .appName(\"test\") \\\n",
    "       .config(\"spark.driver.maxResultSize\", \"20g\")\\\n",
    "       .config(\"spark.driver.memory\", \"100g\")\\\n",
    "       .getOrCreate()\n",
    "\n",
    "#both works\n",
    "# 1: \n",
    "# spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")    #To resolve the error for p1075_38 to_timestamp formating: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '1/2/2019 20:40:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n",
    "# Set Spark SQL legacy time parser policy to LEGACY to handle older date formats\n",
    "# 2:\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "# Increase the max fields in the string representation of a plan\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)  # Increase to 1000 or more as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf20759e-f530-4958-ae73-6e7847f443be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def raw_data_cleaning(dtc_type):\n",
    "    df = spark.sql(\"SELECT * FROM {} WHERE _C0 IS NOT NULL AND _C0 != 'NA' AND _C0 != '' AND MESSAGE_ID IS NOT NULL AND MESSAGE_ID != '' AND MESSAGE_ID != 'NA' AND FAULT_DATE_TIME IS NOT NULL AND FAULT_DATE_TIME != '' AND FAULT_DATE_TIME != 'NA' AND FAULT_STATUS IS NOT NULL AND FAULT_STATUS != '' AND FAULT_STATUS != 'NA' AND VIN IS NOT NULL AND VIN != '' AND VIN != 'NA' AND length(VIN) !< 17\".format(dtc_type)) \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d57ba81-4151-4ad1-8951-9bbfe610f092",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 23:23:29 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file into table\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv\") \\\n",
    "          .createOrReplaceTempView(\"VINs_data\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/PopulationWithChassisId.csv\") \\\n",
    "          .createOrReplaceTempView(\"population\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/CCA Claims.csv\") \\\n",
    "          .createOrReplaceTempView(\"cca_claims\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/EGR Cooler Claims.csv\") \\\n",
    "          .createOrReplaceTempView(\"egr_cooler_claims\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/EGR FG 293 Claims.csv\") \\\n",
    "          .createOrReplaceTempView(\"egr_fg_293_claims\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/EGR Sensors.csv\") \\\n",
    "          .createOrReplaceTempView(\"egr_sensors\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/Engine_Emissions_Table1.csv\") \\\n",
    "          .createOrReplaceTempView(\"engine_emissions_table1\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/Engine_Emissions_Table2.csv\") \\\n",
    "          .createOrReplaceTempView(\"engine_emissions_table2\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_38.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_38\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_75.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_75\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_77.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_77\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_86.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_86\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_92.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_92\")\n",
    " \n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_94.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_94\")   #Adding .option(\"skipRows\", range(2784329,2784830)) does not work for skipping rows for pyspark dataframe\n",
    "\n",
    "# dataFrame = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"skipRows\", range(2784329,2784830)) \\\n",
    "#           .load(\"P1075_94.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = pd.read_csv('P1075_94.csv',skiprows=list(range(2784329,2784830)),low_memory=False)   list[range(2784329,2784830)]  works to skip row for pandas dataframe\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P0401_faults.csv\") \\\n",
    "          .createOrReplaceTempView(\"p0401_faults\")\n",
    "\n",
    "spark.read.option(\"header\",True,) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P2457_faults.csv\") \\\n",
    "          .createOrReplaceTempView(\"p2457_faults\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e0fae1-0016-4b39-a8c2-2ddc41d1a8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_population = spark.sql(\"SELECT * FROM population\")\n",
    "\n",
    "df_p1075_38 = raw_data_cleaning('p1075_38')\n",
    "\n",
    "df_p1075_75 = raw_data_cleaning('p1075_75')\n",
    "\n",
    "df_p1075_77 = raw_data_cleaning('p1075_77')\n",
    "\n",
    "df_p1075_86 = raw_data_cleaning('p1075_86')\n",
    "\n",
    "df_p1075_92 = raw_data_cleaning('p1075_92')\n",
    "\n",
    "df_p1075_94 = raw_data_cleaning('p1075_94')\n",
    "\n",
    "df_p0401 = raw_data_cleaning('p0401_faults')\n",
    "\n",
    "df_p2457 = raw_data_cleaning('p2457_faults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f1b2a1a-509d-4d3e-9c39-89e306d8961f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df38 = spark.read.option(\"header\",True).csv(\"P1075_38.csv\")\n",
    "# df75 = spark.read.option(\"header\",True).csv(\"P1075_75.csv\")\n",
    "# df77 = spark.read.option(\"header\",True).csv(\"P1075_77.csv\")\n",
    "# df86 = spark.read.option(\"header\",True).csv(\"P1075_86.csv\")\n",
    "# df92 = spark.read.option(\"header\",True).csv(\"P1075_92.csv\")\n",
    "# df94 = spark.read.option(\"header\",True).csv(\"P1075_94.csv\")\n",
    "# df0401 = spark.read.option(\"header\",True).csv(\"P0401_faults.csv\")\n",
    "# df2457 = spark.read.option(\"header\",True).csv(\"P2457_faults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ffc862c-a44d-4d09-894b-47a2f4e23936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dtc_type=\"p1075_38\"    \n",
    "# df38.select(\"VIN\", f.to_timestamp(f.concat(lit(\"1/1/2019 20:40\"), lit(\":00\")), \"M/d/yyyy HH:mm:ss\").alias(\"FAULT_DATE_TIME\"), \"FAULT_STATUS\").show()\n",
    "# df38.select(\"VIN\", f.to_timestamp(f.concat(df38.FAULT_DATE_TIME, lit(\":00\")), \"M/d/yyyy HH:mm:ss\").alias(\"FAULT_DATE_TIME\"), \"FAULT_STATUS\").show()\n",
    "# df38.select(\"VIN\", (fix_problem_of_fault_date_time_with_no_seconds(df38, dtc_type)).alias(\"FAULT_DATE_TIME_2\"), \"FAULT_STATUS\").show()\n",
    "# df38.select(\"VIN\", f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), \"M/d/yyyy HH:mm:ss\").alias(\"FAULT_DATE_TIME\"), \"FAULT_STATUS\").show()\n",
    "# df75.select(\"VIN\", f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), \"yyyy-MM-dd HH:mm:ss\"), \"FAULT_STATUS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ac588d3-2dac-4904-afa2-fd8070a23426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headerList = [\"VIN\",    \n",
    "          \"calendar_day\", \n",
    "          \"f_1_dtc38_1th_15d\", \n",
    "          \"f_1_dtc38_2nd_15d\", \n",
    "          \"f_2_dtc38_1th_15d\", \n",
    "          \"f_2_dtc38_2nd_15d\", \n",
    "          \"f_3_dtc38_1th_15d\", \n",
    "          \"f_3_dtc38_2nd_15d\", \n",
    "          \"f_4_dtc38_1th_15d\", \n",
    "          \"f_4_dtc38_2nd_15d\", \n",
    "          \"f_5_dtc38_1th_15d\", \n",
    "          \"f_5_dtc38_2nd_15d\", \n",
    "          \"f_6_dtc38_1th_15d\", \n",
    "          \"f_6_dtc38_2nd_15d\", \n",
    "          \"f_7_dtc38_1th_15d\", \n",
    "          \"f_7_dtc38_2nd_15d\", \n",
    "          \"f_8_dtc38_1th_15d\", \n",
    "          \"f_8_dtc38_2nd_15d\", \n",
    "\n",
    "          \"f_1_dtc75_1th_15d\", \n",
    "          \"f_1_dtc75_2nd_15d\", \n",
    "          \"f_2_dtc75_1th_15d\", \n",
    "          \"f_2_dtc75_2nd_15d\", \n",
    "          \"f_3_dtc75_1th_15d\", \n",
    "          \"f_3_dtc75_2nd_15d\", \n",
    "          \"f_4_dtc75_1th_15d\", \n",
    "          \"f_4_dtc75_2nd_15d\", \n",
    "          \"f_5_dtc75_1th_15d\", \n",
    "          \"f_5_dtc75_2nd_15d\", \n",
    "          \"f_6_dtc75_1th_15d\", \n",
    "          \"f_6_dtc75_2nd_15d\", \n",
    "          \"f_7_dtc75_1th_15d\", \n",
    "          \"f_7_dtc75_2nd_15d\", \n",
    "          \"f_8_dtc75_1th_15d\", \n",
    "          \"f_8_dtc75_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc77_1th_15d\", \n",
    "          \"f_1_dtc77_2nd_15d\", \n",
    "          \"f_2_dtc77_1th_15d\", \n",
    "          \"f_2_dtc77_2nd_15d\", \n",
    "          \"f_3_dtc77_1th_15d\", \n",
    "          \"f_3_dtc77_2nd_15d\", \n",
    "          \"f_4_dtc77_1th_15d\", \n",
    "          \"f_4_dtc77_2nd_15d\", \n",
    "          \"f_5_dtc77_1th_15d\", \n",
    "          \"f_5_dtc77_2nd_15d\", \n",
    "          \"f_6_dtc77_1th_15d\", \n",
    "          \"f_6_dtc77_2nd_15d\", \n",
    "          \"f_7_dtc77_1th_15d\", \n",
    "          \"f_7_dtc77_2nd_15d\", \n",
    "          \"f_8_dtc77_1th_15d\", \n",
    "          \"f_8_dtc77_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc86_1th_15d\", \n",
    "          \"f_1_dtc86_2nd_15d\", \n",
    "          \"f_2_dtc86_1th_15d\", \n",
    "          \"f_2_dtc86_2nd_15d\", \n",
    "          \"f_3_dtc86_1th_15d\", \n",
    "          \"f_3_dtc86_2nd_15d\", \n",
    "          \"f_4_dtc86_1th_15d\", \n",
    "          \"f_4_dtc86_2nd_15d\", \n",
    "          \"f_5_dtc86_1th_15d\", \n",
    "          \"f_5_dtc86_2nd_15d\", \n",
    "          \"f_6_dtc86_1th_15d\", \n",
    "          \"f_6_dtc86_2nd_15d\", \n",
    "          \"f_7_dtc86_1th_15d\", \n",
    "          \"f_7_dtc86_2nd_15d\", \n",
    "          \"f_8_dtc86_1th_15d\", \n",
    "          \"f_8_dtc86_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc92_1th_15d\", \n",
    "          \"f_1_dtc92_2nd_15d\", \n",
    "          \"f_2_dtc92_1th_15d\", \n",
    "          \"f_2_dtc92_2nd_15d\", \n",
    "          \"f_3_dtc92_1th_15d\", \n",
    "          \"f_3_dtc92_2nd_15d\", \n",
    "          \"f_4_dtc92_1th_15d\", \n",
    "          \"f_4_dtc92_2nd_15d\", \n",
    "          \"f_5_dtc92_1th_15d\", \n",
    "          \"f_5_dtc92_2nd_15d\", \n",
    "          \"f_6_dtc92_1th_15d\", \n",
    "          \"f_6_dtc92_2nd_15d\", \n",
    "          \"f_7_dtc92_1th_15d\", \n",
    "          \"f_7_dtc92_2nd_15d\", \n",
    "          \"f_8_dtc92_1th_15d\", \n",
    "          \"f_8_dtc92_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc94_1th_15d\", \n",
    "          \"f_1_dtc94_2nd_15d\", \n",
    "          \"f_2_dtc94_1th_15d\", \n",
    "          \"f_2_dtc94_2nd_15d\", \n",
    "          \"f_3_dtc94_1th_15d\", \n",
    "          \"f_3_dtc94_2nd_15d\", \n",
    "          \"f_4_dtc94_1th_15d\", \n",
    "          \"f_4_dtc94_2nd_15d\", \n",
    "          \"f_5_dtc94_1th_15d\", \n",
    "          \"f_5_dtc94_2nd_15d\", \n",
    "          \"f_6_dtc94_1th_15d\", \n",
    "          \"f_6_dtc94_2nd_15d\", \n",
    "          \"f_7_dtc94_1th_15d\", \n",
    "          \"f_7_dtc94_2nd_15d\", \n",
    "          \"f_8_dtc94_1th_15d\", \n",
    "          \"f_8_dtc94_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc0401_1th_15d\", \n",
    "          \"f_1_dtc0401_2nd_15d\", \n",
    "          \"f_2_dtc0401_1th_15d\", \n",
    "          \"f_2_dtc0401_2nd_15d\", \n",
    "          \"f_3_dtc0401_1th_15d\", \n",
    "          \"f_3_dtc0401_2nd_15d\", \n",
    "          \"f_4_dtc0401_1th_15d\", \n",
    "          \"f_4_dtc0401_2nd_15d\", \n",
    "          \"f_5_dtc0401_1th_15d\", \n",
    "          \"f_5_dtc0401_2nd_15d\", \n",
    "          \"f_6_dtc0401_1th_15d\", \n",
    "          \"f_6_dtc0401_2nd_15d\", \n",
    "          \"f_7_dtc0401_1th_15d\", \n",
    "          \"f_7_dtc0401_2nd_15d\", \n",
    "          \"f_8_dtc0401_1th_15d\", \n",
    "          \"f_8_dtc0401_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc2457_1th_15d\", \n",
    "          \"f_1_dtc2457_2nd_15d\", \n",
    "          \"f_2_dtc2457_1th_15d\", \n",
    "          \"f_2_dtc2457_2nd_15d\", \n",
    "          \"f_3_dtc2457_1th_15d\", \n",
    "          \"f_3_dtc2457_2nd_15d\", \n",
    "          \"f_4_dtc2457_1th_15d\", \n",
    "          \"f_4_dtc2457_2nd_15d\", \n",
    "          \"f_5_dtc2457_1th_15d\", \n",
    "          \"f_5_dtc2457_2nd_15d\", \n",
    "          \"f_6_dtc2457_1th_15d\", \n",
    "          \"f_6_dtc2457_2nd_15d\", \n",
    "          \"f_7_dtc2457_1th_15d\", \n",
    "          \"f_7_dtc2457_2nd_15d\", \n",
    "          \"f_8_dtc2457_1th_15d\", \n",
    "          \"f_8_dtc2457_2nd_15d\",\n",
    "\n",
    "          \"if_parts_replaced_in_1th_15d\", \n",
    "          \"if_parts_replaced_in_2nd_15d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75cf5e13-2336-48ce-ad2b-a04c0f987935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_problem_of_fault_date_time_with_no_seconds(df, dtc_type):\n",
    "    if dtc_type==\"p1075_38\":\n",
    "       return f.to_timestamp(f.concat(df.FAULT_DATE_TIME, lit(\":00\")), \"M/d/yyyy HH:mm:ss\")\n",
    "    else:\n",
    "       return f.to_timestamp(df.FAULT_DATE_TIME, \"yyyy-MM-dd HH:mm:ss\")\n",
    "\n",
    "\n",
    "def feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN, dtc_status_to_calculate_this_feature):\n",
    "      \n",
    "    df_dtc_type_and_status_for_this_VIN = df_dtc_type_for_this_VIN.filter(f.col('FAULT_STATUS') == dtc_status_to_calculate_this_feature)\n",
    "\n",
    "    return df_dtc_type_and_status_for_this_VIN.count()\n",
    "\n",
    "\n",
    "def feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN, dtc_type, dtc_status_to_calculate_this_feature):\n",
    "\n",
    "    fault_date_time_format='yyyy-MM-dd HH:mm:ss'\n",
    "    if dtc_type==\"p1075_38\":\n",
    "        fault_date_time_format='M/d/yyyy HH:mm'\n",
    "\n",
    "    dtcs_with_all_statuses_df = df_dtc_type_for_this_VIN.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format))\n",
    "\n",
    "\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df\n",
    "\n",
    "#     PsSpark: This one is also true to access the first row of pyspark dataframe    \n",
    "#     headRowId = dtcs_with_all_statuses_df2.first()[0]\n",
    "\n",
    "    tailRowId = dtcs_with_all_statuses_df.tail(1)[0][0]\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.filter(f.col('_c0') != tailRowId).fillna(0)\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format)).withColumn(\"_c0\",monotonically_increasing_id())\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.dropna()\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.withColumnRenamed(\"FAULT_STATUS\", \"FAULT_STATUS_CURRENT_DTC\")\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.withColumnRenamed(\"FAULT_DATE_TIME\", \"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\")\n",
    "\n",
    "    headRowId = dtcs_with_all_statuses_df2.head(1)[0][0]\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.filter(f.col('_c0') != headRowId).fillna(0)\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format)).withColumn(\"_c0\",monotonically_increasing_id())\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.dropna()\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.withColumnRenamed(\"FAULT_STATUS\", \"FAULT_ANY_STATUS_NEXT_DTC\")\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.withColumnRenamed(\"FAULT_DATE_TIME\", \"FAULT_DATE_TIME_NEXT_ANY_DTC\")\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.select([c for c in dtcs_with_all_statuses_df2.columns if c != \"VIN\"])\n",
    "\n",
    "    # create list of dataframes\n",
    "    list_df = [dtcs_with_all_statuses_df, dtcs_with_all_statuses_df2]\n",
    "\n",
    "    # merge all at once\n",
    "    my_temp_df = reduce(lambda x, y: x.join(y, on=\"_c0\"), list_df)\n",
    "    my_temp_df = my_temp_df.withColumn(\"FAULT_DATE_TIME_DIFF\", (f.unix_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), format=fault_date_time_format) - f.unix_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), format=fault_date_time_format))) \\\n",
    "    .withColumn(\"DURATION_IN_THIS_15_DAYS_BACK\", f.when((f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format) <= end_date) & \n",
    "                                                                  (f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format) >= start_date), \n",
    "                                                                  (f.unix_timestamp(\"FAULT_DATE_TIME_NEXT_ANY_DTC\", format=fault_date_time_format) - f.unix_timestamp(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\", format=fault_date_time_format)))) \\\n",
    "    .withColumn(\"COUNT_IN_THIS_15_DAYS_BACK\", f.when((f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format) <= end_date) & \n",
    "                                                                  (f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format) >= start_date), (f.lit(1)).cast(IntegerType())))\n",
    "    start_only_with_this_status_dtcs_df = my_temp_df.filter(f.col(\"FAULT_STATUS_CURRENT_DTC\") == f.lit(dtc_status_to_calculate_this_feature))\n",
    "\n",
    "\n",
    "    duration_of_specific_type_dtcs_for_this_VIN = start_only_with_this_status_dtcs_df.select(f.sum(\"DURATION_IN_THIS_15_DAYS_BACK\")).fillna(0)\n",
    "    count_of_specific_type_dtcs_for_this_VIN = start_only_with_this_status_dtcs_df.select(f.sum(\"COUNT_IN_THIS_15_DAYS_BACK\")).fillna(1)\n",
    "    duration = duration_of_specific_type_dtcs_for_this_VIN.collect()[0][0]\n",
    "    return duration\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "def feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN, dtc_type, dtc_status_to_calculate_this_feature):\n",
    "    \n",
    "    fault_date_time_format='yyyy-MM-dd HH:mm:ss'\n",
    "    if dtc_type==\"p1075_38\":\n",
    "        fault_date_time_format='M/d/yyyy HH:mm'\n",
    "\n",
    "\n",
    "    dtcs_with_all_statuses_df = df_dtc_type_for_this_VIN.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format))\n",
    "\n",
    "\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df\n",
    "\n",
    "#     PsSpark: This one is also true to access the first row of pyspark dataframe    \n",
    "    tailRowId = dtcs_with_all_statuses_df.tail(1)[0][0]\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.filter(f.col('_c0') != tailRowId).fillna(0)\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format)).withColumn(\"_c0\",monotonically_increasing_id())\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.dropna()\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.withColumnRenamed(\"FAULT_STATUS\", \"FAULT_STATUS_CURRENT_DTC\")\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.withColumnRenamed(\"FAULT_DATE_TIME\", \"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\")\n",
    "\n",
    "    headRowId = dtcs_with_all_statuses_df2.head(1)[0][0]\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.filter(f.col('_c0') != headRowId).fillna(0)\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format)).withColumn(\"_c0\",monotonically_increasing_id())\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.dropna()\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.withColumnRenamed(\"FAULT_STATUS\", \"FAULT_ANY_STATUS_NEXT_DTC\")\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.withColumnRenamed(\"FAULT_DATE_TIME\", \"FAULT_DATE_TIME_NEXT_ANY_DTC\")\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.select([c for c in dtcs_with_all_statuses_df2.columns if c != \"VIN\"])\n",
    "\n",
    "\n",
    "    # create list of dataframes\n",
    "    list_df = [dtcs_with_all_statuses_df, dtcs_with_all_statuses_df2]\n",
    "\n",
    "    # merge all at once\n",
    "    my_temp_df = reduce(lambda x, y: x.join(y, on=\"_c0\"), list_df)\n",
    "    my_temp_df = my_temp_df.withColumn(\"FAULT_DATE_TIME_DIFF\", (f.unix_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), format=fault_date_time_format) - f.unix_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), format=fault_date_time_format))) \\\n",
    "    .withColumn(\"DURATION_IN_THIS_15_DAYS_BACK\", f.when((f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format) <= end_date) & \n",
    "                                                                  (f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format) >= start_date), \n",
    "                                                                  (f.unix_timestamp(\"FAULT_DATE_TIME_NEXT_ANY_DTC\", format=fault_date_time_format) - f.unix_timestamp(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\", format=fault_date_time_format)))) \\\n",
    "    .withColumn(\"COUNT_IN_THIS_15_DAYS_BACK\", f.when((f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format) <= end_date) & \n",
    "                                                                  (f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format) >= start_date), (f.lit(1)).cast(IntegerType())))\n",
    "#     my_temp_df[\"_c0\", \"VIN\",\"FAULT_STATUS_CURRENT_DTC\", \"FAULT_ANY_STATUS_NEXT_DTC\", \"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\", \"FAULT_DATE_TIME_NEXT_ANY_DTC\", f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format), f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format)].show(194)\n",
    "    start_only_with_this_status_dtcs_df = my_temp_df.filter(f.col(\"FAULT_STATUS_CURRENT_DTC\") == f.lit(dtc_status_to_calculate_this_feature))\n",
    "\n",
    "\n",
    "\n",
    "    duration_of_specific_type_dtcs_for_this_VIN = start_only_with_this_status_dtcs_df.select(f.sum(\"DURATION_IN_THIS_15_DAYS_BACK\")).fillna(0)\n",
    "    count_of_specific_type_dtcs_for_this_VIN = start_only_with_this_status_dtcs_df.select(f.sum(\"COUNT_IN_THIS_15_DAYS_BACK\")).fillna(1)\n",
    "    duration = duration_of_specific_type_dtcs_for_this_VIN.collect()[0][0]\n",
    "\n",
    "    total_count = count_of_specific_type_dtcs_for_this_VIN.collect()[0][0]\n",
    "    if(total_count != 0): \n",
    "        average_of_specific_type_dtcs_for_this_VIN = duration/total_count\n",
    "        return average_of_specific_type_dtcs_for_this_VIN\n",
    "    else:\n",
    "        return 0.0\n",
    "  \n",
    "  \n",
    "def feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN, vehicle_speed, dtc_status_to_calculate_this_feature):\n",
    "    \n",
    "    df_dtc_type_and_status_for_this_VIN = df_dtc_type_for_this_VIN\\\n",
    "                .filter(df_dtc_type_for_this_VIN.FAULT_STATUS == dtc_status_to_calculate_this_feature)\\\n",
    "                .filter(df_dtc_type_for_this_VIN.ROAD_SPEED_MPH == vehicle_speed)\n",
    "             \n",
    "    return df_dtc_type_and_status_for_this_VIN.count()\n",
    "\n",
    "\n",
    "\n",
    "def if_part_is_replaced_for_this_VIN_in_this_timespan(thisVIN, start_date, end_date):\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    replacement_records = []\n",
    "\n",
    "    fault_date_time_format = 'MM/dd/yyyy'\n",
    "    start_date = '2014-12-31'\n",
    "    end_date = '2021-12-31'\n",
    "\n",
    "    # Load all claims tables for the specific VIN\n",
    "    claims_datasets = {\n",
    "        'cca_claims': f\"SELECT VIN, CLAIM_REG_DATE, TOT_CLAIM_PAYMENT_USD FROM cca_claims WHERE VIN = '{thisVIN}'\",\n",
    "        'egr_cooler_claims': f\"SELECT VIN, CLAIM_REG_DATE, TOT_CLAIM_PAYMENT_USD FROM egr_cooler_claims WHERE VIN = '{thisVIN}'\",\n",
    "        'egr_fg_293_claims': f\"SELECT VIN, CLAIM_REG_DATE, TOT_CLAIM_PAYMENT_USD FROM egr_fg_293_claims WHERE VIN = '{thisVIN}'\",\n",
    "        'egr_sensors_claims': f\"SELECT VIN, CLAIM_REG_DATE, TOT_CLAIM_PAYMENT_USD FROM egr_sensors WHERE VIN = '{thisVIN}'\"\n",
    "    }\n",
    "\n",
    "    # Define filtering condition with corrected date format\n",
    "    for dataset_name, query in claims_datasets.items():\n",
    "        try:\n",
    "            df_claims = spark.sql(query)\n",
    "\n",
    "            if df_claims is not None and df_claims.count() > 0:\n",
    "\n",
    "                # df_claims.printSchema()  # Debugging step\n",
    "                # df_claims.show(5, truncate=False)  # Show sample data\n",
    "\n",
    "                df_claims = df_claims.withColumn(\"CLAIM_REG_DATE\", f.to_date(f.col(\"CLAIM_REG_DATE\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "                # Try alternative filtering\n",
    "                df_filtered = df_claims.filter(\n",
    "                    (f.col('CLAIM_REG_DATE') >= f.to_date(f.lit(start_date), \"yyyy-MM-dd\")) &\n",
    "                    (f.col('CLAIM_REG_DATE') <= f.to_date(f.lit(end_date), \"yyyy-MM-dd\")) &\n",
    "                    (f.col('TOT_CLAIM_PAYMENT_USD') > 1000.0)\n",
    "                )\n",
    "\n",
    "                # df_filtered.show(5, truncate=False)  # Show filtered data\n",
    "\n",
    "                if df_filtered is not None and df_filtered.count() > 0:\n",
    "                    for row in df_filtered.collect():\n",
    "                        replacement_records.append([thisVIN, dataset_name, row['CLAIM_REG_DATE'], row['TOT_CLAIM_PAYMENT_USD']])\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            print(f\"Error processing dataset {dataset_name} for VIN {thisVIN}: {e}\")\n",
    "\n",
    "\n",
    "        if replacement_records:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "# Normalize using Min/Max Normalization.\n",
    "def normalize_numeric_feature_values(statement_in_feature_name, df):\n",
    "\n",
    "  selected_col_names_list = [col for col in df.columns.values if statement_in_feature_name in col]   # selects names of columns that contain specific string\n",
    "\n",
    "  selected_cols = df[selected_col_names_list]\n",
    "  for col_name in selected_col_names_list:\n",
    "      selected_cols[col_name] = selected_cols[col_name].str.replace(\",\",\".\")\n",
    "      selected_cols[col_name] = selected_cols[col_name].apply(lambda x: float(x.split()[0]))\n",
    "\n",
    "  selected_cols_norm = selected_cols.apply(lambda iterator: ((iterator - iterator.mean())/(iterator.max() - iterator.min())).round(3))\n",
    "\n",
    "  df[selected_cols_norm.columns] = selected_cols_norm\n",
    "  return df\n",
    "\n",
    "            \n",
    "def move_over_calendar_and_compute_features(df_selected_features_from_population_for_this_VIN, thisVIN, new_15day_end_date, span_length, dayCount, jobID):\n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "    file.writelines([\"A new day move on calendar: thisVIN={}, new_15day_end_date={}, span_length={}, dayCount={} \\n\".format(thisVIN, new_15day_end_date, span_length, dayCount)])\n",
    "    file.close()\n",
    "    print(\"A new day move on calendar: thisVIN={}, new_15day_end_date={}, span_length={}, dayCount={} \\n\".format(thisVIN, new_15day_end_date, span_length, dayCount)) \n",
    "\n",
    "#     schema = StructType([])   ***** This does not work. Creating EmptyRDD does not allow to add further columns later using withColumn  ****\n",
    "#     df_features_for_this_VIN_and_this_dayCount = sqlContext.createDataFrame(sc.emptyRDD(), schema)\n",
    "#     df_features_for_this_VIN_and_this_dayCount = df_features_for_this_VIN_and_this_dayCount.withColumn(\"VIN\", f.lit(thisVIN)).withColumn(\"calendar_day\", (f.lit(dayCount)).cast(IntegerType()))\n",
    "\n",
    "    \n",
    "    schema = StructType([StructField('VIN', StringType(), True),\n",
    "                      StructField('calendar_day', IntegerType(), True)])\n",
    "    data = [\n",
    "        (thisVIN, dayCount)\n",
    "      ]\n",
    "    df_calculated_features_for_this_VIN_and_this_dayCount = spark.createDataFrame(data = data, schema = schema)\n",
    "\n",
    "    new_15day_start_date = new_15day_end_date - timedelta(days = dayCount)\n",
    "\n",
    "    previous_15day_duration_end_date = new_15day_start_date\n",
    "    previous_15day_duration_start_date = previous_15day_duration_end_date - timedelta(days = dayCount)\n",
    "\n",
    "\n",
    "\n",
    "    list_of_dtc_type = ['p1075_38', 'p1075_75', 'p1075_77', 'p1075_86', 'p1075_92', 'p1075_94', 'p0401_faults', 'p2457_faults']\n",
    "    list_of_dtc_df = [df_p1075_38, df_p1075_75, df_p1075_77, df_p1075_86, df_p1075_92, df_p1075_94, df_p0401, df_p2457]\n",
    "    \n",
    "    for i in range(len(list_of_dtc_df)):\n",
    "        dtc_type_df = list_of_dtc_df[i]\n",
    "        \n",
    "        \n",
    "        #filter dtc-type database for thisVIN and then check if there is any dtc's related to this VIN in the first 15 days timespan before \"new_15day_end_date\" and another 15 days before this period.\n",
    "        df_dtc_type_for_this_VIN = dtc_type_df.filter(f.col('VIN')==thisVIN)\n",
    "        \n",
    "        df_dtc_type_for_this_VIN_in_first_15day_timespan = df_dtc_type_for_this_VIN.filter((fix_problem_of_fault_date_time_with_no_seconds(df_dtc_type_for_this_VIN, list_of_dtc_type[i]) > new_15day_start_date) & (fix_problem_of_fault_date_time_with_no_seconds(df_dtc_type_for_this_VIN, list_of_dtc_type[i]) < new_15day_end_date))\n",
    "        df_dtc_type_for_this_VIN_in_previous_15day_timespan = df_dtc_type_for_this_VIN.filter((fix_problem_of_fault_date_time_with_no_seconds(df_dtc_type_for_this_VIN, list_of_dtc_type[i]) > previous_15day_duration_start_date) & (fix_problem_of_fault_date_time_with_no_seconds(df_dtc_type_for_this_VIN, list_of_dtc_type[i]) < previous_15day_duration_end_date))\n",
    "        \n",
    "        count1 = df_dtc_type_for_this_VIN_in_first_15day_timespan.count()\n",
    "        count2 = df_dtc_type_for_this_VIN_in_previous_15day_timespan.count()\n",
    "        '''\n",
    "        Feature 1\n",
    "        \n",
    "        Definition: number of alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines([\"feature 1 for {} in day {} and dtc_type {} \\n\".format(thisVIN, dayCount, list_of_dtc_type[i])])\n",
    "        file.close()\n",
    "        print(\"feature 1 for {} in day {} and dtc_type {} \\n\".format(thisVIN, dayCount, list_of_dtc_type[i]))\n",
    "        \n",
    "        if count1 > 0:\n",
    "            feature_1_value_first_15days = feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, \"Y\")\n",
    "        else:\n",
    "            feature_1_value_first_15days = 0\n",
    "\n",
    "        \n",
    "        if count2 > 0:\n",
    "            feature_1_value_second_15days = feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, \"Y\")\n",
    "        else:\n",
    "            feature_1_value_second_15days = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        '''\n",
    "        Feature 2\n",
    "                \n",
    "        Definition: number of intermittent alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines([\"feature 2 for {} in day {} \\n\".format(thisVIN, dayCount)])\n",
    "        file.close()\n",
    "        print(\"feature 2 for {} in day {} \\n\".format(thisVIN, dayCount))\n",
    "\n",
    "        if count1 > 0:\n",
    "            feature_2_value_first_15days = feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, \"I\")\n",
    "        else:\n",
    "            feature_2_value_first_15days = 0\n",
    "            \n",
    "        if count2 > 0:           \n",
    "            feature_2_value_second_15days = feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, \"I\")\n",
    "        else:\n",
    "            feature_2_value_second_15days = 0\n",
    "            \n",
    "            \n",
    "\n",
    "        '''\n",
    "        Feature 3\n",
    "        \n",
    "        Definition: duration of active alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines([\"feature 3 for {} in day {} \\n\".format(thisVIN, dayCount)])\n",
    "        file.close()\n",
    "        print(\"feature 3 for {} in day {} \\n\".format(thisVIN, dayCount))\n",
    "        \n",
    "        if count1 > 0:\n",
    "            feature_3_value_first_15days = feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, list_of_dtc_type[i], \"Y\")\n",
    "        else:\n",
    "            feature_3_value_first_15days = 0.0\n",
    "            \n",
    "        if count2 > 0:    \n",
    "            feature_3_value_second_15days = feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, list_of_dtc_type[i], \"Y\")\n",
    "        else:\n",
    "            feature_3_value_second_15days = 0.0\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Feature 4:\n",
    "                \n",
    "        Definition: duration of intermittent alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines([\"feature 4 for {} in day {} \\n\".format(thisVIN, dayCount)])\n",
    "        file.close()\n",
    "        print(\"feature 4 for {} in day {} \\n\".format(thisVIN, dayCount))\n",
    "\n",
    "        if count1 > 0:\n",
    "            feature_4_value_first_15days = feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, list_of_dtc_type[i], \"I\")\n",
    "        else:\n",
    "            feature_4_value_first_15days = 0.0\n",
    "            \n",
    "        if count2 > 0: \n",
    "            feature_4_value_second_15days = feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, list_of_dtc_type[i], \"I\")\n",
    "        else:\n",
    "            feature_4_value_second_15days = 0.0\n",
    "            \n",
    "        '''    \n",
    "        Feature 5\n",
    "                \n",
    "        Definition: average time between active alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines([\"feature 5 for {} in day {} \\n\".format(thisVIN, dayCount)])\n",
    "        file.close()\n",
    "        print(\"feature 5 for {} in day {} \\n\".format(thisVIN, dayCount))\n",
    "\n",
    "        if count1 > 0:\n",
    "            feature_5_value_first_15days =  feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, list_of_dtc_type[i], \"Y\")\n",
    "        else:\n",
    "            feature_5_value_first_15days =  0.0 \n",
    "            \n",
    "        if count2 > 0:    \n",
    "            feature_5_value_second_15days = feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, list_of_dtc_type[i], \"Y\")\n",
    "        else:\n",
    "            feature_5_value_second_15days = 0.0\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Feature 6\n",
    "                \n",
    "        Definition: average time between intermittent alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines([\"feature 6 for {} in day {} \\n\".format(thisVIN, dayCount)])\n",
    "        file.close()\n",
    "        print(\"feature 6 for {} in day {} \\n\".format(thisVIN, dayCount))\n",
    "\n",
    "        if count1 > 0:\n",
    "            feature_6_value_first_15days =  feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, list_of_dtc_type[i], \"I\")\n",
    "        else:\n",
    "            feature_6_value_first_15days =  0.0\n",
    "            \n",
    "        if count2 > 0:    \n",
    "            feature_6_value_second_15days = feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, list_of_dtc_type[i], \"I\")\n",
    "        else:\n",
    "            feature_6_value_second_15days = 0.0\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Feature 7\n",
    "                \n",
    "        Definition: number of active alerts with speed = 0 over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines([\"feature 7 for {} in day {} \\n\".format(thisVIN, dayCount)])\n",
    "        file.close()\n",
    "        print(\"feature 7 for {} in day {} \\n\".format(thisVIN, dayCount))\n",
    "\n",
    "        if count1 > 0:        \n",
    "            feature_7_value_first_15days =  feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, 0, \"Y\")\n",
    "        else:\n",
    "            feature_7_value_first_15days =  0\n",
    "        \n",
    "        if count2 > 0:\n",
    "            feature_7_value_second_15days = feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, 0, \"Y\")\n",
    "        else:\n",
    "            feature_7_value_second_15days = 0\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Feature 8\n",
    "                \n",
    "        Definition: number of intermittent alerts with speed = 0 over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\") \n",
    "        file.writelines([\"feature 8 for {} in day {} \\n\".format(thisVIN, dayCount)])\n",
    "        file.close()\n",
    "        print(\"feature 8 for {} in day {} \\n\".format(thisVIN, dayCount))\n",
    "\n",
    "        if count1 > 0:        \n",
    "            feature_8_value_first_15days =  feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, 0, \"I\")\n",
    "        else:\n",
    "            feature_8_value_first_15days =  0\n",
    "            \n",
    "        if count2 > 0:    \n",
    "            feature_8_value_second_15days = feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, 0, \"I\")\n",
    "        else:\n",
    "            feature_8_value_second_15days = 0\n",
    "            \n",
    "            \n",
    "  \n",
    "        dtc = 'aaaaaa'\n",
    "\n",
    "        if i == 0 :\n",
    "          dtc = 'dtc38'\n",
    "        elif i == 1:\n",
    "          dtc = 'dtc75'\n",
    "        elif i == 2:\n",
    "          dtc = 'dtc77'\n",
    "        elif i == 3:\n",
    "          dtc = 'dtc86'\n",
    "        elif i == 4:\n",
    "          dtc = 'dtc92'\n",
    "        elif i == 5:\n",
    "          dtc = 'dtc94'\n",
    "        elif i == 6:\n",
    "          dtc = 'dtc0401'\n",
    "        elif i == 7:\n",
    "          dtc = 'dtc2457'\n",
    "\n",
    "        #end of else for i values\n",
    "        feature1Name1 = 'feature_1_'+dtc+'_first_15_days'\n",
    "        feature1Name2 = 'feature_1_'+dtc+'_second_15_days'\n",
    "        feature2Name1 = 'feature_2_'+dtc+'_first_15_days'\n",
    "        feature2Name2 = 'feature_2_'+dtc+'_second_15_days'\n",
    "        feature3Name1 = 'feature_3_'+dtc+'_first_15_days'\n",
    "        feature3Name2 = 'feature_3_'+dtc+'_second_15_days'\n",
    "        feature4Name1 = 'feature_4_'+dtc+'_first_15_days'\n",
    "        feature4Name2 = 'feature_4_'+dtc+'_second_15_days'\n",
    "        feature5Name1 = 'feature_5_'+dtc+'_first_15_days'\n",
    "        feature5Name2 = 'feature_5_'+dtc+'_second_15_days'\n",
    "        feature6Name1 = 'feature_6_'+dtc+'_first_15_days'\n",
    "        feature6Name2 = 'feature_6_'+dtc+'_second_15_days'\n",
    "        feature7Name1 = 'feature_7_'+dtc+'_first_15_days'\n",
    "        feature7Name2 = 'feature_7_'+dtc+'_second_15_days'\n",
    "        feature8Name1 = 'feature_8_'+dtc+'_first_15_days'\n",
    "        feature8Name2 = 'feature_8_'+dtc+'_second_15_days'\n",
    "\n",
    "\n",
    "        df_calculated_features_for_this_VIN_and_this_dayCount = df_calculated_features_for_this_VIN_and_this_dayCount \\\n",
    "                                                                                               .withColumn(feature1Name1, (f.lit(feature_1_value_first_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature1Name2, (f.lit(feature_1_value_second_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature2Name1, (f.lit(feature_2_value_first_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature2Name2, (f.lit(feature_2_value_second_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature3Name1, (f.lit(feature_3_value_first_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature3Name2, (f.lit(feature_3_value_second_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature4Name1, (f.lit(feature_4_value_first_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature4Name2, (f.lit(feature_4_value_second_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature5Name1, (f.lit(feature_5_value_first_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature5Name2, (f.lit(feature_5_value_second_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature6Name1, (f.lit(feature_6_value_first_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature6Name2, (f.lit(feature_6_value_second_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature7Name1, (f.lit(feature_7_value_first_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature7Name2, (f.lit(feature_7_value_second_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature8Name1, (f.lit(feature_8_value_first_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature8Name2, (f.lit(feature_8_value_second_15days)).cast(IntegerType()))\\\n",
    "\n",
    " \n",
    "        #   for loop finished \n",
    "        \n",
    "    df_calculated_features_for_this_VIN_and_this_dayCount = df_calculated_features_for_this_VIN_and_this_dayCount.withColumn(\"if_parts_replaced_in_first_15days\", (f.lit(if_part_is_replaced_for_this_VIN_in_this_timespan(thisVIN, new_15day_start_date, new_15day_end_date))).cast(IntegerType()))\\\n",
    "  .withColumn(\"if_parts_replaced_in_second_15days\", (f.lit(if_part_is_replaced_for_this_VIN_in_this_timespan(thisVIN, previous_15day_duration_start_date, previous_15day_duration_end_date))).cast(IntegerType()))\n",
    "    print(\"we finally write something of length for {} in day {} \\n\".format(thisVIN, dayCount))\n",
    "#     dataset_is_generated = 1\n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "    file.writelines([\"we finally write something of length for {} in day {} \\n\".format(thisVIN, dayCount)])\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "    VINs_columns_names =['VIN','TOTAL_ROWS']\n",
    "    df_VINs = pd.read_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv', sep=',', names=VINs_columns_names, header=None)\n",
    "    df_VINs.loc[df_VINs['VIN'] == thisVIN, ['TOTAL_ROWS']] = dayCount\n",
    "\n",
    "    # df_VINs.to_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv', index = None, mode = 'w', header=False)\n",
    "    \n",
    "    '''here we aggregate all the selected features from the population for this VIN with the 8 calculated feature values for this VIN \n",
    "    for this specific day and then write it to resultedData.csv as one data point.'''\n",
    "\n",
    "    list_features_for_this_VIN_and_this_dayCount = [df_selected_features_from_population_for_this_VIN, df_calculated_features_for_this_VIN_and_this_dayCount]\n",
    "    df_features_for_this_VIN_and_this_dayCount = reduce(lambda x, y: x.join(y, on=\"VIN\"), list_features_for_this_VIN_and_this_dayCount)\n",
    "    df_features_for_this_VIN_and_this_dayCount.toPandas().to_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/resultedData.csv', index = None, mode = 'a', header=False) \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b3da9f-4629-48fe-bde5-58da75afbe47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current VIN: -f \n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_/storage/home/yqf5148/.local/share/jupyter/runtime/kernel-ecf12fb3-4f13-4c42-85d2-376a3a4cd9cd.json.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#erasing the txt file for output of the submitted job that runs this Notebook:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"w\").close()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent VIN: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(thisVIN))\n\u001b[0;32m----> 9\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mthe_calculator_jobID_for_thisVIN\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m file\u001b[38;5;241m.\u001b[39mwritelines([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent VIN: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(thisVIN)])\n\u001b[1;32m     11\u001b[0m file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/work/anaconda3/envs/volvopennstate-env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_/storage/home/yqf5148/.local/share/jupyter/runtime/kernel-ecf12fb3-4f13-4c42-85d2-376a3a4cd9cd.json.txt'"
     ]
    }
   ],
   "source": [
    "# Loop through the arguments and print them\n",
    "if len(sys.argv) > 1:\n",
    "    thisVIN = sys.argv[1]\n",
    "    the_calculator_jobID_for_thisVIN = sys.argv[2]\n",
    "    #erasing the txt file for output of the submitted job that runs this Notebook:\n",
    "    # open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"w\").close()\n",
    "    \n",
    "    print(\"Current VIN: {} \\n\".format(thisVIN))\n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "    file.writelines([\"Current VIN: {} \\n\".format(thisVIN)])\n",
    "    file.close()\n",
    "    \n",
    "    ##### Features Generator Code:\n",
    "    duration_end_date = '2021-12-31'\n",
    "\n",
    "    day_delta = timedelta(days = 1)\n",
    "    split_date = duration_end_date.split('-')\n",
    "\n",
    "    end_date = date(int(split_date[0]), int(split_date[1]), int(split_date[2]))\n",
    "    start_date = end_date - timedelta(days = 2557)\n",
    "\n",
    "    print(\"start_date = {}, end_date = {}\".format(start_date, end_date))\n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "    file.writelines([\"start_date = {}, end_date = {} \\n\".format(start_date, end_date)])\n",
    "    file.close()\n",
    "\n",
    "    span_length = 15\n",
    "\n",
    "\n",
    "    VIN_feature_columns = StructType([StructField('VIN', StringType(), True),\n",
    "                                      StructField('TOTAL_ROWS', IntegerType(), True)])\n",
    "    df_new_VIN = spark.createDataFrame(data = [(thisVIN, 0)], schema = VIN_feature_columns)\n",
    "    # df_new_VIN.toPandas().to_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv', index = None, mode = 'a', header=False) \n",
    "\n",
    "\n",
    "    df_filtered_population_for_this_VIN = df_population.filter(f.col('VIN')==thisVIN)\n",
    "    selected_features_from_population_for_this_VIN = ['VIN','ENGINE_SIZE','ENGINE_HP','VEH_TYPE']+[s for s in df_filtered_population_for_this_VIN.columns if 'KOLA' in s]\n",
    "    df_selected_features_from_population_for_this_VIN = df_filtered_population_for_this_VIN[selected_features_from_population_for_this_VIN]\n",
    "    if df_selected_features_from_population_for_this_VIN.count()!= 0 :\n",
    "        how_many_month = int((end_date - start_date).days/15)\n",
    "        print(\"how_many_month={} \\n\".format(how_many_month))\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "        file.writelines([\"how_many_month={} \\n\".format(how_many_month)])\n",
    "        file.close()\n",
    "        if how_many_month == 0:\n",
    "            remaining_days = int((end_date - start_date).days) \n",
    "            print(\"remaining_days={} \\n\".format(remaining_days))\n",
    "            file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "            file.writelines([\"remaining_days={} \\n\".format(remaining_days)])\n",
    "            file.close()\n",
    "            Parallel(n_jobs= 5, prefer=\"threads\", batch_size=5)(delayed(move_over_calendar_and_compute_features)(df_selected_features_from_population_for_this_VIN, thisVIN, end_date, span_length, day_count_in_month, the_calculator_jobID_for_thisVIN) for day_count_in_month in range(0, remaining_days))\n",
    "        else:\n",
    "            for number_of_monthes_in_time_duration in range(0, how_many_month):\n",
    "                print(\"number_of_monthes_in_time_duration={} \\n\".format(number_of_monthes_in_time_duration))\n",
    "                file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "                file.writelines([\"number_of_monthes_in_time_duration={} \\n\".format(number_of_monthes_in_time_duration)])\n",
    "                file.close()\n",
    "                if number_of_monthes_in_time_duration < how_many_month:\n",
    "                    Parallel(n_jobs= 5, prefer=\"threads\", batch_size=5)(delayed(move_over_calendar_and_compute_features)(df_selected_features_from_population_for_this_VIN, thisVIN, end_date, span_length, 15 * number_of_monthes_in_time_duration + day_count_in_month, the_calculator_jobID_for_thisVIN) for day_count_in_month in range(0, 15))   \n",
    "\n",
    "                else:\n",
    "                    remaining_days = int((end_date - start_date).days) - 15 * number_of_monthes_in_time_duration \n",
    "                    print(\"remaining_days={} \\n\".format(remaining_days))\n",
    "                    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "                    file.writelines([\"remaining_days={} \\n\".format(remaining_days)])\n",
    "                    file.close()\n",
    "                    Parallel(n_jobs= 5, prefer=\"threads\", batch_size=5)(delayed(move_over_calendar_and_compute_features)(df_selected_features_from_population_for_this_VIN, thisVIN, end_date, span_length, 15 * number_of_monthes_in_time_duration + day_count_in_month, the_calculator_jobID_for_thisVIN) for day_count_in_month in range(0, remaining_days))\n",
    "\n",
    "\n",
    "else:\n",
    "    \n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/errors/error_log_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "    file.writelines([\"The VIN argument is failed to be passed for feature calculations. \\n\".format(remaining_days)])\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984361e-8f24-4af6-b62e-8d05a362f97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#         lists = results_for_this_VIN_this_day\n",
    "#         list_of_res = np.array(pd.concat([pd.Series(x) for x in lists], axis=1)).T.tolist()\n",
    "\n",
    "#         data = list_of_res[0][0]\n",
    "#         df_features_for_this_VIN_and_this_dayCount = spark.createDataFrame(data = [data], schema = feature_columns)\n",
    "\n",
    "#         for i in range(1, int(len(list_of_res))):\n",
    "#             list_for_thisVIN_and_thisDay = list_of_res[i][0]\n",
    "#             df_for_thisVIN_and_thisDay = spark.createDataFrame(data = [list_for_thisVIN_and_thisDay], schema = feature_columns)\n",
    "#             df_features_for_this_VIN_and_this_dayCount = df_features_for_this_VIN_and_this_dayCount.union(df_for_thisVIN_and_thisDay)\n",
    "#         #end for-loop\n",
    "\n",
    "#         ndays = df_features_for_this_VIN_and_this_dayCount.count()\n",
    "#         if ndays > 0:\n",
    "#             print('we finally write something of length', ndays)\n",
    "#     #         df_features_for_this_VIN_and_this_dayCount.write.mode(\"append\").format('delta').saveAsTable('df_AMT_features2')   on Databricks we were saving results into a table on database. \n",
    "#     #         df_features_for_this_VIN_and_this_dayCount.repartition(1).write.option(\"header\",True).csv(path=\"./data\", mode=\"append\")\n",
    "#     #         write_csv_with_specific_file_name(sc, df_features_for_this_VIN_and_this_dayCount, \"./data\", \"/resulted_dataset.csv\")\n",
    "#             df_features_for_this_VIN_and_this_dayCount.repartition(1).write.option(\"header\",True).format(\"csv\").mode(\"append\").save(\"./data/dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25f5cf-d9ea-4f53-a049-81f2d0e9909d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # import pandas module\n",
    "# import pandas as pd\n",
    " \n",
    "# # consider a list\n",
    "# list1 = [\"durga\", \"ramya\", \"sravya\"]\n",
    "# list2 = [\"java\", \"php\", \"mysql\"]\n",
    "# list3 = [67, 89, 65]\n",
    "\n",
    "# lists = [list1, list2, list3]\n",
    "# df_of_res = np.array(pd.concat([pd.Series(x) for x in lists], axis=1)).T.tolist()\n",
    "# print(df_of_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae1a4cb-7573-444c-8d63-8e7b5c755c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list into dataframe row by\n",
    "# using zip()\n",
    "# data = pd.DataFrame(list(zip(list1, list2, list3)),\n",
    "#                     columns=['student', 'subject', 'marks'])\n",
    " \n",
    "# display(data)\n",
    "\n",
    "# df_pop = spark.sql('select * from population')\n",
    "# [c for c in set(df_pop.toPandas()['VIN'].unique()) if c not in tmp]\n",
    "# tmp = df_pop.toPandas()['VIN'].unique()\n",
    "\n",
    "# df = pd.DataFrame({'a': [1, 1, 2], 'b': ['a', 'b', 'c'], 'c': [4, 5, 6]})\n",
    "# # Normalize using Min/Max Normalization.\n",
    "\n",
    "# display(df)\n",
    "# df_num = df.select_dtypes(include='number')\n",
    "# # selected = [s for s in df.columns if 'a' in s]    #+['VIN']\n",
    "# # print(df_num.dtypes)\n",
    "# df_norm = (df_num - df_num.mean()) / (df_num.max() - df_num.min())\n",
    "\n",
    "# # df = df.toPandas()\n",
    "# df[df_norm.columns] = df_norm\n",
    "# display(df)\n",
    "\n",
    "# df = spark.sql('select * from p2457_faults LIMIT 10')\n",
    "# pandas_df = df.toPandas()\n",
    "# pandas_df = pandas_df.replace(\"NA\", None)\n",
    "# pandas_df = pandas_df.replace(\",\", \".\")\n",
    "# display(pandas_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# statement_in_feature_name = 'ROAD_SPEED'\n",
    "# df_normalized = normalize_numeric_feature_values(statement_in_feature_name, pandas_df)\n",
    "# display(df_normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volvopennstate_new_kernel",
   "language": "python",
   "name": "volvopennstate_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
