{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a27aa97-1ee0-4124-a4d8-a74c657e268d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
      "env: PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin\n"
     ]
    }
   ],
   "source": [
    "%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
    "%env PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e21e3dd-ad74-4056-9481-fb1dcd5574df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: ace_tools_open\n",
      "Version: 0.1.0\n",
      "Summary: Open implementation of ace_tools referenced in GPT4o\n",
      "Home-page: https://github.com/zinccat/ace_tools_open\n",
      "Author: ZincCat\n",
      "Author-email: ZincCat <zincchloride@outlook.com>\n",
      "License: MIT\n",
      "Location: /storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/lib/python3.12/site-packages\n",
      "Requires: IPython, itables, pandas\n",
      "Required-by: \n",
      "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
      "\u001b[0mFiles removed: 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip show ace_tools_open\n",
    "!{sys.executable} -m pip cache purge\n",
    "# !{sys.executable} -m pip uninstall ace_tools\n",
    "\n",
    "\n",
    "# !python -m pip uninstall ace_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b49939f-eff5-4226-9873-0a4a2c0b2a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/lib/python312.zip', '/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/lib/python3.12', '/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/lib/python3.12/lib-dynload', '', '/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/lib/python3.12/site-packages', '/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/lib/python3.12/site-packages/setuptools/_vendor']\n",
      "/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin/python3.12\n",
      "Python 3.12.4\n"
     ]
    }
   ],
   "source": [
    "# If path to ace_tools is not listed in the sys.path you should append it manually: \n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "# sys.path.append(\"work/anaconda3/envs/volvopennstate-env/lib/python3.12/site-packages\")\n",
    "\n",
    "print(sys.executable)\n",
    "\n",
    "!{sys.executable} --version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde3da6-653b-4be9-a3ff-0d697cf45e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ps: /storage/icds/RISE/sw8/anaconda/anaconda3/envs/tensorflow/lib/libuuid.so.1: no version information available (required by /usr/lib64/libblkid.so.1)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/30 15:50:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyspark as psk\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Row \n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import time as t \n",
    "from datetime import date, datetime, timedelta\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import reduce  \n",
    "from math import modf\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "import ace_tools_open as tools\n",
    "\n",
    "from delta import * \n",
    "from delta.tables import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "conf = (SparkConf().set(\"spark.driver.maxResultSize\", \"4g\"))\n",
    "\n",
    "# Create new context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# sc = SparkContext(\"local\", \"Simple App\")\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "       .master(\"local[2]\") \\\n",
    "       .appName(\"test\") \\\n",
    "       .config(\"spark.driver.maxResultSize\", \"20g\")\\\n",
    "       .config(\"spark.driver.memory\", \"100g\")\\\n",
    "       .getOrCreate()\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")    #To resolve the error for p1075_38 to_timestamp formating: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '1/2/2019 20:40:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/PopulationWithChassisId.csv\") \\\n",
    "          .createOrReplaceTempView(\"population\")\n",
    "\n",
    "df_population = spark.sql(\"SELECT * FROM population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28644d14-8da2-4f09-aa2c-6ab8d6382b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_features = [    \n",
    "          \"calendar_day\", \n",
    "          \"f_1_dtc38_1th_15d\", \n",
    "          \"f_1_dtc38_2nd_15d\", \n",
    "          \"f_2_dtc38_1th_15d\", \n",
    "          \"f_2_dtc38_2nd_15d\", \n",
    "          \"f_3_dtc38_1th_15d\", \n",
    "          \"f_3_dtc38_2nd_15d\", \n",
    "          \"f_4_dtc38_1th_15d\", \n",
    "          \"f_4_dtc38_2nd_15d\", \n",
    "          \"f_5_dtc38_1th_15d\", \n",
    "          \"f_5_dtc38_2nd_15d\", \n",
    "          \"f_6_dtc38_1th_15d\", \n",
    "          \"f_6_dtc38_2nd_15d\", \n",
    "          \"f_7_dtc38_1th_15d\", \n",
    "          \"f_7_dtc38_2nd_15d\", \n",
    "          \"f_8_dtc38_1th_15d\", \n",
    "          \"f_8_dtc38_2nd_15d\", \n",
    "\n",
    "          \"f_1_dtc75_1th_15d\", \n",
    "          \"f_1_dtc75_2nd_15d\", \n",
    "          \"f_2_dtc75_1th_15d\", \n",
    "          \"f_2_dtc75_2nd_15d\", \n",
    "          \"f_3_dtc75_1th_15d\", \n",
    "          \"f_3_dtc75_2nd_15d\", \n",
    "          \"f_4_dtc75_1th_15d\", \n",
    "          \"f_4_dtc75_2nd_15d\", \n",
    "          \"f_5_dtc75_1th_15d\", \n",
    "          \"f_5_dtc75_2nd_15d\", \n",
    "          \"f_6_dtc75_1th_15d\", \n",
    "          \"f_6_dtc75_2nd_15d\", \n",
    "          \"f_7_dtc75_1th_15d\", \n",
    "          \"f_7_dtc75_2nd_15d\", \n",
    "          \"f_8_dtc75_1th_15d\", \n",
    "          \"f_8_dtc75_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc77_1th_15d\", \n",
    "          \"f_1_dtc77_2nd_15d\", \n",
    "          \"f_2_dtc77_1th_15d\", \n",
    "          \"f_2_dtc77_2nd_15d\", \n",
    "          \"f_3_dtc77_1th_15d\", \n",
    "          \"f_3_dtc77_2nd_15d\", \n",
    "          \"f_4_dtc77_1th_15d\", \n",
    "          \"f_4_dtc77_2nd_15d\", \n",
    "          \"f_5_dtc77_1th_15d\", \n",
    "          \"f_5_dtc77_2nd_15d\", \n",
    "          \"f_6_dtc77_1th_15d\", \n",
    "          \"f_6_dtc77_2nd_15d\", \n",
    "          \"f_7_dtc77_1th_15d\", \n",
    "          \"f_7_dtc77_2nd_15d\", \n",
    "          \"f_8_dtc77_1th_15d\", \n",
    "          \"f_8_dtc77_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc86_1th_15d\", \n",
    "          \"f_1_dtc86_2nd_15d\", \n",
    "          \"f_2_dtc86_1th_15d\", \n",
    "          \"f_2_dtc86_2nd_15d\", \n",
    "          \"f_3_dtc86_1th_15d\", \n",
    "          \"f_3_dtc86_2nd_15d\", \n",
    "          \"f_4_dtc86_1th_15d\", \n",
    "          \"f_4_dtc86_2nd_15d\", \n",
    "          \"f_5_dtc86_1th_15d\", \n",
    "          \"f_5_dtc86_2nd_15d\", \n",
    "          \"f_6_dtc86_1th_15d\", \n",
    "          \"f_6_dtc86_2nd_15d\", \n",
    "          \"f_7_dtc86_1th_15d\", \n",
    "          \"f_7_dtc86_2nd_15d\", \n",
    "          \"f_8_dtc86_1th_15d\", \n",
    "          \"f_8_dtc86_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc92_1th_15d\", \n",
    "          \"f_1_dtc92_2nd_15d\", \n",
    "          \"f_2_dtc92_1th_15d\", \n",
    "          \"f_2_dtc92_2nd_15d\", \n",
    "          \"f_3_dtc92_1th_15d\", \n",
    "          \"f_3_dtc92_2nd_15d\", \n",
    "          \"f_4_dtc92_1th_15d\", \n",
    "          \"f_4_dtc92_2nd_15d\", \n",
    "          \"f_5_dtc92_1th_15d\", \n",
    "          \"f_5_dtc92_2nd_15d\", \n",
    "          \"f_6_dtc92_1th_15d\", \n",
    "          \"f_6_dtc92_2nd_15d\", \n",
    "          \"f_7_dtc92_1th_15d\", \n",
    "          \"f_7_dtc92_2nd_15d\", \n",
    "          \"f_8_dtc92_1th_15d\", \n",
    "          \"f_8_dtc92_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc94_1th_15d\", \n",
    "          \"f_1_dtc94_2nd_15d\", \n",
    "          \"f_2_dtc94_1th_15d\", \n",
    "          \"f_2_dtc94_2nd_15d\", \n",
    "          \"f_3_dtc94_1th_15d\", \n",
    "          \"f_3_dtc94_2nd_15d\", \n",
    "          \"f_4_dtc94_1th_15d\", \n",
    "          \"f_4_dtc94_2nd_15d\", \n",
    "          \"f_5_dtc94_1th_15d\", \n",
    "          \"f_5_dtc94_2nd_15d\", \n",
    "          \"f_6_dtc94_1th_15d\", \n",
    "          \"f_6_dtc94_2nd_15d\", \n",
    "          \"f_7_dtc94_1th_15d\", \n",
    "          \"f_7_dtc94_2nd_15d\", \n",
    "          \"f_8_dtc94_1th_15d\", \n",
    "          \"f_8_dtc94_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc0401_1th_15d\", \n",
    "          \"f_1_dtc0401_2nd_15d\", \n",
    "          \"f_2_dtc0401_1th_15d\", \n",
    "          \"f_2_dtc0401_2nd_15d\", \n",
    "          \"f_3_dtc0401_1th_15d\", \n",
    "          \"f_3_dtc0401_2nd_15d\", \n",
    "          \"f_4_dtc0401_1th_15d\", \n",
    "          \"f_4_dtc0401_2nd_15d\", \n",
    "          \"f_5_dtc0401_1th_15d\", \n",
    "          \"f_5_dtc0401_2nd_15d\", \n",
    "          \"f_6_dtc0401_1th_15d\", \n",
    "          \"f_6_dtc0401_2nd_15d\", \n",
    "          \"f_7_dtc0401_1th_15d\", \n",
    "          \"f_7_dtc0401_2nd_15d\", \n",
    "          \"f_8_dtc0401_1th_15d\", \n",
    "          \"f_8_dtc0401_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc2457_1th_15d\", \n",
    "          \"f_1_dtc2457_2nd_15d\", \n",
    "          \"f_2_dtc2457_1th_15d\", \n",
    "          \"f_2_dtc2457_2nd_15d\", \n",
    "          \"f_3_dtc2457_1th_15d\", \n",
    "          \"f_3_dtc2457_2nd_15d\", \n",
    "          \"f_4_dtc2457_1th_15d\", \n",
    "          \"f_4_dtc2457_2nd_15d\", \n",
    "          \"f_5_dtc2457_1th_15d\", \n",
    "          \"f_5_dtc2457_2nd_15d\", \n",
    "          \"f_6_dtc2457_1th_15d\", \n",
    "          \"f_6_dtc2457_2nd_15d\", \n",
    "          \"f_7_dtc2457_1th_15d\", \n",
    "          \"f_7_dtc2457_2nd_15d\", \n",
    "          \"f_8_dtc2457_1th_15d\", \n",
    "          \"f_8_dtc2457_2nd_15d\",\n",
    "\n",
    "          \"if_parts_replaced_in_1th_15d\", \n",
    "          \"if_parts_replaced_in_2nd_15d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16668df-9f27-467d-805c-86a86ef588a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_features_minus_calendar_day = [     \n",
    "          \"f_1_dtc38_1th_15d\", \n",
    "          \"f_1_dtc38_2nd_15d\", \n",
    "          \"f_2_dtc38_1th_15d\", \n",
    "          \"f_2_dtc38_2nd_15d\", \n",
    "          \"f_3_dtc38_1th_15d\", \n",
    "          \"f_3_dtc38_2nd_15d\", \n",
    "          \"f_4_dtc38_1th_15d\", \n",
    "          \"f_4_dtc38_2nd_15d\", \n",
    "          \"f_5_dtc38_1th_15d\", \n",
    "          \"f_5_dtc38_2nd_15d\", \n",
    "          \"f_6_dtc38_1th_15d\", \n",
    "          \"f_6_dtc38_2nd_15d\", \n",
    "          \"f_7_dtc38_1th_15d\", \n",
    "          \"f_7_dtc38_2nd_15d\", \n",
    "          \"f_8_dtc38_1th_15d\", \n",
    "          \"f_8_dtc38_2nd_15d\", \n",
    "\n",
    "          \"f_1_dtc75_1th_15d\", \n",
    "          \"f_1_dtc75_2nd_15d\", \n",
    "          \"f_2_dtc75_1th_15d\", \n",
    "          \"f_2_dtc75_2nd_15d\", \n",
    "          \"f_3_dtc75_1th_15d\", \n",
    "          \"f_3_dtc75_2nd_15d\", \n",
    "          \"f_4_dtc75_1th_15d\", \n",
    "          \"f_4_dtc75_2nd_15d\", \n",
    "          \"f_5_dtc75_1th_15d\", \n",
    "          \"f_5_dtc75_2nd_15d\", \n",
    "          \"f_6_dtc75_1th_15d\", \n",
    "          \"f_6_dtc75_2nd_15d\", \n",
    "          \"f_7_dtc75_1th_15d\", \n",
    "          \"f_7_dtc75_2nd_15d\", \n",
    "          \"f_8_dtc75_1th_15d\", \n",
    "          \"f_8_dtc75_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc77_1th_15d\", \n",
    "          \"f_1_dtc77_2nd_15d\", \n",
    "          \"f_2_dtc77_1th_15d\", \n",
    "          \"f_2_dtc77_2nd_15d\", \n",
    "          \"f_3_dtc77_1th_15d\", \n",
    "          \"f_3_dtc77_2nd_15d\", \n",
    "          \"f_4_dtc77_1th_15d\", \n",
    "          \"f_4_dtc77_2nd_15d\", \n",
    "          \"f_5_dtc77_1th_15d\", \n",
    "          \"f_5_dtc77_2nd_15d\", \n",
    "          \"f_6_dtc77_1th_15d\", \n",
    "          \"f_6_dtc77_2nd_15d\", \n",
    "          \"f_7_dtc77_1th_15d\", \n",
    "          \"f_7_dtc77_2nd_15d\", \n",
    "          \"f_8_dtc77_1th_15d\", \n",
    "          \"f_8_dtc77_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc86_1th_15d\", \n",
    "          \"f_1_dtc86_2nd_15d\", \n",
    "          \"f_2_dtc86_1th_15d\", \n",
    "          \"f_2_dtc86_2nd_15d\", \n",
    "          \"f_3_dtc86_1th_15d\", \n",
    "          \"f_3_dtc86_2nd_15d\", \n",
    "          \"f_4_dtc86_1th_15d\", \n",
    "          \"f_4_dtc86_2nd_15d\", \n",
    "          \"f_5_dtc86_1th_15d\", \n",
    "          \"f_5_dtc86_2nd_15d\", \n",
    "          \"f_6_dtc86_1th_15d\", \n",
    "          \"f_6_dtc86_2nd_15d\", \n",
    "          \"f_7_dtc86_1th_15d\", \n",
    "          \"f_7_dtc86_2nd_15d\", \n",
    "          \"f_8_dtc86_1th_15d\", \n",
    "          \"f_8_dtc86_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc92_1th_15d\", \n",
    "          \"f_1_dtc92_2nd_15d\", \n",
    "          \"f_2_dtc92_1th_15d\", \n",
    "          \"f_2_dtc92_2nd_15d\", \n",
    "          \"f_3_dtc92_1th_15d\", \n",
    "          \"f_3_dtc92_2nd_15d\", \n",
    "          \"f_4_dtc92_1th_15d\", \n",
    "          \"f_4_dtc92_2nd_15d\", \n",
    "          \"f_5_dtc92_1th_15d\", \n",
    "          \"f_5_dtc92_2nd_15d\", \n",
    "          \"f_6_dtc92_1th_15d\", \n",
    "          \"f_6_dtc92_2nd_15d\", \n",
    "          \"f_7_dtc92_1th_15d\", \n",
    "          \"f_7_dtc92_2nd_15d\", \n",
    "          \"f_8_dtc92_1th_15d\", \n",
    "          \"f_8_dtc92_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc94_1th_15d\", \n",
    "          \"f_1_dtc94_2nd_15d\", \n",
    "          \"f_2_dtc94_1th_15d\", \n",
    "          \"f_2_dtc94_2nd_15d\", \n",
    "          \"f_3_dtc94_1th_15d\", \n",
    "          \"f_3_dtc94_2nd_15d\", \n",
    "          \"f_4_dtc94_1th_15d\", \n",
    "          \"f_4_dtc94_2nd_15d\", \n",
    "          \"f_5_dtc94_1th_15d\", \n",
    "          \"f_5_dtc94_2nd_15d\", \n",
    "          \"f_6_dtc94_1th_15d\", \n",
    "          \"f_6_dtc94_2nd_15d\", \n",
    "          \"f_7_dtc94_1th_15d\", \n",
    "          \"f_7_dtc94_2nd_15d\", \n",
    "          \"f_8_dtc94_1th_15d\", \n",
    "          \"f_8_dtc94_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc0401_1th_15d\", \n",
    "          \"f_1_dtc0401_2nd_15d\", \n",
    "          \"f_2_dtc0401_1th_15d\", \n",
    "          \"f_2_dtc0401_2nd_15d\", \n",
    "          \"f_3_dtc0401_1th_15d\", \n",
    "          \"f_3_dtc0401_2nd_15d\", \n",
    "          \"f_4_dtc0401_1th_15d\", \n",
    "          \"f_4_dtc0401_2nd_15d\", \n",
    "          \"f_5_dtc0401_1th_15d\", \n",
    "          \"f_5_dtc0401_2nd_15d\", \n",
    "          \"f_6_dtc0401_1th_15d\", \n",
    "          \"f_6_dtc0401_2nd_15d\", \n",
    "          \"f_7_dtc0401_1th_15d\", \n",
    "          \"f_7_dtc0401_2nd_15d\", \n",
    "          \"f_8_dtc0401_1th_15d\", \n",
    "          \"f_8_dtc0401_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc2457_1th_15d\", \n",
    "          \"f_1_dtc2457_2nd_15d\", \n",
    "          \"f_2_dtc2457_1th_15d\", \n",
    "          \"f_2_dtc2457_2nd_15d\", \n",
    "          \"f_3_dtc2457_1th_15d\", \n",
    "          \"f_3_dtc2457_2nd_15d\", \n",
    "          \"f_4_dtc2457_1th_15d\", \n",
    "          \"f_4_dtc2457_2nd_15d\", \n",
    "          \"f_5_dtc2457_1th_15d\", \n",
    "          \"f_5_dtc2457_2nd_15d\", \n",
    "          \"f_6_dtc2457_1th_15d\", \n",
    "          \"f_6_dtc2457_2nd_15d\", \n",
    "          \"f_7_dtc2457_1th_15d\", \n",
    "          \"f_7_dtc2457_2nd_15d\", \n",
    "          \"f_8_dtc2457_1th_15d\", \n",
    "          \"f_8_dtc2457_2nd_15d\",\n",
    "\n",
    "          \"if_parts_replaced_in_1th_15d\", \n",
    "          \"if_parts_replaced_in_2nd_15d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0c434-0e2e-4906-bed4-3f76e281fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "egr_cooler_claims_file_path = \"/storage/home/yqf5148/work/volvoPennState/EGR Cooler Claims.csv\"\n",
    "df_egr_cooler_claims = pd.read_csv(egr_cooler_claims_file_path, header=None, names=['BUSINESS_UNIT', 'CAMP_TYPE', 'CLAIM_HANDLING_DECISION', 'CLAIM_JOB_ID', 'CLAIM_REG_DATE', 'CLAIM_STATUS_DETAIL', 'CRED_DATE', 'DEB_CD', 'DEB_CD_DESC', 'DES_AREA', 'FGRP4', 'FGRP4_DESC', 'MAIN_OP', 'MAIN_OP_DESC', 'MILEAGE_MILES_CLAIM', 'PART_NAME', 'PART_NO', 'REF_NO', 'REPAIR_ID', 'REP_CNTRY', 'REP_DATE', 'REP_DEALER', 'REP_DEALER_NAME', 'REP_REGION', 'REP_STATE', 'SUPPL_NAME', 'SUPPL_NO', 'TOT_CLAIM_PAYMENT_USD', 'TOT_LABOR_PAYMENT_USD', 'TOT_MTR_PAYMENT_USD', 'TOT_OTHER_PAYMENT_USD', 'TOT_TRAVEL_PAYMENT_USD', 'VEH_AGE_CLAIM', 'VMT', 'WIS_CLAIM_ID', 'WIS_PRODUCT_ID', 'VEHICLE_ASEMBLY_DATE', 'APPL_TYPE', 'BRAND', 'CHASSIS_SERIES', 'CHASSIS_NO', 'COMPANY', 'CUST_NAME', 'CUST_NO', 'DEL_CNTRY', 'DEL_DEALER', 'DEL_DEALER_NAME', 'DEL_REGION', 'DEL_STATE', 'DEL_STATUS', 'ENGINE_ASSEMB_DATE', 'ENGINE_HP', 'ENGINE_MK', 'ENGINE_SIZE', 'INS_DATE', 'LAST_KNOWN_MILEAGE_MILES', 'SN_ENGINE', 'SN_GEARB', 'VEH_AGE', 'VEH_ASSEMB_DATE', 'VEH_ASSEMB_YEAR', 'VEH_MD_FAMILY', 'VEH_TYPE', 'VIN', 'COM_FIELD1', 'COM_FIELD2', 'COM_FIELD3', 'COM_FIELD_4_EXTERNAL'], index_col=False, encoding='ISO-8859-1', dtype='unicode')\n",
    "\n",
    "egr_sensors_claim_file_path = \"/storage/home/yqf5148/work/volvoPennState/EGR Sensors.csv\"\n",
    "df_egr_sensors_claims = pd.read_csv(egr_cooler_claims_file_path, header=None, names=['BUSINESS_UNIT', 'CAMP_TYPE', 'CLAIM_HANDLING_DECISION', 'CLAIM_JOB_ID', 'CLAIM_REG_DATE', 'CLAIM_STATUS_DETAIL', 'CRED_DATE', 'DEB_CD', 'DEB_CD_DESC', 'DES_AREA', 'FGRP4', 'FGRP4_DESC', 'MAIN_OP', 'MAIN_OP_DESC', 'MILEAGE_MILES_CLAIM', 'PART_NAME', 'PART_NO', 'REF_NO', 'REPAIR_ID', 'REP_CNTRY', 'REP_DATE', 'REP_DEALER', 'REP_DEALER_NAME', 'REP_REGION', 'REP_STATE', 'SUPPL_NAME', 'SUPPL_NO', 'TOT_CLAIM_PAYMENT_USD', 'TOT_LABOR_PAYMENT_USD', 'TOT_MTR_PAYMENT_USD', 'TOT_OTHER_PAYMENT_USD', 'TOT_TRAVEL_PAYMENT_USD', 'VEH_AGE_CLAIM', 'VMT', 'WIS_CLAIM_ID', 'WIS_PRODUCT_ID', 'VEHICLE_ASEMBLY_DATE', 'APPL_TYPE', 'BRAND', 'CHASSIS_SERIES', 'CHASSIS_NO', 'COMPANY', 'CUST_NAME', 'CUST_NO', 'DEL_CNTRY', 'DEL_DEALER', 'DEL_DEALER_NAME', 'DEL_REGION', 'DEL_STATE', 'DEL_STATUS', 'ENGINE_ASSEMB_DATE', 'ENGINE_HP', 'ENGINE_MK', 'ENGINE_SIZE', 'INS_DATE', 'LAST_KNOWN_MILEAGE_MILES', 'SN_ENGINE', 'SN_GEARB', 'VEH_AGE', 'VEH_ASSEMB_DATE', 'VEH_ASSEMB_YEAR', 'VEH_MD_FAMILY', 'VEH_TYPE', 'VIN', 'COM_FIELD1', 'COM_FIELD2', 'COM_FIELD3', 'COM_FIELD_4_EXTERNAL'], index_col=False, encoding='ISO-8859-1', dtype='unicode')\n",
    "\n",
    "egr_fg_293_claims_file_path = \"/storage/home/yqf5148/work/volvoPennState/EGR FG 293 Claims.csv\"\n",
    "df_egr_fg_293_claims = pd.read_csv(egr_cooler_claims_file_path, header=None, names=['BUSINESS_UNIT', 'CAMP_TYPE', 'CLAIM_HANDLING_DECISION', 'CLAIM_JOB_ID', 'CLAIM_REG_DATE', 'CLAIM_STATUS_DETAIL', 'CRED_DATE', 'DEB_CD', 'DEB_CD_DESC', 'DES_AREA', 'FGRP4', 'FGRP4_DESC', 'MAIN_OP', 'MAIN_OP_DESC', 'MILEAGE_MILES_CLAIM', 'PART_NAME', 'PART_NO', 'REF_NO', 'REPAIR_ID', 'REP_CNTRY', 'REP_DATE', 'REP_DEALER', 'REP_DEALER_NAME', 'REP_REGION', 'REP_STATE', 'SUPPL_NAME', 'SUPPL_NO', 'TOT_CLAIM_PAYMENT_USD', 'TOT_LABOR_PAYMENT_USD', 'TOT_MTR_PAYMENT_USD', 'TOT_OTHER_PAYMENT_USD', 'TOT_TRAVEL_PAYMENT_USD', 'VEH_AGE_CLAIM', 'VMT', 'WIS_CLAIM_ID', 'WIS_PRODUCT_ID', 'VEHICLE_ASEMBLY_DATE', 'APPL_TYPE', 'BRAND', 'CHASSIS_SERIES', 'CHASSIS_NO', 'COMPANY', 'CUST_NAME', 'CUST_NO', 'DEL_CNTRY', 'DEL_DEALER', 'DEL_DEALER_NAME', 'DEL_REGION', 'DEL_STATE', 'DEL_STATUS', 'ENGINE_ASSEMB_DATE', 'ENGINE_HP', 'ENGINE_MK', 'ENGINE_SIZE', 'INS_DATE', 'LAST_KNOWN_MILEAGE_MILES', 'SN_ENGINE', 'SN_GEARB', 'VEH_AGE', 'VEH_ASSEMB_DATE', 'VEH_ASSEMB_YEAR', 'VEH_MD_FAMILY', 'VEH_TYPE', 'VIN', 'COM_FIELD1', 'COM_FIELD2', 'COM_FIELD3', 'COM_FIELD_4_EXTERNAL'], index_col=False, encoding='ISO-8859-1', dtype='unicode')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6feb1d1-2836-4cc3-9da3-fef0f83299cb",
   "metadata": {},
   "source": [
    "# Finding the claims, type of them and filter them based on part_replacements for VINs in train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92816b49-69f1-408e-a42c-a01a9a29ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TOT_CLAIM_PAYMENT_USD column to numeric, forcing errors to NaN (in case of any non-numeric values)\n",
    "df_egr_cooler_claims['TOT_CLAIM_PAYMENT_USD'] = pd.to_numeric(df_egr_cooler_claims['TOT_CLAIM_PAYMENT_USD'], errors='coerce')\n",
    "df_egr_fg_293_claims['TOT_CLAIM_PAYMENT_USD'] = pd.to_numeric(df_egr_fg_293_claims['TOT_CLAIM_PAYMENT_USD'], errors='coerce')\n",
    "df_egr_sensors_claims['TOT_CLAIM_PAYMENT_USD'] = pd.to_numeric(df_egr_sensors_claims['TOT_CLAIM_PAYMENT_USD'], errors='coerce')\n",
    "\n",
    "\n",
    "# Load the clustered_aggregated_train_df data\n",
    "columns_names = ['VIN', 'ENGINE_SIZE', 'ENGINE_HP', 'VEH_TYPE'] + [s for s in df_population.columns if 'KOLA' in s] + calculated_features_minus_calendar_day + ['CLUSTER']\n",
    "clustered_aggregated_train_df = pd.read_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/clustered_aggregated_train_df.csv', sep=',', names=columns_names, header=None)\n",
    "\n",
    "\n",
    "# Filter only those claims that match VINs in clustered_aggregated_train_df\n",
    "claims_matches_cooler = df_egr_cooler_claims[df_egr_cooler_claims['VIN'].isin(clustered_aggregated_train_df.iloc[:, 0])]\n",
    "claims_matches_fg = df_egr_fg_293_claims[df_egr_fg_293_claims['VIN'].isin(clustered_aggregated_train_df.iloc[:, 0])]\n",
    "claims_matches_sensors = df_egr_sensors_claims[df_egr_sensors_claims['VIN'].isin(clustered_aggregated_train_df.iloc[:, 0])]\n",
    "\n",
    "\n",
    "# Add a CLAIM_TYPE column\n",
    "claims_matches_cooler['CLAIM_TYPE'] = 'EGR_COOLER'\n",
    "claims_matches_fg['CLAIM_TYPE'] = 'EGR_FG'\n",
    "claims_matches_sensors['CLAIM_TYPE'] = 'EGR_SENSORS'\n",
    "\n",
    "\n",
    "# Add the PART_REPL column (1 if TOT_CLAIM_PAYMENT_USD > 1000, else 0)\n",
    "for df in [claims_matches_cooler, claims_matches_fg, claims_matches_sensors]:\n",
    "    df['PART_REPL'] = df['TOT_CLAIM_PAYMENT_USD'] > 1000.0\n",
    "    df['PART_REPL'] = df['PART_REPL'].astype(int)\n",
    "\n",
    "    \n",
    "# Merge with clustered_aggregated_train_df to get CLUSTER value\n",
    "claims_matches_cooler = pd.merge(claims_matches_cooler, clustered_aggregated_train_df[['VIN', 'CLUSTER']], on='VIN', how='left')\n",
    "claims_matches_fg = pd.merge(claims_matches_fg, clustered_aggregated_train_df[['VIN', 'CLUSTER']], on='VIN', how='left')\n",
    "claims_matches_sensors = pd.merge(claims_matches_sensors, clustered_aggregated_train_df[['VIN', 'CLUSTER']], on='VIN', how='left')\n",
    "\n",
    "claims_matches = pd.concat([claims_matches_cooler, claims_matches_fg, claims_matches_sensors], ignore_index=True)\n",
    "\n",
    "\n",
    "# Calculate DAY_BACK for each claim\n",
    "reference_date = pd.to_datetime('2021-12-31')\n",
    "claims_matches['CLAIM_REG_DATE'] = pd.to_datetime(claims_matches['CLAIM_REG_DATE'])\n",
    "claims_matches['DAYS_BACK'] = (reference_date - claims_matches['CLAIM_REG_DATE']).dt.days\n",
    "\n",
    "\n",
    "# Select relevant columns, including the new PART_REPL column\n",
    "result_df_for_train_data = claims_matches[['VIN', 'CLAIM_JOB_ID', 'CLAIM_REG_DATE', 'TOT_CLAIM_PAYMENT_USD', 'DAYS_BACK', 'CLAIM_TYPE', 'CLUSTER', 'PART_REPL']]\n",
    "\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(result_df_for_train_data)\n",
    "\n",
    "\n",
    "# Save the resulting dataframe to a CSV file\n",
    "result_df_for_train_data.to_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/claims_result_for_train_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f178e5-59d1-4abf-a259-5bb6e4807836",
   "metadata": {},
   "source": [
    "# Finding the claims, type of them and filter them based on part_replacements for VINs in test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ad142-ac57-41c1-8ba2-53129ca74813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test DataFrame\n",
    "clustered_aggregated_test_df = pd.read_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/clustered_aggregated_test_df.csv', sep=',', names=columns_names, header=None)\n",
    "\n",
    "\n",
    "# Filter only those claims that match VINs in clustered_aggregated_test_df\n",
    "claims_matches_cooler = df_egr_cooler_claims[df_egr_cooler_claims['VIN'].isin(clustered_aggregated_test_df.iloc[:, 0])]\n",
    "claims_matches_fg = df_egr_fg_293_claims[df_egr_fg_293_claims['VIN'].isin(clustered_aggregated_test_df.iloc[:, 0])]\n",
    "claims_matches_sensors = df_egr_sensors_claims[df_egr_sensors_claims['VIN'].isin(clustered_aggregated_test_df.iloc[:, 0])]\n",
    "\n",
    "\n",
    "# Add a CLAIM_TYPE column\n",
    "claims_matches_cooler['CLAIM_TYPE'] = 'EGR_COOLER'\n",
    "claims_matches_fg['CLAIM_TYPE'] = 'EGR_FG'\n",
    "claims_matches_sensors['CLAIM_TYPE'] = 'EGR_SENSORS'\n",
    "\n",
    "\n",
    "# Add the PART_REPL column (1 if TOT_CLAIM_PAYMENT_USD > 1000, else 0)\n",
    "for df in [claims_matches_cooler, claims_matches_fg, claims_matches_sensors]:\n",
    "    df['PART_REPL'] = df['TOT_CLAIM_PAYMENT_USD'] > 1000.0\n",
    "    df['PART_REPL'] = df['PART_REPL'].astype(int)\n",
    "\n",
    "\n",
    "# Merge with clustered_aggregated_test_df to get CLUSTER value\n",
    "claims_matches_cooler = pd.merge(claims_matches_cooler, clustered_aggregated_test_df[['VIN', 'CLUSTER']], on='VIN', how='left')\n",
    "claims_matches_fg = pd.merge(claims_matches_fg, clustered_aggregated_test_df[['VIN', 'CLUSTER']], on='VIN', how='left')\n",
    "claims_matches_sensors = pd.merge(claims_matches_sensors, clustered_aggregated_test_df[['VIN', 'CLUSTER']], on='VIN', how='left')\n",
    "\n",
    "\n",
    "claims_matches = pd.concat([claims_matches_cooler, claims_matches_fg, claims_matches_sensors], ignore_index=True)\n",
    "\n",
    "\n",
    "# Calculate DAY_BACK for each claim\n",
    "reference_date = pd.to_datetime('2021-12-31')\n",
    "claims_matches['CLAIM_REG_DATE'] = pd.to_datetime(claims_matches['CLAIM_REG_DATE'])\n",
    "claims_matches['DAYS_BACK'] = (reference_date - claims_matches['CLAIM_REG_DATE']).dt.days\n",
    "\n",
    "\n",
    "# Select relevant columns, including the new PART_REPL column\n",
    "result_df_for_test_data = claims_matches[['VIN', 'CLAIM_JOB_ID', 'CLAIM_REG_DATE', 'TOT_CLAIM_PAYMENT_USD', 'DAYS_BACK', 'CLAIM_TYPE', 'CLUSTER', 'PART_REPL']]\n",
    "\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(result_df_for_test_data)\n",
    "\n",
    "\n",
    "# Save the resulting dataframe to a CSV file\n",
    "result_df_for_test_data.to_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/claims_result_for_test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88570187-4b5a-4a6a-9c9a-970ad3a71f2e",
   "metadata": {},
   "source": [
    "# Finding dates that are the count of clanedar_day(s) back from date 31/12/2021, for which there is a matching claim of part replacement exist and is registered for this VIN in train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a277af-c217-46e8-a8b8-cadbf1de6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DTC file (sample for DTC 75)\n",
    "cleaned_resultedData_path = '/storage/home/yqf5148/work/volvoPennState/data/dataset/cleanedNormalized_resultedData.csv'\n",
    "all_columns_names = ['VIN','ENGINE_SIZE','ENGINE_HP','VEH_TYPE']+[s for s in df_population.columns if 'KOLA' in s] + calculated_features\n",
    "\n",
    "# Read the cleaned CSV file into a DataFrame\n",
    "df_resultedData = pd.read_csv(cleaned_resultedData_path, header=None, names=all_columns_names, index_col=False, dtype='unicode')\n",
    "\n",
    "# Load the replacement claims data\n",
    "df_claims_on_train_data = pd.read_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/claims_result_for_train_data.csv')\n",
    "\n",
    "# Convert date columns to datetime\n",
    "df_claims_on_train_data['CLAIM_REG_DATE'] = pd.to_datetime(df_claims_on_train_data['CLAIM_REG_DATE'])\n",
    "# Convert 'CALENDAR_DAY' in resultedData to int if not already\n",
    "df_claims_on_train_data['DAYS_BACK'] = df_claims_on_train_data['DAYS_BACK'].astype(int)\n",
    "df_resultedData['calendar_day'] = df_resultedData['calendar_day'].astype(int)\n",
    "\n",
    "# Initialize a list to store matching results\n",
    "matching_results = []\n",
    "\n",
    "# Iterate over each row in df_claims_on_train_data\n",
    "i=0\n",
    "for _, claim_row in df_claims_on_train_data.iterrows():\n",
    "    vin = claim_row['VIN']\n",
    "    print(\"VIN in training: \")\n",
    "    print(vin)\n",
    "    days_back = claim_row['DAYS_BACK']\n",
    "    i = i+1\n",
    "    print(\"{0}-days_back for this VIN: {1}\".format(i, days_back))\n",
    "    \n",
    "    \n",
    "    # Filter resultedData for the current VIN\n",
    "    vin_matches = df_resultedData[df_resultedData['VIN'] == vin]\n",
    "    # display(vin_matches)\n",
    "    \n",
    "    matching_vin_matches = vin_matches[vin_matches['calendar_day'] == days_back]\n",
    "\n",
    "    if not matching_vin_matches.empty:\n",
    "        matching_results.append({'VIN': vin, 'CLAIM_JOB_ID': claim_row['CLAIM_JOB_ID'], 'CLAIM_REG_DATE': claim_row['CLAIM_REG_DATE'], 'TOT_CLAIM_PAYMENT_USD': claim_row['TOT_CLAIM_PAYMENT_USD'], 'DAYS_BACK': claim_row['DAYS_BACK'], 'CLAIM_TYPE': claim_row['CLAIM_TYPE'], 'CLUSTER': claim_row['CLUSTER'], 'PART_REPL': claim_row['PART_REPL']})\n",
    "\n",
    "\n",
    "\n",
    "# Convert the list of matching results to a DataFrame\n",
    "matching_results_for_train_df = pd.DataFrame(matching_results)\n",
    "\n",
    "# Display the matching results\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Matching VIN, CLAIM_JOB_ID, DAYS_BACK and CLAIM_REG_DATE\", dataframe=matching_results_for_train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4519beae-5fc2-456c-abac-cf54279543b1",
   "metadata": {},
   "source": [
    "# Finding dates that are the count of clanedar_day(s) back from date 31/12/2021, for which there is a matching claim of part replacement exist and is registered for this VIN in test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fd868-03bc-4d36-b000-1135428d7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DTC file (sample for DTC 75)\n",
    "cleaned_resultedData_path = '/storage/home/yqf5148/work/volvoPennState/data/dataset/cleanedNormalized_resultedData.csv'\n",
    "all_columns_names = ['VIN','ENGINE_SIZE','ENGINE_HP','VEH_TYPE']+[s for s in df_population.columns if 'KOLA' in s] + calculated_features\n",
    "\n",
    "# Read the cleaned CSV file into a DataFrame\n",
    "df_resultedData = pd.read_csv(cleaned_resultedData_path, header=None, names=all_columns_names, index_col=False, dtype='unicode')\n",
    "\n",
    "# Load the replacement claims data\n",
    "df_claims_on_test_data = pd.read_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/claims_result_for_test_data.csv')\n",
    "\n",
    "# Convert date columns to datetime\n",
    "df_claims_on_test_data['CLAIM_REG_DATE'] = pd.to_datetime(df_claims_on_test_data['CLAIM_REG_DATE'])\n",
    "# Convert 'CALENDAR_DAY' in resultedData to int if not already\n",
    "df_claims_on_test_data['DAYS_BACK'] = df_claims_on_test_data['DAYS_BACK'].astype(int)\n",
    "\n",
    "df_resultedData['calendar_day'] = df_resultedData['calendar_day'].astype(int)\n",
    "\n",
    "# Initializ e a list to store matching results\n",
    "matching_results = []\n",
    "i=0\n",
    "# Iterate over each row in df_claims_on_train_data\n",
    "for _, claim_row in df_claims_on_test_data.iterrows():\n",
    "    vin = claim_row['VIN']\n",
    "    print(\"VIN in training: \")\n",
    "    print(vin)\n",
    "    days_back = claim_row['DAYS_BACK']\n",
    "    i = i+1\n",
    "    print(\"{0}-days_back for this VIN: {1}\".format(i, days_back))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Filter resultedData for the current VIN\n",
    "    vin_matches = df_resultedData[df_resultedData['VIN'] == vin]\n",
    "    # display(vin_matches)\n",
    "    \n",
    "    matching_vin_matches = vin_matches[vin_matches['calendar_day'] == days_back]\n",
    "\n",
    "    if not matching_vin_matches.empty:\n",
    "        matching_results.append({'VIN': vin, 'CLAIM_JOB_ID': claim_row['CLAIM_JOB_ID'], 'CLAIM_REG_DATE': claim_row['CLAIM_REG_DATE'], 'TOT_CLAIM_PAYMENT_USD': claim_row['TOT_CLAIM_PAYMENT_USD'], 'DAYS_BACK': claim_row['DAYS_BACK'], 'CLAIM_TYPE': claim_row['CLAIM_TYPE'], 'CLUSTER': claim_row['CLUSTER'], 'PART_REPL': claim_row['PART_REPL']})\n",
    "\n",
    "\n",
    "\n",
    "# Convert the list of matching results to a DataFrame\n",
    "matching_results_for_test_df = pd.DataFrame(matching_results)\n",
    "\n",
    "# Display the matching results\n",
    "\n",
    "tools.display_dataframe_to_user(name=\"Matching VIN, CLAIM_JOB_ID, DAYS_BACK and CLAIM_REG_DATE\", dataframe=matching_results_for_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a88808-033b-44b8-bf3e-554a0caf8797",
   "metadata": {},
   "source": [
    "# Explanation: for a matched row in matching_results_for_train_df, I want to look into that VIN's information in the df_resultedData for a duration of 30 days back from this matched date (calendar_day shows the certain days back from 31/12/2021. for example, if calendar_day is 10 this date is 21/12/2021 and we need to look back at the duration of 30 days from this date, which would be the time duration between dates 21/11/2021 and 21/12/2021. For this time duration, and to do the task of capturing the (degradation) pattern for faulty operation signals, we need to capture the pattern of calculated features from column f_1_dtc38_1th_15d to column f_8_dtc2457_2nd_15d (128 columns). We will then need to compare this captured pattern of these 128 signals with a similar VIN from the test_df that is in the same cluster with this VIN number that is from train_df. So, the captured 128 signals for this duration of 30 days back from that 31/12/2021 - calendar_day (31/12/2021 - 10 = 21/12/2021 in the above example) will later be used as an index for degradation pattern of monitored elements in the truck engine. (these changes of the degradation pattern are shown through the changes of these 128 signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55b378-e0d6-48ce-8e26-bf1b6da76bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize a list to store degradation patterns for matched VINs\n",
    "degradation_patterns = []\n",
    "\n",
    "# Iterate over each row in matching_results_for_train_df\n",
    "for _, match in matching_results_for_train_df.iterrows():\n",
    "    vin = match['VIN']\n",
    "    claim_reg_date = match['CLAIM_REG_DATE']\n",
    "    days_back = match['DAYS_BACK']\n",
    "\n",
    "    # Find the calendar_day corresponding to this VIN in the matched row\n",
    "    calculatedFeaturesFromResultedData_vin_matches = df_resultedData[df_resultedData['VIN'] == vin]\n",
    "    calendar_day_match = calculatedFeaturesFromResultedData_vin_matches[calculatedFeaturesFromResultedData_vin_matches['calendar_day'] == days_back]\n",
    "\n",
    "    if not calendar_day_match.empty:\n",
    "        # Step 2: Define the 30-day window for degradation pattern\n",
    "        target_day = int(calendar_day_match.iloc[0]['calendar_day'])\n",
    "        start_day = target_day + 30  # 30 days back\n",
    "        end_day = target_day         # Ending on the matched day\n",
    "\n",
    "        # Filter for the 30-day window in df_resultedData\n",
    "        calculatedFeaturesFromResultedData_vin_matches_30_day_window = calculatedFeaturesFromResultedData_vin_matches[\n",
    "            (calculatedFeaturesFromResultedData_vin_matches['calendar_day'] >= end_day) &\n",
    "            (calculatedFeaturesFromResultedData_vin_matches['calendar_day'] <= start_day)\n",
    "        ]\n",
    "        \n",
    "        if not calculatedFeaturesFromResultedData_vin_matches_30_day_window.empty:\n",
    "            # Step 3: Define the feature columns and create the structured DataFrame\n",
    "            feature_columns = ['calendar_day'] + \\\n",
    "                              [col for col in df_resultedData.columns if col.startswith('f_')] + \\\n",
    "                              [col for col in df_resultedData.columns if col.endswith('_parts')]\n",
    "\n",
    "            # Extract the relevant columns and convert to DataFrame\n",
    "            vin_degradation_pattern = calculatedFeaturesFromResultedData_vin_matches_30_day_window[feature_columns]\n",
    "            vin_degradation_pattern_df = pd.DataFrame(vin_degradation_pattern.values, columns=feature_columns)\n",
    "\n",
    "            # Store the degradation pattern along with VIN and claim details\n",
    "            degradation_patterns.append({\n",
    "                'VIN': vin,\n",
    "                'CLAIM_JOB_ID': match['CLAIM_JOB_ID'],\n",
    "                'CLAIM_REG_DATE': claim_reg_date,\n",
    "                'CLAIM_TYPE': match['CLAIM_TYPE'],\n",
    "                'CLUSTER': match['CLUSTER'],\n",
    "                'PATTERN': vin_degradation_pattern_df  # Store as structured DataFrame\n",
    "            })\n",
    "\n",
    "# Step 4: Create a DataFrame from the collected patterns\n",
    "degradation_patterns_df_from_train_data = pd.DataFrame(degradation_patterns)\n",
    "\n",
    "# Display the patterns to the user\n",
    "tools.display_dataframe_to_user(name=\"Degradation Patterns for 30-Day Window\", dataframe=degradation_patterns_df_from_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531dd16-df1f-4ef5-b6c1-14bddaf67db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Step 0: Load degradation patterns from degradation_patterns_df_from_train_data and identify unique clusters\n",
    "clusters = degradation_patterns_df_from_train_data['CLUSTER'].unique()\n",
    "models_by_cluster = {}  # Dictionary to store models for each cluster\n",
    "\n",
    "# Step 1: Iterate over each unique cluster\n",
    "for cluster in clusters:\n",
    "    # Filter degradation patterns for this specific cluster\n",
    "    cluster_data = degradation_patterns_df_from_train_data[degradation_patterns_df_from_train_data['CLUSTER'] == cluster]\n",
    "    \n",
    "    # Initialize lists to collect data from all VINs in this cluster\n",
    "    X_cluster = []\n",
    "    y_cluster = []\n",
    "    \n",
    "    # Identify unique VINs within the cluster\n",
    "    vins = cluster_data['VIN'].unique()\n",
    "    \n",
    "    for vin in vins:\n",
    "        # Filter data for this specific VIN within the cluster\n",
    "        vin_data = cluster_data[cluster_data['VIN'] == vin]\n",
    "\n",
    "        # Extract features (X) and targets (y) from the 'PATTERN' column in vin_data\n",
    "        # Assuming 'PATTERN' is a DataFrame itself with feature columns (_f) and target columns (_parts)\n",
    "        for _, pattern_row in vin_data.iterrows():\n",
    "            vin_pattern_df = pattern_row['PATTERN']  # Extract the DataFrame stored in 'PATTERN' column\n",
    "\n",
    "            # Separate features and target within the 30-day degradation pattern for this VIN\n",
    "            X_vin = vin_pattern_df[[col for col in vin_pattern_df.columns if col.startswith('f_')]]\n",
    "            y_vin = vin_pattern_df[[col for col in vin_pattern_df.columns if col.endswith('_parts')]].values.ravel()  # Flatten for model input\n",
    "\n",
    "            # Append data to the cluster lists\n",
    "            X_cluster.append(X_vin)\n",
    "            y_cluster.append(y_vin)\n",
    "    \n",
    "    # Concatenate all VIN data within this cluster\n",
    "    X_cluster = pd.concat(X_cluster)\n",
    "    y_cluster = pd.concat([pd.Series(y) for y in y_cluster])  # Convert each y array to Series and concatenate\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_cluster, y_cluster, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the model for this cluster\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Optional: Evaluate model on validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"Cluster {cluster} - Validation Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Store the trained model for this cluster\n",
    "    models_by_cluster[cluster] = model\n",
    "\n",
    "print(\"Training completed for all clusters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99516fa-8dfc-4e4a-893a-bb5804808968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 0: Read sorted_VINs_by_similarity dataframe\n",
    "# sorted_VINs_by_similarity = pd.read_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/sorted_VINs_by_similarity.csv', sep=',', header=None)\n",
    "\n",
    "# # Step 1: Initialize a list to store comparison results\n",
    "# comparison_results = []\n",
    "\n",
    "# # Iterate over each VIN in the test set (matching_results_for_test_df)\n",
    "# for _, test_vin_row in matching_results_for_test_df.iterrows():\n",
    "#     vin_a = test_vin_row['VIN']\n",
    "#     vin_a_cluster = test_vin_row['CLUSTER']\n",
    "    \n",
    "#     # Extract VIN A's degradation patterns from the 30-day window (from resultedData)\n",
    "#     vin_a_degradation_patterns = df_resultedData[df_resultedData['VIN'] == vin_a]\n",
    "    \n",
    "#     # Retrieve the row from sorted_VINs_by_similarity where column 1 matches VIN A (VIN from test data)\n",
    "#     similar_vins_row = sorted_VINs_by_similarity[sorted_VINs_by_similarity.iloc[:, 1] == vin_a]\n",
    "    \n",
    "#     if similar_vins_row.empty:\n",
    "#         # If no similar VINs are found, skip to the next iteration\n",
    "#         continue\n",
    "    \n",
    "#     # Extract the similar VINs (all columns from 2 onwards in the sorted_VINs_by_similarity dataframe)\n",
    "#     similar_vins = similar_vins_row.iloc[0, 2:].dropna().values.tolist()  # Get list of similar VINs in descending order of similarity\n",
    "#     print(\"Similar VINs in train data to {0} are\".format(vin_a))\n",
    "#     print(similar_vins)\n",
    "#     # Iterate over the similar VINs (from train data)\n",
    "#     for similar_vin in similar_vins:\n",
    "#         # Retrieve the degradation patterns for this similar VIN\n",
    "#         similar_vin_degradation_patterns = df_resultedData[df_resultedData['VIN'] == similar_vin]\n",
    "\n",
    "#         # Iterate over each day, going back one day at a time to form 30-day windows for comparison\n",
    "#         for day in range(1, len(similar_vin_degradation_patterns) - 30):\n",
    "#             # Define the 30-day window for VIN A (from test data)\n",
    "#             vin_a_30_day_window = vin_a_degradation_patterns[(vin_a_degradation_patterns['calendar_day'] >= day) &\n",
    "#                                                             (vin_a_degradation_patterns['calendar_day'] <= day + 30)]\n",
    "\n",
    "#             # Define the corresponding 30-day window for the similar VIN from train data\n",
    "#             similar_vin_30_day_window = similar_vin_degradation_patterns[(similar_vin_degradation_patterns['calendar_day'] >= day) &\n",
    "#                                                                          (similar_vin_degradation_patterns['calendar_day'] <= day + 30)]\n",
    "            \n",
    "#             if not vin_a_30_day_window.empty and not similar_vin_30_day_window.empty:\n",
    "#                 # Step 2: Perform pattern comparison between VIN A and the similar VIN\n",
    "#                 comparison_result = {\n",
    "#                     'VIN_A': vin_a,\n",
    "#                     'Similar_VIN': similar_vin,\n",
    "#                     'Cluster': vin_a_cluster,\n",
    "#                     'Day': day,\n",
    "#                     'VIN_A_Pattern': vin_a_30_day_window,\n",
    "#                     'Similar_VIN_Pattern': similar_vin_30_day_window,\n",
    "#                 }\n",
    "\n",
    "#                 # Append the comparison result to the list\n",
    "#                 comparison_results.append(comparison_result)\n",
    "\n",
    "# # Step 3: Store the comparison results in a dataframe for further analysis or display\n",
    "# comparison_results_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "# # Display the comparison results dataframe\n",
    "# tools.display_dataframe_to_user(name=\"VIN Comparison Results\", dataframe=comparison_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da08b0-0ec5-40ea-b323-dbd5ca3e5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import IsolationForest\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import joblib\n",
    "\n",
    "# # Dictionary to store models for each cluster\n",
    "# cluster_models = {}\n",
    "\n",
    "# # Iterate over each unique cluster\n",
    "# for cluster in degradation_patterns_df['CLUSTER'].unique():\n",
    "#     # Filter data for the current cluster\n",
    "#     cluster_data = degradation_patterns_df[degradation_patterns_df['CLUSTER'] == cluster]\n",
    "    \n",
    "#     # Extract degradation patterns and standardize them\n",
    "#     patterns = []\n",
    "#     for _, row in cluster_data.iterrows():\n",
    "#         pattern = row['PATTERN']  # This should be a DataFrame or Series of features\n",
    "#         patterns.append(pattern.values.flatten())  # Flattening to get feature vector\n",
    "\n",
    "#     # Standardize patterns\n",
    "#     scaler = StandardScaler()\n",
    "#     patterns_scaled = scaler.fit_transform(patterns)\n",
    "    \n",
    "#     # Train the Isolation Forest model on scaled patterns\n",
    "#     model = IsolationForest(random_state=42)\n",
    "#     model.fit(patterns_scaled)\n",
    "    \n",
    "#     # Store the model and scaler for this cluster\n",
    "#     cluster_models[cluster] = {'model': model, 'scaler': scaler}\n",
    "    \n",
    "#     # Optionally save the model and scaler using joblib\n",
    "#     joblib.dump(model, f'cluster_{cluster}_isolation_forest_model.joblib')\n",
    "#     joblib.dump(scaler, f'cluster_{cluster}_scaler.joblib')\n",
    "\n",
    "# print(\"Models trained and saved for each cluster.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec80d06-8ed6-45c6-8f94-df5a3138285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate over each VIN in degradation_patterns_df\n",
    "# for _, pattern_data in degradation_patterns_df.iterrows():\n",
    "#     vin = pattern_data['VIN']\n",
    "#     cluster = pattern_data['CLUSTER']\n",
    "#     claim_type = pattern_data['CLAIM_TYPE']\n",
    "#     pattern_df = pattern_data['PATTERN']\n",
    "    \n",
    "#     # Create a new figure for the merged plot\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "    \n",
    "#     # Iterate over each 'f_' feature column\n",
    "#     feature_columns = [col for col in pattern_df.columns if col.startswith('f_')]\n",
    "#     for feature in feature_columns:\n",
    "#         # Plot each feature on the same axes\n",
    "#         plt.plot(pattern_df['calendar_day'], pattern_df[feature], marker='o', label=feature)\n",
    "    \n",
    "#     # Set plot title and labels\n",
    "#     plt.title(f'Degradation Patterns for All Features (VIN: {vin}, CLUSTER: {cluster}, CLAIM_TYPE: {claim_type})')\n",
    "#     plt.xlabel('Calendar Day')\n",
    "#     plt.ylabel('Feature Value')\n",
    "#     plt.gca().invert_xaxis()  # Optional: Invert x-axis to show days back from the reference date\n",
    "    \n",
    "#     # Add a legend to differentiate the features\n",
    "#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "#     # Show the plot\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b2871-1899-4eb1-ae02-336db1b9b411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volvopennstate_new_kernel",
   "language": "python",
   "name": "volvopennstate_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
