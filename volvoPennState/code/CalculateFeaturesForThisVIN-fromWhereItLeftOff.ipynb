{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d27272c-55f1-48ed-b3bd-445ce73244d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
    "%env PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "302060b9-b23b-4dab-8fe9-4d92a71d0ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark as psk\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import time as t \n",
    "from datetime import date, datetime, timedelta\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import reduce  \n",
    "from math import modf\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from delta import * \n",
    "from delta.tables import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from IPython import get_ipython\n",
    "\n",
    "from filelock import FileLock\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "conf = (SparkConf().set(\"spark.driver.maxResultSize\", \"4g\"))\n",
    "\n",
    "# Create new context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# sc = SparkContext(\"local\", \"Simple App\")\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "       .master(\"local[2]\") \\\n",
    "       .appName(\"test\") \\\n",
    "       .config(\"spark.driver.maxResultSize\", \"20g\")\\\n",
    "       .config(\"spark.driver.memory\", \"100g\")\\\n",
    "       .getOrCreate()\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")    #To resolve the error for p1075_38 to_timestamp formating: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '1/2/2019 20:40:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n",
    "# Increase the max fields in the string representation of a plan\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)  # Increase to 1000 or more as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf20759e-f530-4958-ae73-6e7847f443be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data_cleaning(dtc_type):\n",
    "    df = spark.sql(\"SELECT * FROM {} WHERE _C0 IS NOT NULL AND _C0 != 'NA' AND _C0 != '' AND MESSAGE_ID IS NOT NULL AND MESSAGE_ID != '' AND MESSAGE_ID != 'NA' AND FAULT_DATE_TIME IS NOT NULL AND FAULT_DATE_TIME != '' AND FAULT_DATE_TIME != 'NA' AND FAULT_STATUS IS NOT NULL AND FAULT_STATUS != '' AND FAULT_STATUS != 'NA' AND VIN IS NOT NULL AND VIN != '' AND VIN != 'NA' AND length(VIN) !< 17\".format(dtc_type)) \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d57ba81-4151-4ad1-8951-9bbfe610f092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read CSV file into table\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv\") \\\n",
    "          .createOrReplaceTempView(\"VINs_data\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/PopulationWithChassisId.csv\") \\\n",
    "          .createOrReplaceTempView(\"population\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/CCA Claims.csv\") \\\n",
    "          .createOrReplaceTempView(\"cca_claims\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/EGR Cooler Claims.csv\") \\\n",
    "          .createOrReplaceTempView(\"egr_cooler_claims\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/EGR FG 293 Claims.csv\") \\\n",
    "          .createOrReplaceTempView(\"egr_fg_293_claims\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/EGR Sensors.csv\") \\\n",
    "          .createOrReplaceTempView(\"egr_sensors\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/Engine_Emissions_Table1.csv\") \\\n",
    "          .createOrReplaceTempView(\"engine_emissions_table1\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/Engine_Emissions_Table2.csv\") \\\n",
    "          .createOrReplaceTempView(\"engine_emissions_table2\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_38.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_38\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_75.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_75\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_77.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_77\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_86.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_86\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_92.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_92\")\n",
    " \n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_94.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_94\")   #Adding .option(\"skipRows\", range(2784329,2784830)) does not work for skipping rows for pyspark dataframe\n",
    "\n",
    "# dataFrame = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"skipRows\", range(2784329,2784830)) \\\n",
    "#           .load(\"P1075_94.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = pd.read_csv('P1075_94.csv',skiprows=list(range(2784329,2784830)),low_memory=False)   list[range(2784329,2784830)]  works to skip row for pandas dataframe\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P0401_faults.csv\") \\\n",
    "          .createOrReplaceTempView(\"p0401_faults\")\n",
    "\n",
    "spark.read.option(\"header\",True,) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P2457_faults.csv\") \\\n",
    "          .createOrReplaceTempView(\"p2457_faults\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e0fae1-0016-4b39-a8c2-2ddc41d1a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population = spark.sql(\"SELECT * FROM population\")\n",
    "\n",
    "df_p1075_38 = raw_data_cleaning('p1075_38')\n",
    "\n",
    "df_p1075_75 = raw_data_cleaning('p1075_75')\n",
    "\n",
    "df_p1075_77 = raw_data_cleaning('p1075_77')\n",
    "\n",
    "df_p1075_86 = raw_data_cleaning('p1075_86')\n",
    "\n",
    "df_p1075_92 = raw_data_cleaning('p1075_92')\n",
    "\n",
    "df_p1075_94 = raw_data_cleaning('p1075_94')\n",
    "\n",
    "df_p0401 = raw_data_cleaning('p0401_faults')\n",
    "\n",
    "df_p2457 = raw_data_cleaning('p2457_faults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c146b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headerList = [\"VIN\",  \n",
    "          \"calendar_day\", \n",
    "          \"f_1_dtc38_1th_15d\", \n",
    "          \"f_1_dtc38_2nd_15d\", \n",
    "          \"f_2_dtc38_1th_15d\", \n",
    "          \"f_2_dtc38_2nd_15d\", \n",
    "          \"f_3_dtc38_1th_15d\", \n",
    "          \"f_3_dtc38_2nd_15d\", \n",
    "          \"f_4_dtc38_1th_15d\", \n",
    "          \"f_4_dtc38_2nd_15d\", \n",
    "          \"f_5_dtc38_1th_15d\", \n",
    "          \"f_5_dtc38_2nd_15d\", \n",
    "          \"f_6_dtc38_1th_15d\", \n",
    "          \"f_6_dtc38_2nd_15d\", \n",
    "          \"f_7_dtc38_1th_15d\", \n",
    "          \"f_7_dtc38_2nd_15d\", \n",
    "          \"f_8_dtc38_1th_15d\", \n",
    "          \"f_8_dtc38_2nd_15d\", \n",
    "\n",
    "          \"f_1_dtc75_1th_15d\", \n",
    "          \"f_1_dtc75_2nd_15d\", \n",
    "          \"f_2_dtc75_1th_15d\", \n",
    "          \"f_2_dtc75_2nd_15d\", \n",
    "          \"f_3_dtc75_1th_15d\", \n",
    "          \"f_3_dtc75_2nd_15d\", \n",
    "          \"f_4_dtc75_1th_15d\", \n",
    "          \"f_4_dtc75_2nd_15d\", \n",
    "          \"f_5_dtc75_1th_15d\", \n",
    "          \"f_5_dtc75_2nd_15d\", \n",
    "          \"f_6_dtc75_1th_15d\", \n",
    "          \"f_6_dtc75_2nd_15d\", \n",
    "          \"f_7_dtc75_1th_15d\", \n",
    "          \"f_7_dtc75_2nd_15d\", \n",
    "          \"f_8_dtc75_1th_15d\", \n",
    "          \"f_8_dtc75_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc77_1th_15d\", \n",
    "          \"f_1_dtc77_2nd_15d\", \n",
    "          \"f_2_dtc77_1th_15d\", \n",
    "          \"f_2_dtc77_2nd_15d\", \n",
    "          \"f_3_dtc77_1th_15d\", \n",
    "          \"f_3_dtc77_2nd_15d\", \n",
    "          \"f_4_dtc77_1th_15d\", \n",
    "          \"f_4_dtc77_2nd_15d\", \n",
    "          \"f_5_dtc77_1th_15d\", \n",
    "          \"f_5_dtc77_2nd_15d\", \n",
    "          \"f_6_dtc77_1th_15d\", \n",
    "          \"f_6_dtc77_2nd_15d\", \n",
    "          \"f_7_dtc77_1th_15d\", \n",
    "          \"f_7_dtc77_2nd_15d\", \n",
    "          \"f_8_dtc77_1th_15d\", \n",
    "          \"f_8_dtc77_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc86_1th_15d\", \n",
    "          \"f_1_dtc86_2nd_15d\", \n",
    "          \"f_2_dtc86_1th_15d\", \n",
    "          \"f_2_dtc86_2nd_15d\", \n",
    "          \"f_3_dtc86_1th_15d\", \n",
    "          \"f_3_dtc86_2nd_15d\", \n",
    "          \"f_4_dtc86_1th_15d\", \n",
    "          \"f_4_dtc86_2nd_15d\", \n",
    "          \"f_5_dtc86_1th_15d\", \n",
    "          \"f_5_dtc86_2nd_15d\", \n",
    "          \"f_6_dtc86_1th_15d\", \n",
    "          \"f_6_dtc86_2nd_15d\", \n",
    "          \"f_7_dtc86_1th_15d\", \n",
    "          \"f_7_dtc86_2nd_15d\", \n",
    "          \"f_8_dtc86_1th_15d\", \n",
    "          \"f_8_dtc86_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc92_1th_15d\", \n",
    "          \"f_1_dtc92_2nd_15d\", \n",
    "          \"f_2_dtc92_1th_15d\", \n",
    "          \"f_2_dtc92_2nd_15d\", \n",
    "          \"f_3_dtc92_1th_15d\", \n",
    "          \"f_3_dtc92_2nd_15d\", \n",
    "          \"f_4_dtc92_1th_15d\", \n",
    "          \"f_4_dtc92_2nd_15d\", \n",
    "          \"f_5_dtc92_1th_15d\", \n",
    "          \"f_5_dtc92_2nd_15d\", \n",
    "          \"f_6_dtc92_1th_15d\", \n",
    "          \"f_6_dtc92_2nd_15d\", \n",
    "          \"f_7_dtc92_1th_15d\", \n",
    "          \"f_7_dtc92_2nd_15d\", \n",
    "          \"f_8_dtc92_1th_15d\", \n",
    "          \"f_8_dtc92_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc94_1th_15d\", \n",
    "          \"f_1_dtc94_2nd_15d\", \n",
    "          \"f_2_dtc94_1th_15d\", \n",
    "          \"f_2_dtc94_2nd_15d\", \n",
    "          \"f_3_dtc94_1th_15d\", \n",
    "          \"f_3_dtc94_2nd_15d\", \n",
    "          \"f_4_dtc94_1th_15d\", \n",
    "          \"f_4_dtc94_2nd_15d\", \n",
    "          \"f_5_dtc94_1th_15d\", \n",
    "          \"f_5_dtc94_2nd_15d\", \n",
    "          \"f_6_dtc94_1th_15d\", \n",
    "          \"f_6_dtc94_2nd_15d\", \n",
    "          \"f_7_dtc94_1th_15d\", \n",
    "          \"f_7_dtc94_2nd_15d\", \n",
    "          \"f_8_dtc94_1th_15d\", \n",
    "          \"f_8_dtc94_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc0401_1th_15d\", \n",
    "          \"f_1_dtc0401_2nd_15d\", \n",
    "          \"f_2_dtc0401_1th_15d\", \n",
    "          \"f_2_dtc0401_2nd_15d\", \n",
    "          \"f_3_dtc0401_1th_15d\", \n",
    "          \"f_3_dtc0401_2nd_15d\", \n",
    "          \"f_4_dtc0401_1th_15d\", \n",
    "          \"f_4_dtc0401_2nd_15d\", \n",
    "          \"f_5_dtc0401_1th_15d\", \n",
    "          \"f_5_dtc0401_2nd_15d\", \n",
    "          \"f_6_dtc0401_1th_15d\", \n",
    "          \"f_6_dtc0401_2nd_15d\", \n",
    "          \"f_7_dtc0401_1th_15d\", \n",
    "          \"f_7_dtc0401_2nd_15d\", \n",
    "          \"f_8_dtc0401_1th_15d\", \n",
    "          \"f_8_dtc0401_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc2457_1th_15d\", \n",
    "          \"f_1_dtc2457_2nd_15d\", \n",
    "          \"f_2_dtc2457_1th_15d\", \n",
    "          \"f_2_dtc2457_2nd_15d\", \n",
    "          \"f_3_dtc2457_1th_15d\", \n",
    "          \"f_3_dtc2457_2nd_15d\", \n",
    "          \"f_4_dtc2457_1th_15d\", \n",
    "          \"f_4_dtc2457_2nd_15d\", \n",
    "          \"f_5_dtc2457_1th_15d\", \n",
    "          \"f_5_dtc2457_2nd_15d\", \n",
    "          \"f_6_dtc2457_1th_15d\", \n",
    "          \"f_6_dtc2457_2nd_15d\", \n",
    "          \"f_7_dtc2457_1th_15d\", \n",
    "          \"f_7_dtc2457_2nd_15d\", \n",
    "          \"f_8_dtc2457_1th_15d\", \n",
    "          \"f_8_dtc2457_2nd_15d\",\n",
    "\n",
    "          \"if_parts_replaced_in_1th_15d\", \n",
    "          \"if_parts_replaced_in_2nd_15d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1b2a1a-509d-4d3e-9c39-89e306d8961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df38 = spark.read.option(\"header\",True).csv(\"P1075_38.csv\")\n",
    "# df75 = spark.read.option(\"header\",True).csv(\"P1075_75.csv\")\n",
    "# df77 = spark.read.option(\"header\",True).csv(\"P1075_77.csv\")\n",
    "# df86 = spark.read.option(\"header\",True).csv(\"P1075_86.csv\")\n",
    "# df92 = spark.read.option(\"header\",True).csv(\"P1075_92.csv\")\n",
    "# df94 = spark.read.option(\"header\",True).csv(\"P1075_94.csv\")\n",
    "# df0401 = spark.read.option(\"header\",True).csv(\"P0401_faults.csv\")\n",
    "# df2457 = spark.read.option(\"header\",True).csv(\"P2457_faults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ffc862c-a44d-4d09-894b-47a2f4e23936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtc_type=\"p1075_38\"    \n",
    "# df38.select(\"VIN\", f.to_timestamp(f.concat(lit(\"1/1/2019 20:40\"), lit(\":00\")), \"M/d/yyyy HH:mm:ss\").alias(\"FAULT_DATE_TIME\"), \"FAULT_STATUS\").show()\n",
    "# df38.select(\"VIN\", f.to_timestamp(f.concat(df38.FAULT_DATE_TIME, lit(\":00\")), \"M/d/yyyy HH:mm:ss\").alias(\"FAULT_DATE_TIME\"), \"FAULT_STATUS\").show()\n",
    "# df38.select(\"VIN\", (fix_problem_of_fault_date_time_with_no_seconds(df38, dtc_type)).alias(\"FAULT_DATE_TIME_2\"), \"FAULT_STATUS\").show()\n",
    "# df38.select(\"VIN\", f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), \"M/d/yyyy HH:mm:ss\").alias(\"FAULT_DATE_TIME\"), \"FAULT_STATUS\").show()\n",
    "# df75.select(\"VIN\", f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), \"yyyy-MM-dd HH:mm:ss\"), \"FAULT_STATUS\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf5e13-2336-48ce-ad2b-a04c0f987935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_problem_of_fault_date_time_with_no_seconds(df, dtc_type):\n",
    "    if dtc_type==\"p1075_38\":\n",
    "       return f.to_timestamp(f.concat(df.FAULT_DATE_TIME, lit(\":00\")), \"M/d/yyyy HH:mm:ss\")\n",
    "    else:\n",
    "       return f.to_timestamp(df.FAULT_DATE_TIME, \"yyyy-MM-dd HH:mm:ss\")\n",
    "\n",
    "\n",
    "def partIsReplacedForVINs(df_claims, start_date_str, end_date_str, previous_15day_duration_start_date_str, previous_15day_duration_end_date_str):\n",
    "    df_temp_1 = df_claims.filter((f.col('CLAIM_REG_DATE') > end_date_str) & (f.col('CLAIM_REG_DATE') < start_date_str) & (f.col('TOT_CLAIM_PAYMENT_USD') > 1000.0))\\\n",
    "                                          .withColumn('part_replacement',f.lit('first_15_days_back'))\n",
    "    df_temp_2 = df_claims.filter((f.col('CLAIM_REG_DATE') > previous_15day_duration_end_date_str) & (f.col('CLAIM_REG_DATE') < previous_15day_duration_start_date_str) & (f.col('TOT_CLAIM_PAYMENT_USD') > 1000.0))\\\n",
    "                                          .withColumn('part_replacement',f.lit('another_15_days_before_first_15d'))\n",
    "    \n",
    "    df_temp = df_temp_1.unionByName(df_temp_2)\n",
    "    \n",
    "    df_part_repl = df_temp.groupBy('VIN').pivot('part_replacement',['first_15_days_back','another_15_days_before_first_15d']).count().fillna(0)\n",
    "    return df_part_repl\n",
    "\n",
    "\n",
    "def feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN, dtc_status_to_calculate_this_feature):\n",
    "      \n",
    "    df_dtc_type_and_status_for_this_VIN = df_dtc_type_for_this_VIN.filter(f.col('FAULT_STATUS') == dtc_status_to_calculate_this_feature)\n",
    "\n",
    "    return df_dtc_type_and_status_for_this_VIN.count()\n",
    "\n",
    "\n",
    "def feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN, dtc_type, dtc_status_to_calculate_this_feature):\n",
    "\n",
    "    fault_date_time_format='yyyy-MM-dd HH:mm:ss'\n",
    "    if dtc_type==\"p1075_38\":\n",
    "        fault_date_time_format='M/d/yyyy HH:mm'\n",
    "\n",
    "    dtcs_with_all_statuses_df = df_dtc_type_for_this_VIN.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format))\n",
    "\n",
    "\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df\n",
    "\n",
    "#     PsSpark: This one is also true to access the first row of pyspark dataframe    \n",
    "#     headRowId = dtcs_with_all_statuses_df2.first()[0]\n",
    "\n",
    "    tailRowId = dtcs_with_all_statuses_df.tail(1)[0][0]\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.filter(f.col('_c0') != tailRowId).fillna(0)\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format)).withColumn(\"_c0\",monotonically_increasing_id())\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.dropna()\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.withColumnRenamed(\"FAULT_STATUS\", \"FAULT_STATUS_CURRENT_DTC\")\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.withColumnRenamed(\"FAULT_DATE_TIME\", \"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\")\n",
    "\n",
    "    headRowId = dtcs_with_all_statuses_df2.head(1)[0][0]\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.filter(f.col('_c0') != headRowId).fillna(0)\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format)).withColumn(\"_c0\",monotonically_increasing_id())\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.dropna()\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.withColumnRenamed(\"FAULT_STATUS\", \"FAULT_ANY_STATUS_NEXT_DTC\")\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.withColumnRenamed(\"FAULT_DATE_TIME\", \"FAULT_DATE_TIME_NEXT_ANY_DTC\")\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.select([c for c in dtcs_with_all_statuses_df2.columns if c != \"VIN\"])\n",
    "\n",
    "    # create list of dataframes\n",
    "    list_df = [dtcs_with_all_statuses_df, dtcs_with_all_statuses_df2]\n",
    "\n",
    "    # merge all at once\n",
    "    my_temp_df = reduce(lambda x, y: x.join(y, on=\"_c0\"), list_df)\n",
    "    my_temp_df = my_temp_df.withColumn(\"FAULT_DATE_TIME_DIFF\", (f.unix_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), format=fault_date_time_format) - f.unix_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), format=fault_date_time_format))) \\\n",
    "    .withColumn(\"DURATION_IN_THIS_15_DAYS_BACK\", f.when((f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format) <= end_date) & \n",
    "                                                                  (f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format) >= start_date), \n",
    "                                                                  (f.unix_timestamp(\"FAULT_DATE_TIME_NEXT_ANY_DTC\", format=fault_date_time_format) - f.unix_timestamp(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\", format=fault_date_time_format)))) \\\n",
    "    .withColumn(\"COUNT_IN_THIS_15_DAYS_BACK\", f.when((f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format) <= end_date) & \n",
    "                                                                  (f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format) >= start_date), (f.lit(1)).cast(IntegerType())))\n",
    "    start_only_with_this_status_dtcs_df = my_temp_df.filter(f.col(\"FAULT_STATUS_CURRENT_DTC\") == f.lit(dtc_status_to_calculate_this_feature))\n",
    "\n",
    "\n",
    "    duration_of_specific_type_dtcs_for_this_VIN = start_only_with_this_status_dtcs_df.select(f.sum(\"DURATION_IN_THIS_15_DAYS_BACK\")).fillna(0)\n",
    "    count_of_specific_type_dtcs_for_this_VIN = start_only_with_this_status_dtcs_df.select(f.sum(\"COUNT_IN_THIS_15_DAYS_BACK\")).fillna(1)\n",
    "    duration = duration_of_specific_type_dtcs_for_this_VIN.collect()[0][0]\n",
    "    return duration\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "def feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN, dtc_type, dtc_status_to_calculate_this_feature):\n",
    "    \n",
    "    fault_date_time_format='yyyy-MM-dd HH:mm:ss'\n",
    "    if dtc_type==\"p1075_38\":\n",
    "        fault_date_time_format='M/d/yyyy HH:mm'\n",
    "\n",
    "\n",
    "    dtcs_with_all_statuses_df = df_dtc_type_for_this_VIN.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format))\n",
    "\n",
    "\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df\n",
    "\n",
    "#     PsSpark: This one is also true to access the first row of pyspark dataframe    \n",
    "    tailRowId = dtcs_with_all_statuses_df.tail(1)[0][0]\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.filter(f.col('_c0') != tailRowId).fillna(0)\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format)).withColumn(\"_c0\",monotonically_increasing_id())\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.dropna()\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.withColumnRenamed(\"FAULT_STATUS\", \"FAULT_STATUS_CURRENT_DTC\")\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.withColumnRenamed(\"FAULT_DATE_TIME\", \"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\")\n",
    "\n",
    "    headRowId = dtcs_with_all_statuses_df2.head(1)[0][0]\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.filter(f.col('_c0') != headRowId).fillna(0)\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format)).withColumn(\"_c0\",monotonically_increasing_id())\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.dropna()\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.withColumnRenamed(\"FAULT_STATUS\", \"FAULT_ANY_STATUS_NEXT_DTC\")\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.withColumnRenamed(\"FAULT_DATE_TIME\", \"FAULT_DATE_TIME_NEXT_ANY_DTC\")\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.select([c for c in dtcs_with_all_statuses_df2.columns if c != \"VIN\"])\n",
    "\n",
    "\n",
    "    # create list of dataframes\n",
    "    list_df = [dtcs_with_all_statuses_df, dtcs_with_all_statuses_df2]\n",
    "\n",
    "    # merge all at once\n",
    "    my_temp_df = reduce(lambda x, y: x.join(y, on=\"_c0\"), list_df)\n",
    "    my_temp_df = my_temp_df.withColumn(\"FAULT_DATE_TIME_DIFF\", (f.unix_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), format=fault_date_time_format) - f.unix_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), format=fault_date_time_format))) \\\n",
    "    .withColumn(\"DURATION_IN_THIS_15_DAYS_BACK\", f.when((f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format) <= end_date) & \n",
    "                                                                  (f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format) >= start_date), \n",
    "                                                                  (f.unix_timestamp(\"FAULT_DATE_TIME_NEXT_ANY_DTC\", format=fault_date_time_format) - f.unix_timestamp(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\", format=fault_date_time_format)))) \\\n",
    "    .withColumn(\"COUNT_IN_THIS_15_DAYS_BACK\", f.when((f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format) <= end_date) & \n",
    "                                                                  (f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format) >= start_date), (f.lit(1)).cast(IntegerType())))\n",
    "#     my_temp_df[\"_c0\", \"VIN\",\"FAULT_STATUS_CURRENT_DTC\", \"FAULT_ANY_STATUS_NEXT_DTC\", \"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\", \"FAULT_DATE_TIME_NEXT_ANY_DTC\", f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format), f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format)].show(194)\n",
    "    start_only_with_this_status_dtcs_df = my_temp_df.filter(f.col(\"FAULT_STATUS_CURRENT_DTC\") == f.lit(dtc_status_to_calculate_this_feature))\n",
    "\n",
    "\n",
    "\n",
    "    duration_of_specific_type_dtcs_for_this_VIN = start_only_with_this_status_dtcs_df.select(f.sum(\"DURATION_IN_THIS_15_DAYS_BACK\")).fillna(0)\n",
    "    count_of_specific_type_dtcs_for_this_VIN = start_only_with_this_status_dtcs_df.select(f.sum(\"COUNT_IN_THIS_15_DAYS_BACK\")).fillna(1)\n",
    "    duration = duration_of_specific_type_dtcs_for_this_VIN.collect()[0][0]\n",
    "\n",
    "    total_count = count_of_specific_type_dtcs_for_this_VIN.collect()[0][0]\n",
    "    if(total_count != 0): \n",
    "        average_of_specific_type_dtcs_for_this_VIN = duration/total_count\n",
    "        return average_of_specific_type_dtcs_for_this_VIN\n",
    "    else:\n",
    "        return 0.0\n",
    "  \n",
    "  \n",
    "def feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN, vehicle_speed, dtc_status_to_calculate_this_feature):\n",
    "    \n",
    "    df_dtc_type_and_status_for_this_VIN = df_dtc_type_for_this_VIN\\\n",
    "                .filter(df_dtc_type_for_this_VIN.FAULT_STATUS == dtc_status_to_calculate_this_feature)\\\n",
    "                .filter(df_dtc_type_for_this_VIN.ROAD_SPEED_MPH == vehicle_speed)\n",
    "             \n",
    "    return df_dtc_type_and_status_for_this_VIN.count()\n",
    "\n",
    "  \n",
    "def if_part_is_replaced_for_this_VIN_in_this_timespan(thisVIN, start_date, end_date):\n",
    "    # Initialize an empty list to store the results\n",
    "    replacement_records = []\n",
    "\n",
    "    fault_date_time_format = 'MM/dd/yyyy'\n",
    "    start_date = '2014-12-31'\n",
    "    end_date = '2021-12-31'\n",
    "\n",
    "    # Load all claims tables for the specific VIN\n",
    "    claims_datasets = {\n",
    "        'cca_claims': f\"SELECT VIN, CLAIM_REG_DATE, TOT_CLAIM_PAYMENT_USD FROM cca_claims WHERE VIN = '{thisVIN}'\",\n",
    "        'egr_cooler_claims': f\"SELECT VIN, CLAIM_REG_DATE, TOT_CLAIM_PAYMENT_USD FROM egr_cooler_claims WHERE VIN = '{thisVIN}'\",\n",
    "        'egr_fg_293_claims': f\"SELECT VIN, CLAIM_REG_DATE, TOT_CLAIM_PAYMENT_USD FROM egr_fg_293_claims WHERE VIN = '{thisVIN}'\",\n",
    "        'egr_sensors_claims': f\"SELECT VIN, CLAIM_REG_DATE, TOT_CLAIM_PAYMENT_USD FROM egr_sensors WHERE VIN = '{thisVIN}'\"\n",
    "    }\n",
    "\n",
    "    # Define filtering condition with corrected date format\n",
    "    for dataset_name, query in claims_datasets.items():\n",
    "        try:\n",
    "            df_claims = spark.sql(query)\n",
    "\n",
    "            if df_claims is not None and df_claims.count() > 0:\n",
    "\n",
    "                # df_claims.printSchema()  # Debugging step\n",
    "                # df_claims.show(5, truncate=False)  # Show sample data\n",
    "\n",
    "                df_claims = df_claims.withColumn(\"CLAIM_REG_DATE\", f.to_date(f.col(\"CLAIM_REG_DATE\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "                # Try alternative filtering\n",
    "                df_filtered = df_claims.filter(\n",
    "                    (f.col('CLAIM_REG_DATE') >= f.to_date(f.lit(start_date), \"yyyy-MM-dd\")) &\n",
    "                    (f.col('CLAIM_REG_DATE') <= f.to_date(f.lit(end_date), \"yyyy-MM-dd\")) &\n",
    "                    (f.col('TOT_CLAIM_PAYMENT_USD') > 1000.0)\n",
    "                )\n",
    "\n",
    "                # df_filtered.show(5, truncate=False)  # Show filtered data\n",
    "\n",
    "                if df_filtered is not None and df_filtered.count() > 0:\n",
    "                    for row in df_filtered.collect():\n",
    "                        replacement_records.append([thisVIN, dataset_name, row['CLAIM_REG_DATE'], row['TOT_CLAIM_PAYMENT_USD']])\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            #             print(f\"Error processing dataset {dataset_name} for VIN {thisVIN}: {e}\")\n",
    "            file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/errors/errorFileName_{jobID}.txt\", \"a\")\n",
    "            file.writelines(f\"Error processing dataset {dataset_name} for VIN {thisVIN}: {e}\")\n",
    "            file.close()\n",
    "\n",
    "\n",
    "        if replacement_records:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "# Normalize using Min/Max Normalization.\n",
    "def normalize_numeric_feature_values(statement_in_feature_name, df):\n",
    "\n",
    "  selected_col_names_list = [col for col in df.columns.values if statement_in_feature_name in col]   # selects names of columns that contain specific string\n",
    "\n",
    "  selected_cols = df[selected_col_names_list]\n",
    "  for col_name in selected_col_names_list:\n",
    "      selected_cols[col_name] = selected_cols[col_name].str.replace(\",\",\".\")\n",
    "      selected_cols[col_name] = selected_cols[col_name].apply(lambda x: float(x.split()[0]))\n",
    "\n",
    "  selected_cols_norm = selected_cols.apply(lambda iterator: ((iterator - iterator.mean())/(iterator.max() - iterator.min())).round(3))\n",
    "\n",
    "  df[selected_cols_norm.columns] = selected_cols_norm\n",
    "  return df\n",
    "\n",
    "            \n",
    "def move_over_calendar_and_compute_features(df_selected_features_from_population_for_this_VIN, thisVIN, calendar_day_from_where_it_left_off, new_15day_end_date, span_length, dayCount, jobID):\n",
    "    \n",
    "    if calendar_day_from_where_it_left_off + dayCount > 2557:\n",
    "#         print(f\"Skipping calendar_day={calendar_day_from_where_it_left_off + dayCount} as it exceeds 2557.\")\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"Skipping calendar_day={calendar_day_from_where_it_left_off + dayCount} as it exceeds 2557.\")\n",
    "        file.close()\n",
    "        return\n",
    "    \n",
    "    \n",
    "#     print(f\"A new day move on calendar: thisVIN={thisVIN}, new_15day_end_date={new_15day_end_date}, span_length={span_length}, dayCount={dayCount+calendar_day_from_where_it_left_off} \\n\") \n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "    file.writelines(f\"A new day move on calendar: thisVIN={thisVIN}, new_15day_end_date={new_15day_end_date}, span_length={span_length}, dayCount={dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "    file.close()\n",
    "    \n",
    "    schema = StructType([StructField('VIN', StringType(), True),\n",
    "                  StructField('calendar_day', IntegerType(), True)])\n",
    "    data = [\n",
    "        (thisVIN, dayCount + calendar_day_from_where_it_left_off)\n",
    "      ]\n",
    "    df_calculated_features_for_this_VIN_and_this_dayCount = spark.createDataFrame(data = data, schema = schema)\n",
    "\n",
    "    new_15day_start_date = new_15day_end_date - timedelta(days = dayCount)\n",
    "\n",
    "    previous_15day_duration_end_date = new_15day_start_date\n",
    "    previous_15day_duration_start_date = previous_15day_duration_end_date - timedelta(days = dayCount)\n",
    "\n",
    "\n",
    "\n",
    "    list_of_dtc_type = ['p1075_38', 'p1075_75', 'p1075_77', 'p1075_86', 'p1075_92', 'p1075_94', 'p0401_faults', 'p2457_faults']\n",
    "    list_of_dtc_df = [df_p1075_38, df_p1075_75, df_p1075_77, df_p1075_86, df_p1075_92, df_p1075_94, df_p0401, df_p2457]\n",
    "    \n",
    "    for i in range(len(list_of_dtc_df)):\n",
    "        dtc_type_df = list_of_dtc_df[i]\n",
    "        \n",
    "        \n",
    "        #filter dtc-type database for thisVIN and then check if there is any dtc's related to this VIN in the first 15 days timespan before \"new_15day_end_date\" and another 15 days before this period.\n",
    "        df_dtc_type_for_this_VIN = dtc_type_df.filter(f.col('VIN')==thisVIN)\n",
    "        \n",
    "        df_dtc_type_for_this_VIN_in_first_15day_timespan = df_dtc_type_for_this_VIN.filter((fix_problem_of_fault_date_time_with_no_seconds(df_dtc_type_for_this_VIN, list_of_dtc_type[i]) > new_15day_start_date) & (fix_problem_of_fault_date_time_with_no_seconds(df_dtc_type_for_this_VIN, list_of_dtc_type[i]) < new_15day_end_date))\n",
    "        df_dtc_type_for_this_VIN_in_previous_15day_timespan = df_dtc_type_for_this_VIN.filter((fix_problem_of_fault_date_time_with_no_seconds(df_dtc_type_for_this_VIN, list_of_dtc_type[i]) > previous_15day_duration_start_date) & (fix_problem_of_fault_date_time_with_no_seconds(df_dtc_type_for_this_VIN, list_of_dtc_type[i]) < previous_15day_duration_end_date))\n",
    "        \n",
    "        count1 = df_dtc_type_for_this_VIN_in_first_15day_timespan.count()\n",
    "        count2 = df_dtc_type_for_this_VIN_in_previous_15day_timespan.count()\n",
    "        '''\n",
    "        Feature 1\n",
    "        \n",
    "        Definition: number of alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 1 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} and dtc_type {list_of_dtc_type[i]} \\n\")\n",
    "        file.close()\n",
    "#         print(f\"feature 1 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} and dtc_type {list_of_dtc_type[i]} \\n\")\n",
    "        \n",
    "        if count1 > 0:\n",
    "            feature_1_value_first_15days = feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, \"Y\")\n",
    "        else:\n",
    "            feature_1_value_first_15days = 0\n",
    "\n",
    "        \n",
    "        if count2 > 0:\n",
    "            feature_1_value_second_15days = feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, \"Y\")\n",
    "        else:\n",
    "            feature_1_value_second_15days = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        '''\n",
    "        Feature 2\n",
    "                \n",
    "        Definition: number of intermittent alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 2 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "        file.close()\n",
    "#         print(f\"feature 2 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "\n",
    "        if count1 > 0:\n",
    "            feature_2_value_first_15days = feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, \"I\")\n",
    "        else:\n",
    "            feature_2_value_first_15days = 0\n",
    "            \n",
    "        if count2 > 0:           \n",
    "            feature_2_value_second_15days = feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, \"I\")\n",
    "        else:\n",
    "            feature_2_value_second_15days = 0\n",
    "            \n",
    "            \n",
    "\n",
    "        '''\n",
    "        Feature 3\n",
    "        \n",
    "        Definition: duration of active alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 3 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "        file.close()\n",
    "#         print(f\"feature 3 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "        \n",
    "        if count1 > 0:\n",
    "            feature_3_value_first_15days = feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, list_of_dtc_type[i], \"Y\")\n",
    "        else:\n",
    "            feature_3_value_first_15days = 0.0\n",
    "            \n",
    "        if count2 > 0:    \n",
    "            feature_3_value_second_15days = feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, list_of_dtc_type[i], \"Y\")\n",
    "        else:\n",
    "            feature_3_value_second_15days = 0.0\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Feature 4:\n",
    "                \n",
    "        Definition: duration of intermittent alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 4 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "        file.close()\n",
    "#         print(f\"feature 4 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "\n",
    "        if count1 > 0:\n",
    "            feature_4_value_first_15days = feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, list_of_dtc_type[i], \"I\")\n",
    "        else:\n",
    "            feature_4_value_first_15days = 0.0\n",
    "            \n",
    "        if count2 > 0: \n",
    "            feature_4_value_second_15days = feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, list_of_dtc_type[i], \"I\")\n",
    "        else:\n",
    "            feature_4_value_second_15days = 0.0\n",
    "            \n",
    "        '''    \n",
    "        Feature 5\n",
    "                \n",
    "        Definition: average time between active alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 5 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "        file.close()\n",
    "#         print(f\"feature 5 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "\n",
    "        if count1 > 0:\n",
    "            feature_5_value_first_15days =  feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, list_of_dtc_type[i], \"Y\")\n",
    "        else:\n",
    "            feature_5_value_first_15days =  0.0 \n",
    "            \n",
    "        if count2 > 0:    \n",
    "            feature_5_value_second_15days = feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, list_of_dtc_type[i], \"Y\")\n",
    "        else:\n",
    "            feature_5_value_second_15days = 0.0\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Feature 6\n",
    "                \n",
    "        Definition: average time between intermittent alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 6 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "        file.close()\n",
    "#         print(f\"feature 6 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "\n",
    "        if count1 > 0:\n",
    "            feature_6_value_first_15days =  feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, list_of_dtc_type[i], \"I\")\n",
    "        else:\n",
    "            feature_6_value_first_15days =  0.0\n",
    "            \n",
    "        if count2 > 0:    \n",
    "            feature_6_value_second_15days = feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, list_of_dtc_type[i], \"I\")\n",
    "        else:\n",
    "            feature_6_value_second_15days = 0.0\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Feature 7\n",
    "                \n",
    "        Definition: number of active alerts with speed = 0 over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 7 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "        file.close()\n",
    "#         print(f\"feature 7 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "\n",
    "        if count1 > 0:        \n",
    "            feature_7_value_first_15days =  feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, 0, \"Y\")\n",
    "        else:\n",
    "            feature_7_value_first_15days =  0\n",
    "        \n",
    "        if count2 > 0:\n",
    "            feature_7_value_second_15days = feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, 0, \"Y\")\n",
    "        else:\n",
    "            feature_7_value_second_15days = 0\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Feature 8\n",
    "                \n",
    "        Definition: number of intermittent alerts with speed = 0 over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\") \n",
    "        file.writelines(f\"feature 8 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "        file.close()\n",
    "#         print(f\"feature 8 for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "\n",
    "        if count1 > 0:        \n",
    "            feature_8_value_first_15days =  feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, 0, \"I\")\n",
    "        else:\n",
    "            feature_8_value_first_15days =  0\n",
    "            \n",
    "        if count2 > 0:    \n",
    "            feature_8_value_second_15days = feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, 0, \"I\")\n",
    "        else:\n",
    "            feature_8_value_second_15days = 0\n",
    "            \n",
    "            \n",
    "  \n",
    "        dtc = 'aaaaaa'\n",
    "\n",
    "        if i == 0 :\n",
    "          dtc = 'dtc38'\n",
    "        elif i == 1:\n",
    "          dtc = 'dtc75'\n",
    "        elif i == 2:\n",
    "          dtc = 'dtc77'\n",
    "        elif i == 3:\n",
    "          dtc = 'dtc86'\n",
    "        elif i == 4:\n",
    "          dtc = 'dtc92'\n",
    "        elif i == 5:\n",
    "          dtc = 'dtc94'\n",
    "        elif i == 6:\n",
    "          dtc = 'dtc0401'\n",
    "        elif i == 7:\n",
    "          dtc = 'dtc2457'\n",
    "\n",
    "        #end of else for i values\n",
    "        feature1Name1 = 'feature_1_'+dtc+'_first_15_days'\n",
    "        feature1Name2 = 'feature_1_'+dtc+'_second_15_days'\n",
    "        feature2Name1 = 'feature_2_'+dtc+'_first_15_days'\n",
    "        feature2Name2 = 'feature_2_'+dtc+'_second_15_days'\n",
    "        feature3Name1 = 'feature_3_'+dtc+'_first_15_days'\n",
    "        feature3Name2 = 'feature_3_'+dtc+'_second_15_days'\n",
    "        feature4Name1 = 'feature_4_'+dtc+'_first_15_days'\n",
    "        feature4Name2 = 'feature_4_'+dtc+'_second_15_days'\n",
    "        feature5Name1 = 'feature_5_'+dtc+'_first_15_days'\n",
    "        feature5Name2 = 'feature_5_'+dtc+'_second_15_days'\n",
    "        feature6Name1 = 'feature_6_'+dtc+'_first_15_days'\n",
    "        feature6Name2 = 'feature_6_'+dtc+'_second_15_days'\n",
    "        feature7Name1 = 'feature_7_'+dtc+'_first_15_days'\n",
    "        feature7Name2 = 'feature_7_'+dtc+'_second_15_days'\n",
    "        feature8Name1 = 'feature_8_'+dtc+'_first_15_days'\n",
    "        feature8Name2 = 'feature_8_'+dtc+'_second_15_days'\n",
    "\n",
    "\n",
    "        df_calculated_features_for_this_VIN_and_this_dayCount = df_calculated_features_for_this_VIN_and_this_dayCount \\\n",
    "                                                                                               .withColumn(feature1Name1, (f.lit(feature_1_value_first_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature1Name2, (f.lit(feature_1_value_second_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature2Name1, (f.lit(feature_2_value_first_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature2Name2, (f.lit(feature_2_value_second_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature3Name1, (f.lit(feature_3_value_first_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature3Name2, (f.lit(feature_3_value_second_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature4Name1, (f.lit(feature_4_value_first_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature4Name2, (f.lit(feature_4_value_second_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature5Name1, (f.lit(feature_5_value_first_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature5Name2, (f.lit(feature_5_value_second_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature6Name1, (f.lit(feature_6_value_first_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature6Name2, (f.lit(feature_6_value_second_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature7Name1, (f.lit(feature_7_value_first_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature7Name2, (f.lit(feature_7_value_second_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature8Name1, (f.lit(feature_8_value_first_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature8Name2, (f.lit(feature_8_value_second_15days)).cast(IntegerType()))\\\n",
    "\n",
    " \n",
    "        #   for loop finished \n",
    "        \n",
    "    df_calculated_features_for_this_VIN_and_this_dayCount = df_calculated_features_for_this_VIN_and_this_dayCount.withColumn(\"if_parts_replaced_in_first_15days\", (f.lit(if_part_is_replaced_for_this_VIN_in_this_timespan(thisVIN, new_15day_start_date, new_15day_end_date))).cast(IntegerType()))\\\n",
    "  .withColumn(\"if_parts_replaced_in_second_15days\", (f.lit(if_part_is_replaced_for_this_VIN_in_this_timespan(thisVIN, previous_15day_duration_start_date, previous_15day_duration_end_date))).cast(IntegerType()))\n",
    "#     print(f\"we finally write something of length for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "#     dataset_is_generated = 1\n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "    file.writelines(f\"we finally write something of length for {thisVIN} in day {dayCount+calendar_day_from_where_it_left_off} \\n\")\n",
    "    file.close()\n",
    "    \n",
    "    vin_file_path = '/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv'\n",
    "    lock_file_path = vin_file_path + '.lock'  # creates VINs_data.csv.lock\n",
    "    lock = FileLock(lock_file_path)\n",
    "\n",
    "    VINs_columns_names =['VIN','TOTAL_ROWS','CALCULATION']\n",
    "    \n",
    "    with lock:\n",
    "        df_VINs = pd.read_csv(vin_file_path, sep=',', names=VINs_columns_names, header=None)\n",
    "#         print(f\"🔓 Safely read from {vin_file_path}\")\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"🔓 Safely read from {vin_file_path}\")\n",
    "        file.close()\n",
    "    \n",
    "    # Assuming this is inside a loop or function where thisVIN, dayCount, calendar_day_from_where_it_left_off, and df_VINs are defined\n",
    "    if df_VINs.loc[df_VINs['VIN'] == thisVIN, 'CALCULATION'].iloc[0] != 'FN':\n",
    "        df_VINs.loc[df_VINs['VIN'] == thisVIN, ['TOTAL_ROWS', 'CALCULATION']] = [dayCount + calendar_day_from_where_it_left_off, 'UP']\n",
    "    else:\n",
    "        df_VINs.loc[df_VINs['VIN'] == thisVIN, ['TOTAL_ROWS', 'CALCULATION']] = [dayCount + calendar_day_from_where_it_left_off, 'FN']\n",
    "\n",
    "    \n",
    "    with lock:  # This ensures only one job writes at a time\n",
    "        df_VINs.to_csv(vin_file_path, index = None, mode = 'w', header=False)\n",
    "#         print(f\"✔️ Wrote to {vin_file_path} safely with lock.\")\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"✔️ Wrote to {vin_file_path} safely with lock.\")\n",
    "        file.close()\n",
    "    \n",
    "    \n",
    "    '''here we aggregate all the selected features from the population for this VIN with the 8 calculated feature values for this VIN \n",
    "    for this specific day and then write it to resultedData.csv as one data point.'''\n",
    "\n",
    "    list_features_for_this_VIN_and_this_dayCount = [df_selected_features_from_population_for_this_VIN, df_calculated_features_for_this_VIN_and_this_dayCount]\n",
    "    df_features_for_this_VIN_and_this_dayCount = reduce(lambda x, y: x.join(y, on=\"VIN\"), list_features_for_this_VIN_and_this_dayCount)\n",
    "    df_features_for_this_VIN_and_this_dayCount.toPandas().to_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/resultedData.csv', index = None, mode = 'a', header=False) \n",
    "    return\n",
    "\n",
    "\n",
    "    \n",
    "def get_max_valid_calendar_day(ins_date_str, end_date_str=\"2021-12-31\"):\n",
    "    \"\"\"\n",
    "    Calculates the maximum valid calendar day (number of days back from end_date)\n",
    "    for a VIN based on its installation date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ins_date = datetime.strptime(ins_date_str, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "        max_days = (end_date - ins_date).days\n",
    "        return max_days if max_days >= 0 else 0\n",
    "    except Exception as e:\n",
    "#         print(f\"Error parsing INS_DATE {ins_date_str}: {e}\")\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/errors/errorFileName_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"Error parsing INS_DATE {ins_date_str}: {e}\")\n",
    "        file.close()\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def get_ins_date_for_vin(vin, df_pop):\n",
    "    row = df_pop[df_pop[\"VIN\"] == vin]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0][\"INS_DATE\"].strftime(\"%Y-%m-%d\")  # Return as string\n",
    "    else:\n",
    "#         print(f\"❗ VIN {vin} not found in population data.\")\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"❗ VIN {vin} not found in population data.\")\n",
    "        file.close()\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3da9f-4629-48fe-bde5-58da75afbe47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through the arguments and print them\n",
    "if len(sys.argv) > 1:\n",
    "    thisVIN = sys.argv[1]\n",
    "    calendar_day_from_where_it_left_off = int(sys.argv[2])\n",
    "    the_calculator_jobID_for_thisVIN = sys.argv[3]\n",
    "    #erasing the txt file for output of the submitted job that runs this Notebook:\n",
    "    # open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"w\").close()\n",
    "    \n",
    "    \n",
    "    columns_of_population = ['VIN','ENGINE_SIZE','ENGINE_HP','VEH_TYPE']+[s for s in df_population.columns if 'KOLA' in s]\n",
    "    \n",
    "    \n",
    "    # print(f\"Current VIN: {thisVIN} \\n\")\n",
    "    # print(f\"Calendar_day from where it left off: {calendar_day_from_where_it_left_off} \\n\")\n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "    file.writelines(f\"Current VIN: {thisVIN} \\n\")\n",
    "    file.writelines(f\"Calendar_day from where it left off: {calendar_day_from_where_it_left_off} \\n\")\n",
    "    file.close()\n",
    "    \n",
    "    # Load the population dataset\n",
    "    # population_path = \"/storage/home/yqf5148/work/volvoPennState/PopulationWithChassisId.csv\"\n",
    "    df_population = spark.sql(\"SELECT * FROM population\")\n",
    "    # Filter to keep only VIN and INS_DATE columns\n",
    "    df_population_filtered_spark = df_population[[\"VIN\", \"INS_DATE\"]]\n",
    "    df_population_filtered = df_population_filtered_spark.toPandas()\n",
    "    \n",
    "    # Make sure INS_DATE is parsed correctly\n",
    "    df_population_filtered[\"INS_DATE\"] = pd.to_datetime(df_population_filtered[\"INS_DATE\"], errors=\"coerce\")\n",
    "    ins_date_str = get_ins_date_for_vin(thisVIN, df_population_filtered)\n",
    "    \n",
    "    duration_start_date = '2014-12-31'\n",
    "    if ins_date_str:\n",
    "        duration_start_date = ins_date_str\n",
    "        \n",
    "    # print(f\"duration_start_date: {duration_start_date} \\n\")\n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "    file.writelines(f\"duration_start_date: {duration_start_date} \\n\")\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "    day_delta = timedelta(days = 1)\n",
    "    start_split_date = duration_start_date.split('-')\n",
    "\n",
    "    start_date = date(int(start_split_date[0]), int(start_split_date[1]), int(start_split_date[2]))\n",
    "    \n",
    "    \n",
    "    duration_end_date_str = '2021-12-31'\n",
    "    duration_end_split_date = duration_end_date_str.split('-')\n",
    "\n",
    "    duration_end_date = date(int(duration_end_split_date[0]), int(duration_end_split_date[1]), int(duration_end_split_date[2]))\n",
    "    \n",
    "    #minus 1 is to consider 12/31/2021 itself\n",
    "    end_date = duration_end_date - timedelta(days = calendar_day_from_where_it_left_off - 1)\n",
    "\n",
    "    # print(f\"start_date = {start_date}, end_date = {end_date}\")\n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "    file.writelines(f\"start_date = {start_date}, end_date = {end_date} \\n\")\n",
    "    file.close()\n",
    "\n",
    "    span_length = 15\n",
    "\n",
    "\n",
    "    VIN_feature_columns = StructType([StructField('VIN', StringType(), True),\n",
    "                                      StructField('TOTAL_ROWS', IntegerType(), True),\n",
    "                                      StructField('CALCULATION', StringType(), True)])\n",
    "    df_new_VIN = spark.createDataFrame(data = [(thisVIN, 0,  'NF')], schema = VIN_feature_columns)\n",
    "    # df_new_VIN.toPandas().to_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv', index = None, mode = 'a', header=False) \n",
    "\n",
    "\n",
    "    df_filtered_population_for_this_VIN = df_population.filter(f.col('VIN') == thisVIN)\n",
    "\n",
    "    \n",
    "    selected_features_from_population_for_this_VIN = ['VIN','ENGINE_SIZE','ENGINE_HP','VEH_TYPE']+[s for s in df_filtered_population_for_this_VIN.columns if 'KOLA' in s]\n",
    "    df_selected_features_from_population_for_this_VIN = df_filtered_population_for_this_VIN[selected_features_from_population_for_this_VIN]\n",
    "    \n",
    "    if df_selected_features_from_population_for_this_VIN.count()!= 0 :\n",
    "        how_many_month = int((end_date - start_date).days/15)\n",
    "        # print(\"how_many_month={} \\n\".format(how_many_month))\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "        file.writelines([\"how_many_month={} \\n\".format(how_many_month)])\n",
    "        file.close()\n",
    "        if how_many_month == 0:\n",
    "            remaining_days = int((end_date - start_date).days) \n",
    "            # print(f\"remaining_days={remaining_days} \\n\")\n",
    "            file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "            file.writelines(f\"remaining_days={remaining_days} \\n\")\n",
    "            file.close()\n",
    "            Parallel(n_jobs= 5, prefer=\"threads\", batch_size=5)(delayed(move_over_calendar_and_compute_features)(df_selected_features_from_population_for_this_VIN, thisVIN, calendar_day_from_where_it_left_off, end_date, span_length, day_count_in_month, the_calculator_jobID_for_thisVIN) for day_count_in_month in range(0, remaining_days))\n",
    "\n",
    "        else:\n",
    "            for number_of_monthes_in_time_duration in range(0, how_many_month):\n",
    "                # print(\"number_of_monthes_in_time_duration={} \\n\".format(number_of_monthes_in_time_duration))\n",
    "                file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "                file.writelines([\"number_of_monthes_in_time_duration={} \\n\".format(number_of_monthes_in_time_duration)])\n",
    "                file.close()\n",
    "                if number_of_monthes_in_time_duration < how_many_month:\n",
    "                    Parallel(n_jobs= 5, prefer=\"threads\", batch_size=5)(delayed(move_over_calendar_and_compute_features)(df_selected_features_from_population_for_this_VIN, thisVIN, calendar_day_from_where_it_left_off, end_date, span_length, 15 * number_of_monthes_in_time_duration + day_count_in_month, the_calculator_jobID_for_thisVIN) for day_count_in_month in range(0, 15))   \n",
    "\n",
    "                else:\n",
    "                    remaining_days = int((end_date - start_date).days) - 15 * number_of_monthes_in_time_duration \n",
    "                    # print(\"remaining_days={} \\n\".format(remaining_days))\n",
    "                    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "                    file.writelines([\"remaining_days={} \\n\".format(remaining_days)])\n",
    "                    file.close()\n",
    "                    Parallel(n_jobs= 5, prefer=\"threads\", batch_size=5)(delayed(move_over_calendar_and_compute_features)(df_selected_features_from_population_for_this_VIN, thisVIN, calendar_day_from_where_it_left_off, end_date, span_length, 15 * number_of_monthes_in_time_duration + day_count_in_month, the_calculator_jobID_for_thisVIN) for day_count_in_month in range(0, remaining_days))\n",
    "\n",
    "        vin_file_path = '/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv'\n",
    "        lock_file_path = vin_file_path + '.lock'  # creates VINs_data.csv.lock\n",
    "        lock = FileLock(lock_file_path)\n",
    "\n",
    "        VINs_columns_names =['VIN','TOTAL_ROWS','CALCULATION']\n",
    "    \n",
    "        with lock:\n",
    "            df_VINs = pd.read_csv(vin_file_path, sep=',', names=VINs_columns_names, header=None)\n",
    "            # print(f\"🔓 Safely read from {vin_file_path}\")\n",
    "            file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "            file.writelines(f\"🔓 Safely read from {vin_file_path}\")\n",
    "            file.close()\n",
    "        \n",
    "        df_VINs.loc[df_VINs['VIN'] == thisVIN, ['CALCULATION']] = ['FN']\n",
    "\n",
    "    \n",
    "        with lock:  # This ensures only one job writes at a time\n",
    "            df_VINs.to_csv(vin_file_path, index = None, mode = 'w', header=False)\n",
    "            # print(f\"✔️ Wrote to {vin_file_path} safely with lock.\")\n",
    "            file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "            file.writelines(f\"✔️ Wrote to {vin_file_path} safely with lock.\")\n",
    "            file.close()\n",
    "else:\n",
    "    \n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/errors/error_log_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "    file.writelines([\"The VIN argument is failed to be passed for feature calculations. \\n\".format(remaining_days)])\n",
    "    file.close()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volvopennstate_new_kernel",
   "language": "python",
   "name": "volvopennstate_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
