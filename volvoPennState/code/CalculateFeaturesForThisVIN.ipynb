{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d27272c-55f1-48ed-b3bd-445ce73244d8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
      "env: PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin\n"
     ]
    }
   ],
   "source": [
    "%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
    "%env PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "302060b9-b23b-4dab-8fe9-4d92a71d0ecb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ps: /storage/icds/RISE/sw8/anaconda/anaconda3/envs/tensorflow/lib/libuuid.so.1: no version information available (required by /usr/lib64/libblkid.so.1)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/19 23:23:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/19 23:23:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/11/19 23:23:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/11/19 23:23:24 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/11/19 23:23:24 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/11/19 23:23:24 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark as psk\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import time as t \n",
    "from datetime import date, datetime, timedelta\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import reduce  \n",
    "from math import modf\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from delta import * \n",
    "from delta.tables import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "from filelock import FileLock\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "conf = (SparkConf().set(\"spark.driver.maxResultSize\", \"12g\"))\n",
    "\n",
    "# Create new context\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# sc = SparkContext(\"local\", \"Simple App\")\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "       .master(\"local[2]\") \\\n",
    "       .appName(\"test\") \\\n",
    "       .config(\"spark.driver.maxResultSize\", \"20g\")\\\n",
    "       .config(\"spark.driver.memory\", \"100g\")\\\n",
    "       .getOrCreate()\n",
    "\n",
    "#both works\n",
    "# 1: \n",
    "# spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")    #To resolve the error for p1075_38 to_timestamp formating: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '1/2/2019 20:40:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n",
    "# Set Spark SQL legacy time parser policy to LEGACY to handle older date formats\n",
    "# 2:\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "# Increase the max fields in the string representation of a plan\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)  # Increase to 1000 or more as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1af8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_data_cleaning(dtc_type):\n",
    "    df = spark.sql(\"SELECT * FROM {} WHERE _C0 IS NOT NULL AND _C0 != 'NA' AND _C0 != '' AND MESSAGE_ID IS NOT NULL AND MESSAGE_ID != '' AND MESSAGE_ID != 'NA' AND FAULT_DATE_TIME IS NOT NULL AND FAULT_DATE_TIME != '' AND FAULT_DATE_TIME != 'NA' AND FAULT_STATUS IS NOT NULL AND FAULT_STATUS != '' AND FAULT_STATUS != 'NA' AND VIN IS NOT NULL AND VIN != '' AND VIN != 'NA' AND length(VIN) !< 17\".format(dtc_type)) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d57ba81-4151-4ad1-8951-9bbfe610f092",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 23:23:29 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file into table\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv\") \\\n",
    "          .createOrReplaceTempView(\"VINs_data\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/PopulationWithChassisId.csv\") \\\n",
    "          .createOrReplaceTempView(\"population\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/CCA Claims.csv\") \\\n",
    "          .createOrReplaceTempView(\"cca_claims\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/EGR Cooler Claims.csv\") \\\n",
    "          .createOrReplaceTempView(\"egr_cooler_claims\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/EGR FG 293 Claims.csv\") \\\n",
    "          .createOrReplaceTempView(\"egr_fg_293_claims\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/EGR Sensors.csv\") \\\n",
    "          .createOrReplaceTempView(\"egr_sensors\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/Engine_Emissions_Table1.csv\") \\\n",
    "          .createOrReplaceTempView(\"engine_emissions_table1\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/Engine_Emissions_Table2.csv\") \\\n",
    "          .createOrReplaceTempView(\"engine_emissions_table2\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_38.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_38\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_75.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_75\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_77.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_77\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_86.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_86\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_92.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_92\")\n",
    " \n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P1075_94.csv\") \\\n",
    "          .createOrReplaceTempView(\"p1075_94\")   \n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P0401_faults.csv\") \\\n",
    "          .createOrReplaceTempView(\"p0401_faults\")\n",
    "\n",
    "spark.read.option(\"header\",True,) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/P2457_faults.csv\") \\\n",
    "          .createOrReplaceTempView(\"p2457_faults\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e0fae1-0016-4b39-a8c2-2ddc41d1a8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_population = spark.sql(\"SELECT * FROM population\")\n",
    "\n",
    "df_p1075_38 = raw_data_cleaning('p1075_38')\n",
    "\n",
    "df_p1075_75 = raw_data_cleaning('p1075_75')\n",
    "\n",
    "df_p1075_77 = raw_data_cleaning('p1075_77')\n",
    "\n",
    "df_p1075_86 = raw_data_cleaning('p1075_86')\n",
    "\n",
    "df_p1075_92 = raw_data_cleaning('p1075_92')\n",
    "\n",
    "df_p1075_94 = raw_data_cleaning('p1075_94')\n",
    "\n",
    "df_p0401 = raw_data_cleaning('p0401_faults')\n",
    "\n",
    "df_p2457 = raw_data_cleaning('p2457_faults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75cf5e13-2336-48ce-ad2b-a04c0f987935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_problem_of_fault_date_time_with_no_seconds(df, dtc_type):\n",
    "    if dtc_type==\"p1075_38\":\n",
    "       return f.to_timestamp(f.concat(df.FAULT_DATE_TIME, lit(\":00\")), \"M/d/yyyy HH:mm:ss\")\n",
    "    else:\n",
    "       return f.to_timestamp(df.FAULT_DATE_TIME, \"yyyy-MM-dd HH:mm:ss\")\n",
    "\n",
    "\n",
    "def feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN, dtc_status_to_calculate_this_feature):\n",
    "      \n",
    "    df_dtc_type_and_status_for_this_VIN = df_dtc_type_for_this_VIN.filter(f.col('FAULT_STATUS') == dtc_status_to_calculate_this_feature)\n",
    "\n",
    "    return df_dtc_type_and_status_for_this_VIN.count()\n",
    "\n",
    "\n",
    "def feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN, dtc_type, dtc_status_to_calculate_this_feature):\n",
    "\n",
    "    fault_date_time_format='yyyy-MM-dd HH:mm:ss'\n",
    "    if dtc_type==\"p1075_38\":\n",
    "        fault_date_time_format='M/d/yyyy HH:mm'\n",
    "\n",
    "    dtcs_with_all_statuses_df = df_dtc_type_for_this_VIN.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format))\n",
    "\n",
    "\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df\n",
    "\n",
    "#     PsSpark: This one is also true to access the first row of pyspark dataframe    \n",
    "#     headRowId = dtcs_with_all_statuses_df2.first()[0]\n",
    "\n",
    "    tailRowId = dtcs_with_all_statuses_df.tail(1)[0][0]\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.filter(f.col('_c0') != tailRowId).fillna(0)\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format)).withColumn(\"_c0\",monotonically_increasing_id())\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.dropna()\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.withColumnRenamed(\"FAULT_STATUS\", \"FAULT_STATUS_CURRENT_DTC\")\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.withColumnRenamed(\"FAULT_DATE_TIME\", \"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\")\n",
    "\n",
    "    headRowId = dtcs_with_all_statuses_df2.head(1)[0][0]\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.filter(f.col('_c0') != headRowId).fillna(0)\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format)).withColumn(\"_c0\",monotonically_increasing_id())\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.dropna()\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.withColumnRenamed(\"FAULT_STATUS\", \"FAULT_ANY_STATUS_NEXT_DTC\")\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.withColumnRenamed(\"FAULT_DATE_TIME\", \"FAULT_DATE_TIME_NEXT_ANY_DTC\")\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.select([c for c in dtcs_with_all_statuses_df2.columns if c != \"VIN\"])\n",
    "\n",
    "    # create list of dataframes\n",
    "    list_df = [dtcs_with_all_statuses_df, dtcs_with_all_statuses_df2]\n",
    "\n",
    "    # merge all at once\n",
    "    my_temp_df = reduce(lambda x, y: x.join(y, on=\"_c0\"), list_df)\n",
    "    my_temp_df = my_temp_df.withColumn(\"FAULT_DATE_TIME_DIFF\", (f.unix_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), format=fault_date_time_format) - f.unix_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), format=fault_date_time_format))) \\\n",
    "    .withColumn(\"DURATION_IN_THIS_15_DAYS_BACK\", f.when((f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format) <= end_date) & \n",
    "                                                                  (f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format) >= start_date), \n",
    "                                                                  (f.unix_timestamp(\"FAULT_DATE_TIME_NEXT_ANY_DTC\", format=fault_date_time_format) - f.unix_timestamp(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\", format=fault_date_time_format)))) \\\n",
    "    .withColumn(\"COUNT_IN_THIS_15_DAYS_BACK\", f.when((f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format) <= end_date) & \n",
    "                                                                  (f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format) >= start_date), (f.lit(1)).cast(IntegerType())))\n",
    "    start_only_with_this_status_dtcs_df = my_temp_df.filter(f.col(\"FAULT_STATUS_CURRENT_DTC\") == f.lit(dtc_status_to_calculate_this_feature))\n",
    "\n",
    "\n",
    "    duration_of_specific_type_dtcs_for_this_VIN = start_only_with_this_status_dtcs_df.select(f.sum(\"DURATION_IN_THIS_15_DAYS_BACK\")).fillna(0)\n",
    "    count_of_specific_type_dtcs_for_this_VIN = start_only_with_this_status_dtcs_df.select(f.sum(\"COUNT_IN_THIS_15_DAYS_BACK\")).fillna(1)\n",
    "    duration = duration_of_specific_type_dtcs_for_this_VIN.collect()[0][0]\n",
    "    return duration\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "def feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN, dtc_type, dtc_status_to_calculate_this_feature):\n",
    "    \n",
    "    fault_date_time_format='yyyy-MM-dd HH:mm:ss'\n",
    "    if dtc_type==\"p1075_38\":\n",
    "        fault_date_time_format='M/d/yyyy HH:mm'\n",
    "\n",
    "\n",
    "    dtcs_with_all_statuses_df = df_dtc_type_for_this_VIN.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format))\n",
    "\n",
    "\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df\n",
    "\n",
    "#     PsSpark: This one is also true to access the first row of pyspark dataframe    \n",
    "    tailRowId = dtcs_with_all_statuses_df.tail(1)[0][0]\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.filter(f.col('_c0') != tailRowId).fillna(0)\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format)).withColumn(\"_c0\",monotonically_increasing_id())\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.dropna()\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.withColumnRenamed(\"FAULT_STATUS\", \"FAULT_STATUS_CURRENT_DTC\")\n",
    "    dtcs_with_all_statuses_df = dtcs_with_all_statuses_df.withColumnRenamed(\"FAULT_DATE_TIME\", \"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\")\n",
    "\n",
    "    headRowId = dtcs_with_all_statuses_df2.head(1)[0][0]\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.filter(f.col('_c0') != headRowId).fillna(0)\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.sort(f.to_timestamp(f.col(\"FAULT_DATE_TIME\"), fault_date_time_format)).withColumn(\"_c0\",monotonically_increasing_id())\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.dropna()\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.withColumnRenamed(\"FAULT_STATUS\", \"FAULT_ANY_STATUS_NEXT_DTC\")\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.withColumnRenamed(\"FAULT_DATE_TIME\", \"FAULT_DATE_TIME_NEXT_ANY_DTC\")\n",
    "    dtcs_with_all_statuses_df2 = dtcs_with_all_statuses_df2.select([c for c in dtcs_with_all_statuses_df2.columns if c != \"VIN\"])\n",
    "\n",
    "\n",
    "    # create list of dataframes\n",
    "    list_df = [dtcs_with_all_statuses_df, dtcs_with_all_statuses_df2]\n",
    "\n",
    "    # merge all at once\n",
    "    my_temp_df = reduce(lambda x, y: x.join(y, on=\"_c0\"), list_df)\n",
    "    my_temp_df = my_temp_df.withColumn(\"FAULT_DATE_TIME_DIFF\", (f.unix_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), format=fault_date_time_format) - f.unix_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), format=fault_date_time_format))) \\\n",
    "    .withColumn(\"DURATION_IN_THIS_15_DAYS_BACK\", f.when((f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format) <= end_date) & \n",
    "                                                                  (f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format) >= start_date), \n",
    "                                                                  (f.unix_timestamp(\"FAULT_DATE_TIME_NEXT_ANY_DTC\", format=fault_date_time_format) - f.unix_timestamp(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\", format=fault_date_time_format)))) \\\n",
    "    .withColumn(\"COUNT_IN_THIS_15_DAYS_BACK\", f.when((f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format) <= end_date) & \n",
    "                                                                  (f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format) >= start_date), (f.lit(1)).cast(IntegerType())))\n",
    "#     my_temp_df[\"_c0\", \"VIN\",\"FAULT_STATUS_CURRENT_DTC\", \"FAULT_ANY_STATUS_NEXT_DTC\", \"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\", \"FAULT_DATE_TIME_NEXT_ANY_DTC\", f.to_timestamp(f.col(\"FAULT_DATE_TIME_CURRENT_SPECIFIC_TYPE_DTC\"), fault_date_time_format), f.to_timestamp(f.col(\"FAULT_DATE_TIME_NEXT_ANY_DTC\"), fault_date_time_format)].show(194)\n",
    "    start_only_with_this_status_dtcs_df = my_temp_df.filter(f.col(\"FAULT_STATUS_CURRENT_DTC\") == f.lit(dtc_status_to_calculate_this_feature))\n",
    "\n",
    "\n",
    "\n",
    "    duration_of_specific_type_dtcs_for_this_VIN = start_only_with_this_status_dtcs_df.select(f.sum(\"DURATION_IN_THIS_15_DAYS_BACK\")).fillna(0)\n",
    "    count_of_specific_type_dtcs_for_this_VIN = start_only_with_this_status_dtcs_df.select(f.sum(\"COUNT_IN_THIS_15_DAYS_BACK\")).fillna(1)\n",
    "    duration = duration_of_specific_type_dtcs_for_this_VIN.collect()[0][0]\n",
    "\n",
    "    total_count = count_of_specific_type_dtcs_for_this_VIN.collect()[0][0]\n",
    "    if(total_count != 0): \n",
    "        average_of_specific_type_dtcs_for_this_VIN = duration/total_count\n",
    "        return average_of_specific_type_dtcs_for_this_VIN\n",
    "    else:\n",
    "        return 0.0\n",
    "  \n",
    "  \n",
    "def feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN, vehicle_speed, dtc_status_to_calculate_this_feature):\n",
    "    \n",
    "    df_dtc_type_and_status_for_this_VIN = df_dtc_type_for_this_VIN\\\n",
    "                .filter(df_dtc_type_for_this_VIN.FAULT_STATUS == dtc_status_to_calculate_this_feature)\\\n",
    "                .filter(df_dtc_type_for_this_VIN.ROAD_SPEED_MPH == vehicle_speed)\n",
    "             \n",
    "    return df_dtc_type_and_status_for_this_VIN.count()\n",
    "\n",
    "\n",
    "\n",
    "def if_part_is_replaced_for_this_VIN_in_this_timespan(thisVIN, start_date, end_date):\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    replacement_records = []\n",
    "\n",
    "    fault_date_time_format = 'MM/dd/yyyy'\n",
    "    start_date = '2014-12-31'\n",
    "    end_date = '2021-12-31'\n",
    "\n",
    "    # Load all claims tables for the specific VIN\n",
    "    claims_datasets = {\n",
    "        'cca_claims': f\"SELECT VIN, CLAIM_REG_DATE, TOT_CLAIM_PAYMENT_USD FROM cca_claims WHERE VIN = '{thisVIN}'\",\n",
    "        'egr_cooler_claims': f\"SELECT VIN, CLAIM_REG_DATE, TOT_CLAIM_PAYMENT_USD FROM egr_cooler_claims WHERE VIN = '{thisVIN}'\",\n",
    "        'egr_fg_293_claims': f\"SELECT VIN, CLAIM_REG_DATE, TOT_CLAIM_PAYMENT_USD FROM egr_fg_293_claims WHERE VIN = '{thisVIN}'\",\n",
    "        'egr_sensors_claims': f\"SELECT VIN, CLAIM_REG_DATE, TOT_CLAIM_PAYMENT_USD FROM egr_sensors WHERE VIN = '{thisVIN}'\"\n",
    "    }\n",
    "\n",
    "    # Define filtering condition with corrected date format\n",
    "    for dataset_name, query in claims_datasets.items():\n",
    "        try:\n",
    "            df_claims = spark.sql(query)\n",
    "\n",
    "            if df_claims is not None and df_claims.count() > 0:\n",
    "\n",
    "                # df_claims.printSchema()  # Debugging step\n",
    "                # df_claims.show(5, truncate=False)  # Show sample data\n",
    "\n",
    "                df_claims = df_claims.withColumn(\"CLAIM_REG_DATE\", f.to_date(f.col(\"CLAIM_REG_DATE\"), \"MM/dd/yyyy\"))\n",
    "\n",
    "                # Try alternative filtering\n",
    "                df_filtered = df_claims.filter(\n",
    "                    (f.col('CLAIM_REG_DATE') >= f.to_date(f.lit(start_date), \"yyyy-MM-dd\")) &\n",
    "                    (f.col('CLAIM_REG_DATE') <= f.to_date(f.lit(end_date), \"yyyy-MM-dd\")) &\n",
    "                    (f.col('TOT_CLAIM_PAYMENT_USD') > 1000.0)\n",
    "                )\n",
    "\n",
    "                # df_filtered.show(5, truncate=False)  # Show filtered data\n",
    "\n",
    "                if df_filtered is not None and df_filtered.count() > 0:\n",
    "                    for row in df_filtered.collect():\n",
    "                        replacement_records.append([thisVIN, dataset_name, row['CLAIM_REG_DATE'], row['TOT_CLAIM_PAYMENT_USD']])\n",
    "\n",
    "        except AnalysisException as e:\n",
    "            print(f\"Error processing dataset {dataset_name} for VIN {thisVIN}: {e}\")\n",
    "\n",
    "\n",
    "        if replacement_records:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "# Normalize using Min/Max Normalization.\n",
    "def normalize_numeric_feature_values(statement_in_feature_name, df):\n",
    "    selected_col_names_list = [col for col in df.columns.values if statement_in_feature_name in col]   # selects names of columns that contain specific string\n",
    "    selected_cols = df[selected_col_names_list]\n",
    "    \n",
    "    for col_name in selected_col_names_list:\n",
    "        selected_cols[col_name] = selected_cols[col_name].str.replace(\",\",\".\")\n",
    "        selected_cols[col_name] = selected_cols[col_name].apply(lambda x: float(x.split()[0]))\n",
    "        \n",
    "    selected_cols_norm = selected_cols.apply(lambda iterator: ((iterator - iterator.mean())/(iterator.max() - iterator.min())).round(3))\n",
    "    \n",
    "    df[selected_cols_norm.columns] = selected_cols_norm\n",
    "    return df\n",
    "\n",
    "            \n",
    "def move_over_calendar_and_compute_features(df_selected_features_from_population_for_this_VIN, thisVIN, new_15day_end_date, span_length, dayCount, jobID):\n",
    "    if dayCount > 2557:\n",
    "            print(f\"Skipping calendar_day={dayCount} as it exceeds 2557.\")\n",
    "            return\n",
    "    \n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "    file.writelines(f\"A new day move on calendar: thisVIN={thisVIN}, new_15day_end_date={new_15day_end_date}, span_length={span_length}, dayCount={dayCount} \\n\")\n",
    "    file.close()\n",
    "    print(f\"A new day move on calendar: thisVIN={thisVIN}, new_15day_end_date={new_15day_end_date}, span_length={span_length}, dayCount={dayCount} \\n\") \n",
    "\n",
    "#     schema = StructType([])   ***** This does not work. Creating EmptyRDD does not allow to add further columns later using withColumn  ****\n",
    "#     df_features_for_this_VIN_and_this_dayCount = sqlContext.createDataFrame(sc.emptyRDD(), schema)\n",
    "#     df_features_for_this_VIN_and_this_dayCount = df_features_for_this_VIN_and_this_dayCount.withColumn(\"VIN\", f.lit(thisVIN)).withColumn(\"calendar_day\", (f.lit(dayCount)).cast(IntegerType()))\n",
    "\n",
    "    \n",
    "    schema = StructType([StructField('VIN', StringType(), True),\n",
    "                      StructField('calendar_day', IntegerType(), True)])\n",
    "    data = [\n",
    "        (thisVIN, dayCount)\n",
    "      ]\n",
    "    df_calculated_features_for_this_VIN_and_this_dayCount = spark.createDataFrame(data = data, schema = schema)\n",
    "\n",
    "    new_15day_start_date = new_15day_end_date - timedelta(days = dayCount)\n",
    "\n",
    "    previous_15day_duration_end_date = new_15day_start_date\n",
    "    previous_15day_duration_start_date = previous_15day_duration_end_date - timedelta(days = dayCount)\n",
    "\n",
    "\n",
    "\n",
    "    list_of_dtc_type = ['p1075_38', 'p1075_75', 'p1075_77', 'p1075_86', 'p1075_92', 'p1075_94', 'p0401_faults', 'p2457_faults']\n",
    "    list_of_dtc_df = [df_p1075_38, df_p1075_75, df_p1075_77, df_p1075_86, df_p1075_92, df_p1075_94, df_p0401, df_p2457]\n",
    "    \n",
    "    for i in range(len(list_of_dtc_df)):\n",
    "        dtc_type_df = list_of_dtc_df[i]\n",
    "        \n",
    "        \n",
    "        #filter dtc-type database for thisVIN and then check if there is any dtc's related to this VIN in the first 15 days timespan before \"new_15day_end_date\" and another 15 days before this period.\n",
    "        df_dtc_type_for_this_VIN = dtc_type_df.filter(f.col('VIN')==thisVIN)\n",
    "        \n",
    "        df_dtc_type_for_this_VIN_in_first_15day_timespan = df_dtc_type_for_this_VIN.filter((fix_problem_of_fault_date_time_with_no_seconds(df_dtc_type_for_this_VIN, list_of_dtc_type[i]) > new_15day_start_date) & (fix_problem_of_fault_date_time_with_no_seconds(df_dtc_type_for_this_VIN, list_of_dtc_type[i]) < new_15day_end_date))\n",
    "        df_dtc_type_for_this_VIN_in_previous_15day_timespan = df_dtc_type_for_this_VIN.filter((fix_problem_of_fault_date_time_with_no_seconds(df_dtc_type_for_this_VIN, list_of_dtc_type[i]) > previous_15day_duration_start_date) & (fix_problem_of_fault_date_time_with_no_seconds(df_dtc_type_for_this_VIN, list_of_dtc_type[i]) < previous_15day_duration_end_date))\n",
    "        \n",
    "        count1 = df_dtc_type_for_this_VIN_in_first_15day_timespan.count()\n",
    "        count2 = df_dtc_type_for_this_VIN_in_previous_15day_timespan.count()\n",
    "        '''\n",
    "        Feature 1\n",
    "        \n",
    "        Definition: number of alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 1 for {thisVIN} in day {dayCount} and dtc_type {list_of_dtc_type[i]} \\n\")\n",
    "        file.close()\n",
    "        print(f\"feature 1 for {thisVIN} in day {dayCount} and dtc_type {list_of_dtc_type[i]} \\n\")\n",
    "        \n",
    "        if count1 > 0:\n",
    "            feature_1_value_first_15days = feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, \"Y\")\n",
    "        else:\n",
    "            feature_1_value_first_15days = 0\n",
    "\n",
    "        \n",
    "        if count2 > 0:\n",
    "            feature_1_value_second_15days = feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, \"Y\")\n",
    "        else:\n",
    "            feature_1_value_second_15days = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        '''\n",
    "        Feature 2\n",
    "                \n",
    "        Definition: number of intermittent alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 2 for {thisVIN} in day {dayCount} \\n\")\n",
    "        file.close()\n",
    "        print(f\"feature 2 for {thisVIN} in day {dayCount} \\n\")\n",
    "\n",
    "        if count1 > 0:\n",
    "            feature_2_value_first_15days = feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, \"I\")\n",
    "        else:\n",
    "            feature_2_value_first_15days = 0\n",
    "            \n",
    "        if count2 > 0:           \n",
    "            feature_2_value_second_15days = feature_1_or_2_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, \"I\")\n",
    "        else:\n",
    "            feature_2_value_second_15days = 0\n",
    "            \n",
    "            \n",
    "\n",
    "        '''\n",
    "        Feature 3\n",
    "        \n",
    "        Definition: duration of active alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 3 for {thisVIN} in day {dayCount} \\n\")\n",
    "        file.close()\n",
    "        print(f\"feature 3 for {thisVIN} in day {dayCount} \\n\")\n",
    "        \n",
    "        if count1 > 0:\n",
    "            feature_3_value_first_15days = feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, list_of_dtc_type[i], \"Y\")\n",
    "        else:\n",
    "            feature_3_value_first_15days = 0.0\n",
    "            \n",
    "        if count2 > 0:    \n",
    "            feature_3_value_second_15days = feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, list_of_dtc_type[i], \"Y\")\n",
    "        else:\n",
    "            feature_3_value_second_15days = 0.0\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Feature 4:\n",
    "                \n",
    "        Definition: duration of intermittent alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 4 for {thisVIN} in day {dayCount} \\n\")\n",
    "        file.close()\n",
    "        print(f\"feature 4 for {thisVIN} in day {dayCount} \\n\")\n",
    "\n",
    "        if count1 > 0:\n",
    "            feature_4_value_first_15days = feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, list_of_dtc_type[i], \"I\")\n",
    "        else:\n",
    "            feature_4_value_first_15days = 0.0\n",
    "            \n",
    "        if count2 > 0: \n",
    "            feature_4_value_second_15days = feature_3_or_4_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, list_of_dtc_type[i], \"I\")\n",
    "        else:\n",
    "            feature_4_value_second_15days = 0.0\n",
    "            \n",
    "        '''    \n",
    "        Feature 5\n",
    "                \n",
    "        Definition: average time between active alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 5 for {thisVIN} in day {dayCount} \\n\")\n",
    "        file.close()\n",
    "        print(f\"feature 5 for {thisVIN} in day {dayCount} \\n\")\n",
    "\n",
    "        if count1 > 0:\n",
    "            feature_5_value_first_15days =  feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, list_of_dtc_type[i], \"Y\")\n",
    "        else:\n",
    "            feature_5_value_first_15days =  0.0 \n",
    "            \n",
    "        if count2 > 0:    \n",
    "            feature_5_value_second_15days = feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, list_of_dtc_type[i], \"Y\")\n",
    "        else:\n",
    "            feature_5_value_second_15days = 0.0\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Feature 6\n",
    "                \n",
    "        Definition: average time between intermittent alerts over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 6 for {thisVIN} in day {dayCount} \\n\")\n",
    "        file.close()\n",
    "        print(f\"feature 6 for {thisVIN} in day {dayCount} \\n\")\n",
    "\n",
    "        if count1 > 0:\n",
    "            feature_6_value_first_15days =  feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, list_of_dtc_type[i], \"I\")\n",
    "        else:\n",
    "            feature_6_value_first_15days =  0.0\n",
    "            \n",
    "        if count2 > 0:    \n",
    "            feature_6_value_second_15days = feature_5_or_6_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, list_of_dtc_type[i], \"I\")\n",
    "        else:\n",
    "            feature_6_value_second_15days = 0.0\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Feature 7\n",
    "                \n",
    "        Definition: number of active alerts with speed = 0 over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"feature 7 for {thisVIN} in day {dayCount} \\n\")\n",
    "        file.close()\n",
    "        print(f\"feature 7 for {thisVIN} in day {dayCount} \\n\")\n",
    "\n",
    "        if count1 > 0:        \n",
    "            feature_7_value_first_15days =  feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, 0, \"Y\")\n",
    "        else:\n",
    "            feature_7_value_first_15days =  0\n",
    "        \n",
    "        if count2 > 0:\n",
    "            feature_7_value_second_15days = feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, 0, \"Y\")\n",
    "        else:\n",
    "            feature_7_value_second_15days = 0\n",
    "            \n",
    "            \n",
    "        '''\n",
    "        Feature 8\n",
    "                \n",
    "        Definition: number of intermittent alerts with speed = 0 over the last (span_length = )15 days and the second #(span_length = )15 days before this period for each VIN for the duration of X year(s) (duration_of_compare)\n",
    "        '''\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\") \n",
    "        file.writelines(f\"feature 8 for {thisVIN} in day {dayCount} \\n\")\n",
    "        file.close()\n",
    "        print(f\"feature 8 for {thisVIN} in day {dayCount} \\n\")\n",
    "\n",
    "        if count1 > 0:        \n",
    "            feature_8_value_first_15days =  feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_first_15day_timespan, 0, \"I\")\n",
    "        else:\n",
    "            feature_8_value_first_15days =  0\n",
    "            \n",
    "        if count2 > 0:    \n",
    "            feature_8_value_second_15days = feature_7_or_8_calculate_for_this_VIN_and_this_timespan(df_dtc_type_for_this_VIN_in_previous_15day_timespan, 0, \"I\")\n",
    "        else:\n",
    "            feature_8_value_second_15days = 0\n",
    "            \n",
    "            \n",
    "  \n",
    "        dtc = 'aaaaaa'\n",
    "\n",
    "        if i == 0 :\n",
    "          dtc = 'dtc38'\n",
    "        elif i == 1:\n",
    "          dtc = 'dtc75'\n",
    "        elif i == 2:\n",
    "          dtc = 'dtc77'\n",
    "        elif i == 3:\n",
    "          dtc = 'dtc86'\n",
    "        elif i == 4:\n",
    "          dtc = 'dtc92'\n",
    "        elif i == 5:\n",
    "          dtc = 'dtc94'\n",
    "        elif i == 6:\n",
    "          dtc = 'dtc0401'\n",
    "        elif i == 7:\n",
    "          dtc = 'dtc2457'\n",
    "\n",
    "        #end of else for i values\n",
    "        feature1Name1 = 'feature_1_'+dtc+'_first_15_days'\n",
    "        feature1Name2 = 'feature_1_'+dtc+'_second_15_days'\n",
    "        feature2Name1 = 'feature_2_'+dtc+'_first_15_days'\n",
    "        feature2Name2 = 'feature_2_'+dtc+'_second_15_days'\n",
    "        feature3Name1 = 'feature_3_'+dtc+'_first_15_days'\n",
    "        feature3Name2 = 'feature_3_'+dtc+'_second_15_days'\n",
    "        feature4Name1 = 'feature_4_'+dtc+'_first_15_days'\n",
    "        feature4Name2 = 'feature_4_'+dtc+'_second_15_days'\n",
    "        feature5Name1 = 'feature_5_'+dtc+'_first_15_days'\n",
    "        feature5Name2 = 'feature_5_'+dtc+'_second_15_days'\n",
    "        feature6Name1 = 'feature_6_'+dtc+'_first_15_days'\n",
    "        feature6Name2 = 'feature_6_'+dtc+'_second_15_days'\n",
    "        feature7Name1 = 'feature_7_'+dtc+'_first_15_days'\n",
    "        feature7Name2 = 'feature_7_'+dtc+'_second_15_days'\n",
    "        feature8Name1 = 'feature_8_'+dtc+'_first_15_days'\n",
    "        feature8Name2 = 'feature_8_'+dtc+'_second_15_days'\n",
    "\n",
    "\n",
    "        df_calculated_features_for_this_VIN_and_this_dayCount = df_calculated_features_for_this_VIN_and_this_dayCount \\\n",
    "                                                                                               .withColumn(feature1Name1, (f.lit(feature_1_value_first_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature1Name2, (f.lit(feature_1_value_second_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature2Name1, (f.lit(feature_2_value_first_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature2Name2, (f.lit(feature_2_value_second_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature3Name1, (f.lit(feature_3_value_first_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature3Name2, (f.lit(feature_3_value_second_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature4Name1, (f.lit(feature_4_value_first_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature4Name2, (f.lit(feature_4_value_second_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature5Name1, (f.lit(feature_5_value_first_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature5Name2, (f.lit(feature_5_value_second_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature6Name1, (f.lit(feature_6_value_first_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature6Name2, (f.lit(feature_6_value_second_15days)).cast(FloatType()))\\\n",
    "                                                                                               .withColumn(feature7Name1, (f.lit(feature_7_value_first_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature7Name2, (f.lit(feature_7_value_second_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature8Name1, (f.lit(feature_8_value_first_15days)).cast(IntegerType()))\\\n",
    "                                                                                               .withColumn(feature8Name2, (f.lit(feature_8_value_second_15days)).cast(IntegerType()))\\\n",
    "\n",
    " \n",
    "        #   for loop finished \n",
    "        \n",
    "    df_calculated_features_for_this_VIN_and_this_dayCount = df_calculated_features_for_this_VIN_and_this_dayCount.withColumn(\"if_parts_replaced_in_first_15days\", (f.lit(if_part_is_replaced_for_this_VIN_in_this_timespan(thisVIN, new_15day_start_date, new_15day_end_date))).cast(IntegerType()))\\\n",
    "  .withColumn(\"if_parts_replaced_in_second_15days\", (f.lit(if_part_is_replaced_for_this_VIN_in_this_timespan(thisVIN, previous_15day_duration_start_date, previous_15day_duration_end_date))).cast(IntegerType()))\n",
    "    print(f\"we finally write something of length for {thisVIN} in day {dayCount} \\n\")\n",
    "\n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "    file.writelines(f\"we finally write something of length for {thisVIN} in day {dayCount} \\n\")\n",
    "    file.close()\n",
    "    \n",
    "    vin_file_path = '/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv'\n",
    "    lock_file_path = vin_file_path + '.lock'  # creates VINs_data.csv.lock\n",
    "    lock = FileLock(lock_file_path)\n",
    "\n",
    "    VINs_columns_names =['VIN','TOTAL_ROWS','CALCULATION']\n",
    "    \n",
    "    with lock:\n",
    "        df_VINs = pd.read_csv(vin_file_path, sep=',', names=VINs_columns_names, header=None)\n",
    "        print(f\"🔓 Safely read from {vin_file_path}\")\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"🔓 Safely read from {vin_file_path}\")\n",
    "        file.close()\n",
    "    \n",
    "    df_VINs.loc[df_VINs['VIN'] == thisVIN, ['TOTAL_ROWS', 'CALCULATION']] = [dayCount, 'UP']\n",
    "\n",
    "    \n",
    "    with lock:  # This ensures only one job writes at a time\n",
    "        df_VINs.to_csv(vin_file_path, index = None, mode = 'w', header=False)\n",
    "        print(f\"✔️ Wrote to {vin_file_path} safely with lock.\")\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{jobID}.txt\", \"a\")\n",
    "        file.writelines(f\"✔️ Wrote to {vin_file_path} safely with lock.\")\n",
    "        file.close()\n",
    "    \n",
    "    \n",
    "    '''here we aggregate all the selected features from the population for this VIN with the 8 calculated feature values for this VIN \n",
    "    for this specific day and then write it to resultedData.csv as one data point.'''\n",
    "\n",
    "    list_features_for_this_VIN_and_this_dayCount = [df_selected_features_from_population_for_this_VIN, ]\n",
    "    df_features_for_this_VIN_and_this_dayCount = reduce(lambda x, y: x.join(y, on=\"VIN\"), list_features_for_this_VIN_and_this_dayCount)\n",
    "    df_features_for_this_VIN_and_this_dayCount.toPandas().to_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/resultedData.csv', index = None, mode = 'a', header=False) \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def get_max_valid_calendar_day(ins_date_str, end_date_str=\"2021-12-31\"):\n",
    "    \"\"\"\n",
    "    Calculates the maximum valid calendar day (number of days back from end_date)\n",
    "    for a VIN based on its installation date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ins_date = datetime.strptime(ins_date_str, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "        max_days = (end_date - ins_date).days\n",
    "        return max_days if max_days >= 0 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing INS_DATE {ins_date_str}: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def get_ins_date_for_vin(vin, df_pop):\n",
    "    row = df_pop[df_pop[\"VIN\"] == vin]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0][\"INS_DATE\"].strftime(\"%Y-%m-%d\")  # Return as string\n",
    "    else:\n",
    "        print(f\"❗ VIN {vin} not found in population data.\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b3da9f-4629-48fe-bde5-58da75afbe47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current VIN: -f \n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_/storage/home/yqf5148/.local/share/jupyter/runtime/kernel-ecf12fb3-4f13-4c42-85d2-376a3a4cd9cd.json.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#erasing the txt file for output of the submitted job that runs this Notebook:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"w\").close()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent VIN: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(thisVIN))\n\u001b[0;32m----> 9\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mthe_calculator_jobID_for_thisVIN\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m file\u001b[38;5;241m.\u001b[39mwritelines([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent VIN: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(thisVIN)])\n\u001b[1;32m     11\u001b[0m file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/work/anaconda3/envs/volvopennstate-env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_/storage/home/yqf5148/.local/share/jupyter/runtime/kernel-ecf12fb3-4f13-4c42-85d2-376a3a4cd9cd.json.txt'"
     ]
    }
   ],
   "source": [
    "# Loop through the arguments and print them\n",
    "if len(sys.argv) > 1:\n",
    "    thisVIN = sys.argv[1]\n",
    "    the_calculator_jobID_for_thisVIN = sys.argv[2]\n",
    "    #erasing the txt file for output of the submitted job that runs this Notebook:\n",
    "    # open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"w\").close()\n",
    "    \n",
    "    \n",
    "    print(\"Current VIN: {} \\n\".format(thisVIN))\n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "    file.writelines([\"Current VIN: {} \\n\".format(thisVIN)])\n",
    "    file.close()\n",
    "    \n",
    "    ##### Features Generator Code:\n",
    "    duration_end_date = '2021-12-31'\n",
    "\n",
    "    day_delta = timedelta(days = 1)\n",
    "    split_duration_end_date = duration_end_date.split('-')\n",
    "    end_date = date(int(split_duration_end_date[0]), int(split_duration_end_date[1]), int(split_duration_end_date[2]))\n",
    "    \n",
    "    # Limit calendar_day range based on VIN's actual INS_DATE (installation date).\n",
    "    # This avoids computing features for days before the engine started operating.\n",
    "\n",
    "    # Load the population dataset\n",
    "    df_population = spark.sql(\"SELECT * FROM population\")\n",
    "    # Filter to keep only VIN and INS_DATE columns\n",
    "    df_population_filtered_spark = df_population[[\"VIN\", \"INS_DATE\"]]\n",
    "    df_population_filtered = df_population_filtered_spark.toPandas()\n",
    "    \n",
    "    # Make sure INS_DATE is parsed correctly\n",
    "    df_population_filtered[\"INS_DATE\"] = pd.to_datetime(df_population_filtered[\"INS_DATE\"], errors=\"coerce\")\n",
    "    ins_date_str = get_ins_date_for_vin(thisVIN, df_population_filtered)\n",
    "    \n",
    "    max_dayCount = 2557\n",
    "    if ins_date_str:\n",
    "        max_dayCount = get_max_valid_calendar_day(ins_date_str)\n",
    "    \n",
    "    print(f\"→ Max calendar_day for {thisVIN}: {max_dayCount}\")\n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "    file.writelines(f\"→ Max calendar_day for {thisVIN}: {max_dayCount}\")\n",
    "    file.close()\n",
    "\n",
    "    #minus 1 is to consider 12/31/2021 itself\n",
    "    start_date = end_date - timedelta(days = max_dayCount - 1)\n",
    "\n",
    "    print(\"start_date = {}, end_date = {}\".format(start_date, end_date))\n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "    file.writelines([\"start_date = {}, end_date = {} \\n\".format(start_date, end_date)])\n",
    "    file.close()\n",
    "\n",
    "    span_length = 15\n",
    "\n",
    "\n",
    "    VIN_feature_columns = StructType([StructField('VIN', StringType(), True),\n",
    "                                      StructField('TOTAL_ROWS', IntegerType(), True),\n",
    "                                      StructField('CALCULATION', StringType(), True)])\n",
    "    df_new_VIN = spark.createDataFrame(data = [(thisVIN, 0, 'NF')], schema = VIN_feature_columns)\n",
    "    # df_new_VIN.toPandas().to_csv('/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv', index = None, mode = 'a', header=False) \n",
    "\n",
    "\n",
    "    df_filtered_population_for_this_VIN = df_population_filtered.filter(f.col('VIN')==thisVIN)\n",
    "    \n",
    "    columns_of_population = ['VIN','ENGINE_SIZE','ENGINE_HP','VEH_TYPE']+[s for s in df_population.columns if 'KOLA' in s]\n",
    "    df_selected_features_from_population_for_this_VIN = df_filtered_population_for_this_VIN[columns_of_population]\n",
    "    \n",
    "    if df_selected_features_from_population_for_this_VIN.count()!= 0 :\n",
    "        \n",
    "        how_many_month = int((end_date - start_date).days/15)\n",
    "        print(\"how_many_month={} \\n\".format(how_many_month))\n",
    "        file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "        file.writelines([\"how_many_month={} \\n\".format(how_many_month)])\n",
    "        file.close()\n",
    "        if how_many_month == 0:\n",
    "            remaining_days = int((end_date - start_date).days) \n",
    "            print(\"remaining_days={} \\n\".format(remaining_days))\n",
    "            file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "            file.writelines([\"remaining_days={} \\n\".format(remaining_days)])\n",
    "            file.close()\n",
    "            Parallel(n_jobs= 5, prefer=\"threads\", batch_size=5)(delayed(move_over_calendar_and_compute_features)(df_selected_features_from_population_for_this_VIN, thisVIN, end_date, span_length, day_count_in_month, the_calculator_jobID_for_thisVIN) for day_count_in_month in range(0, remaining_days))\n",
    "        else:\n",
    "            for number_of_monthes_in_time_duration in range(0, how_many_month):\n",
    "                print(\"number_of_monthes_in_time_duration={} \\n\".format(number_of_monthes_in_time_duration))\n",
    "                file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "                file.writelines([\"number_of_monthes_in_time_duration={} \\n\".format(number_of_monthes_in_time_duration)])\n",
    "                file.close()\n",
    "                if number_of_monthes_in_time_duration < how_many_month:\n",
    "                    Parallel(n_jobs= 5, prefer=\"threads\", batch_size=5)(delayed(move_over_calendar_and_compute_features)(df_selected_features_from_population_for_this_VIN, thisVIN, end_date, span_length, 15 * number_of_monthes_in_time_duration + day_count_in_month, the_calculator_jobID_for_thisVIN) for day_count_in_month in range(0, 15))   \n",
    "\n",
    "                else:\n",
    "                    remaining_days = int((end_date - start_date).days) - 15 * number_of_monthes_in_time_duration \n",
    "                    print(\"remaining_days={} \\n\".format(remaining_days))\n",
    "                    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "                    file.writelines([\"remaining_days={} \\n\".format(remaining_days)])\n",
    "                    file.close()\n",
    "                    Parallel(n_jobs= 5, prefer=\"threads\", batch_size=5)(delayed(move_over_calendar_and_compute_features)(df_selected_features_from_population_for_this_VIN, thisVIN, end_date, span_length, 15 * number_of_monthes_in_time_duration + day_count_in_month, the_calculator_jobID_for_thisVIN) for day_count_in_month in range(0, remaining_days))\n",
    "\n",
    "        vin_file_path = '/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv'\n",
    "        lock_file_path = vin_file_path + '.lock'  # creates VINs_data.csv.lock\n",
    "        lock = FileLock(lock_file_path)\n",
    "\n",
    "        VINs_columns_names =['VIN','TOTAL_ROWS','CALCULATION']\n",
    "    \n",
    "        with lock:\n",
    "            df_VINs = pd.read_csv(vin_file_path, sep=',', names=VINs_columns_names, header=None)\n",
    "            print(f\"🔓 Safely read from {vin_file_path}\")\n",
    "            file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "            file.writelines(f\"🔓 Safely read from {vin_file_path}\")\n",
    "            file.close()\n",
    "        \n",
    "        df_VINs.loc[df_VINs['VIN'] == thisVIN, ['CALCULATION']] = ['FN']\n",
    "\n",
    "    \n",
    "        with lock:  # This ensures only one job writes at a time\n",
    "            df_VINs.to_csv(vin_file_path, index = None, mode = 'w', header=False)\n",
    "            print(f\"✔️ Wrote to {vin_file_path} safely with lock.\")\n",
    "            file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/outputs/outputForJob_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "            file.writelines(f\"✔️ Wrote to {vin_file_path} safely with lock.\")\n",
    "            file.close()            \n",
    "\n",
    "else:\n",
    "    \n",
    "    file = open(f\"/storage/home/yqf5148/work/volvoPennState/Jobs/errors/error_log_{the_calculator_jobID_for_thisVIN}.txt\", \"a\")\n",
    "    file.writelines([\"The VIN argument is failed to be passed for feature calculations. \\n\".format(remaining_days)])\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984361e-8f24-4af6-b62e-8d05a362f97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#         lists = results_for_this_VIN_this_day\n",
    "#         list_of_res = np.array(pd.concat([pd.Series(x) for x in lists], axis=1)).T.tolist()\n",
    "\n",
    "#         data = list_of_res[0][0]\n",
    "#         df_features_for_this_VIN_and_this_dayCount = spark.createDataFrame(data = [data], schema = feature_columns)\n",
    "\n",
    "#         for i in range(1, int(len(list_of_res))):\n",
    "#             list_for_thisVIN_and_thisDay = list_of_res[i][0]\n",
    "#             df_for_thisVIN_and_thisDay = spark.createDataFrame(data = [list_for_thisVIN_and_thisDay], schema = feature_columns)\n",
    "#             df_features_for_this_VIN_and_this_dayCount = df_features_for_this_VIN_and_this_dayCount.union(df_for_thisVIN_and_thisDay)\n",
    "#         #end for-loop\n",
    "\n",
    "#         ndays = df_features_for_this_VIN_and_this_dayCount.count()\n",
    "#         if ndays > 0:\n",
    "#             print('we finally write something of length', ndays)\n",
    "#     #         df_features_for_this_VIN_and_this_dayCount.write.mode(\"append\").format('delta').saveAsTable('df_AMT_features2')   on Databricks we were saving results into a table on database. \n",
    "#     #         df_features_for_this_VIN_and_this_dayCount.repartition(1).write.option(\"header\",True).csv(path=\"./data\", mode=\"append\")\n",
    "#     #         write_csv_with_specific_file_name(sc, df_features_for_this_VIN_and_this_dayCount, \"./data\", \"/resulted_dataset.csv\")\n",
    "#             df_features_for_this_VIN_and_this_dayCount.repartition(1).write.option(\"header\",True).format(\"csv\").mode(\"append\").save(\"./data/dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25f5cf-d9ea-4f53-a049-81f2d0e9909d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # import pandas module\n",
    "# import pandas as pd\n",
    " \n",
    "# # consider a list\n",
    "# list1 = [\"durga\", \"ramya\", \"sravya\"]\n",
    "# list2 = [\"java\", \"php\", \"mysql\"]\n",
    "# list3 = [67, 89, 65]\n",
    "\n",
    "# lists = [list1, list2, list3]\n",
    "# df_of_res = np.array(pd.concat([pd.Series(x) for x in lists], axis=1)).T.tolist()\n",
    "# print(df_of_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae1a4cb-7573-444c-8d63-8e7b5c755c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list into dataframe row by\n",
    "# using zip()\n",
    "# data = pd.DataFrame(list(zip(list1, list2, list3)),\n",
    "#                     columns=['student', 'subject', 'marks'])\n",
    " \n",
    "# display(data)\n",
    "\n",
    "# df_pop = spark.sql('select * from population')\n",
    "# [c for c in set(df_pop.toPandas()['VIN'].unique()) if c not in tmp]\n",
    "# tmp = df_pop.toPandas()['VIN'].unique()\n",
    "\n",
    "# df = pd.DataFrame({'a': [1, 1, 2], 'b': ['a', 'b', 'c'], 'c': [4, 5, 6]})\n",
    "# # Normalize using Min/Max Normalization.\n",
    "\n",
    "# display(df)\n",
    "# df_num = df.select_dtypes(include='number')\n",
    "# # selected = [s for s in df.columns if 'a' in s]    #+['VIN']\n",
    "# # print(df_num.dtypes)\n",
    "# df_norm = (df_num - df_num.mean()) / (df_num.max() - df_num.min())\n",
    "\n",
    "# # df = df.toPandas()\n",
    "# df[df_norm.columns] = df_norm\n",
    "# display(df)\n",
    "\n",
    "# df = spark.sql('select * from p2457_faults LIMIT 10')\n",
    "# pandas_df = df.toPandas()\n",
    "# pandas_df = pandas_df.replace(\"NA\", None)\n",
    "# pandas_df = pandas_df.replace(\",\", \".\")\n",
    "# display(pandas_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# statement_in_feature_name = 'ROAD_SPEED'\n",
    "# df_normalized = normalize_numeric_feature_values(statement_in_feature_name, pandas_df)\n",
    "# display(df_normalized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volvopennstate_new_kernel",
   "language": "python",
   "name": "volvopennstate_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
