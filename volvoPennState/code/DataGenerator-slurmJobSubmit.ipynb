{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "167a9ca2-b7be-442b-b3e6-c160215bdc9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install SQLAlchemy\n",
    "# !pip install pyspark\n",
    "# !pip install jupysql duckdb duckdb-engine\n",
    "# !pip install delta-spark\n",
    "# !pip install py4j\n",
    "# !pip install -q findspark\n",
    "# conda install -c conda-forge submitit \n",
    "# conda install corn, nbconvert\n",
    "# !pip install graphviz\n",
    "# !pip install hydra-core\n",
    "# !pip install hydra-submitit-launcher\n",
    "# !pip install submitit\n",
    "# !pip install covalent-slurm-plugin\n",
    "# !pip install ipynb-py-convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368574ca-80d4-413c-804b-a86c05c1339f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
      "env: PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin\n"
     ]
    }
   ],
   "source": [
    "%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
    "%env PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "302060b9-b23b-4dab-8fe9-4d92a71d0ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ps: /storage/icds/RISE/sw8/anaconda/anaconda3/envs/tensorflow/lib/libuuid.so.1: no version information available (required by /usr/lib64/libblkid.so.1)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/16 10:34:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark as psk\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# import submitit\n",
    "\n",
    "import time as t \n",
    "from datetime import date, datetime, timedelta\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from functools import reduce  \n",
    "from math import modf\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from delta import * \n",
    "from delta.tables import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "# import covalent as ct\n",
    "\n",
    "import subprocess\n",
    "import re\n",
    "import random\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# os.environ['PYDEVD_DISABLE_FILE_VALIDATION']=1\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "       .master(\"local[2]\") \\\n",
    "       .appName(\"MyApp\") \\\n",
    "       .config(\"spark.driver.maxResultSize\", \"20g\")\\\n",
    "       .config(\"spark.driver.memory\", \"100g\")\\\n",
    "       .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "#both works\n",
    "# 1: \n",
    "# spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")    #To resolve the error for p1075_38 to_timestamp formating: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '1/2/2019 20:40:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n",
    "# Set Spark SQL legacy time parser policy to LEGACY to handle older date formats\n",
    "# 2:\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "# Increase the max fields in the string representation of a plan\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)  # Increase to 1000 or more as needed\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d57ba81-4151-4ad1-8951-9bbfe610f092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv\") \\\n",
    "          .createOrReplaceTempView(\"VINs_data\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/PopulationWithChassisId.csv\") \\\n",
    "          .createOrReplaceTempView(\"population\")\n",
    "df_population = spark.sql(\"SELECT * FROM population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5293668b-a97b-45b9-a8c9-ed75eb6d878c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b48cf55-1f5a-4eb2-9e66-620bf82d6a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headerList = [    \n",
    "          \"calendar_day\", \n",
    "          \"f_1_dtc38_1th_15d\", \n",
    "          \"f_1_dtc38_2nd_15d\", \n",
    "          \"f_2_dtc38_1th_15d\", \n",
    "          \"f_2_dtc38_2nd_15d\", \n",
    "          \"f_3_dtc38_1th_15d\", \n",
    "          \"f_3_dtc38_2nd_15d\", \n",
    "          \"f_4_dtc38_1th_15d\", \n",
    "          \"f_4_dtc38_2nd_15d\", \n",
    "          \"f_5_dtc38_1th_15d\", \n",
    "          \"f_5_dtc38_2nd_15d\", \n",
    "          \"f_6_dtc38_1th_15d\", \n",
    "          \"f_6_dtc38_2nd_15d\", \n",
    "          \"f_7_dtc38_1th_15d\", \n",
    "          \"f_7_dtc38_2nd_15d\", \n",
    "          \"f_8_dtc38_1th_15d\", \n",
    "          \"f_8_dtc38_2nd_15d\", \n",
    "\n",
    "          \"f_1_dtc75_1th_15d\", \n",
    "          \"f_1_dtc75_2nd_15d\", \n",
    "          \"f_2_dtc75_1th_15d\", \n",
    "          \"f_2_dtc75_2nd_15d\", \n",
    "          \"f_3_dtc75_1th_15d\", \n",
    "          \"f_3_dtc75_2nd_15d\", \n",
    "          \"f_4_dtc75_1th_15d\", \n",
    "          \"f_4_dtc75_2nd_15d\", \n",
    "          \"f_5_dtc75_1th_15d\", \n",
    "          \"f_5_dtc75_2nd_15d\", \n",
    "          \"f_6_dtc75_1th_15d\", \n",
    "          \"f_6_dtc75_2nd_15d\", \n",
    "          \"f_7_dtc75_1th_15d\", \n",
    "          \"f_7_dtc75_2nd_15d\", \n",
    "          \"f_8_dtc75_1th_15d\", \n",
    "          \"f_8_dtc75_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc77_1th_15d\", \n",
    "          \"f_1_dtc77_2nd_15d\", \n",
    "          \"f_2_dtc77_1th_15d\", \n",
    "          \"f_2_dtc77_2nd_15d\", \n",
    "          \"f_3_dtc77_1th_15d\", \n",
    "          \"f_3_dtc77_2nd_15d\", \n",
    "          \"f_4_dtc77_1th_15d\", \n",
    "          \"f_4_dtc77_2nd_15d\", \n",
    "          \"f_5_dtc77_1th_15d\", \n",
    "          \"f_5_dtc77_2nd_15d\", \n",
    "          \"f_6_dtc77_1th_15d\", \n",
    "          \"f_6_dtc77_2nd_15d\", \n",
    "          \"f_7_dtc77_1th_15d\", \n",
    "          \"f_7_dtc77_2nd_15d\", \n",
    "          \"f_8_dtc77_1th_15d\", \n",
    "          \"f_8_dtc77_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc86_1th_15d\", \n",
    "          \"f_1_dtc86_2nd_15d\", \n",
    "          \"f_2_dtc86_1th_15d\", \n",
    "          \"f_2_dtc86_2nd_15d\", \n",
    "          \"f_3_dtc86_1th_15d\", \n",
    "          \"f_3_dtc86_2nd_15d\", \n",
    "          \"f_4_dtc86_1th_15d\", \n",
    "          \"f_4_dtc86_2nd_15d\", \n",
    "          \"f_5_dtc86_1th_15d\", \n",
    "          \"f_5_dtc86_2nd_15d\", \n",
    "          \"f_6_dtc86_1th_15d\", \n",
    "          \"f_6_dtc86_2nd_15d\", \n",
    "          \"f_7_dtc86_1th_15d\", \n",
    "          \"f_7_dtc86_2nd_15d\", \n",
    "          \"f_8_dtc86_1th_15d\", \n",
    "          \"f_8_dtc86_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc92_1th_15d\", \n",
    "          \"f_1_dtc92_2nd_15d\", \n",
    "          \"f_2_dtc92_1th_15d\", \n",
    "          \"f_2_dtc92_2nd_15d\", \n",
    "          \"f_3_dtc92_1th_15d\", \n",
    "          \"f_3_dtc92_2nd_15d\", \n",
    "          \"f_4_dtc92_1th_15d\", \n",
    "          \"f_4_dtc92_2nd_15d\", \n",
    "          \"f_5_dtc92_1th_15d\", \n",
    "          \"f_5_dtc92_2nd_15d\", \n",
    "          \"f_6_dtc92_1th_15d\", \n",
    "          \"f_6_dtc92_2nd_15d\", \n",
    "          \"f_7_dtc92_1th_15d\", \n",
    "          \"f_7_dtc92_2nd_15d\", \n",
    "          \"f_8_dtc92_1th_15d\", \n",
    "          \"f_8_dtc92_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc94_1th_15d\", \n",
    "          \"f_1_dtc94_2nd_15d\", \n",
    "          \"f_2_dtc94_1th_15d\", \n",
    "          \"f_2_dtc94_2nd_15d\", \n",
    "          \"f_3_dtc94_1th_15d\", \n",
    "          \"f_3_dtc94_2nd_15d\", \n",
    "          \"f_4_dtc94_1th_15d\", \n",
    "          \"f_4_dtc94_2nd_15d\", \n",
    "          \"f_5_dtc94_1th_15d\", \n",
    "          \"f_5_dtc94_2nd_15d\", \n",
    "          \"f_6_dtc94_1th_15d\", \n",
    "          \"f_6_dtc94_2nd_15d\", \n",
    "          \"f_7_dtc94_1th_15d\", \n",
    "          \"f_7_dtc94_2nd_15d\", \n",
    "          \"f_8_dtc94_1th_15d\", \n",
    "          \"f_8_dtc94_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc0401_1th_15d\", \n",
    "          \"f_1_dtc0401_2nd_15d\", \n",
    "          \"f_2_dtc0401_1th_15d\", \n",
    "          \"f_2_dtc0401_2nd_15d\", \n",
    "          \"f_3_dtc0401_1th_15d\", \n",
    "          \"f_3_dtc0401_2nd_15d\", \n",
    "          \"f_4_dtc0401_1th_15d\", \n",
    "          \"f_4_dtc0401_2nd_15d\", \n",
    "          \"f_5_dtc0401_1th_15d\", \n",
    "          \"f_5_dtc0401_2nd_15d\", \n",
    "          \"f_6_dtc0401_1th_15d\", \n",
    "          \"f_6_dtc0401_2nd_15d\", \n",
    "          \"f_7_dtc0401_1th_15d\", \n",
    "          \"f_7_dtc0401_2nd_15d\", \n",
    "          \"f_8_dtc0401_1th_15d\", \n",
    "          \"f_8_dtc0401_2nd_15d\",\n",
    "\n",
    "\n",
    "          \"f_1_dtc2457_1th_15d\", \n",
    "          \"f_1_dtc2457_2nd_15d\", \n",
    "          \"f_2_dtc2457_1th_15d\", \n",
    "          \"f_2_dtc2457_2nd_15d\", \n",
    "          \"f_3_dtc2457_1th_15d\", \n",
    "          \"f_3_dtc2457_2nd_15d\", \n",
    "          \"f_4_dtc2457_1th_15d\", \n",
    "          \"f_4_dtc2457_2nd_15d\", \n",
    "          \"f_5_dtc2457_1th_15d\", \n",
    "          \"f_5_dtc2457_2nd_15d\", \n",
    "          \"f_6_dtc2457_1th_15d\", \n",
    "          \"f_6_dtc2457_2nd_15d\", \n",
    "          \"f_7_dtc2457_1th_15d\", \n",
    "          \"f_7_dtc2457_2nd_15d\", \n",
    "          \"f_8_dtc2457_1th_15d\", \n",
    "          \"f_8_dtc2457_2nd_15d\",\n",
    "\n",
    "          \"if_parts_replaced_in_1th_15d\", \n",
    "          \"if_parts_replaced_in_2nd_15d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75cf5e13-2336-48ce-ad2b-a04c0f987935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def whether_thisVIN_data_already_generated(thisVIN): \n",
    "    df_VINs_data = spark.sql(\"SELECT * FROM VINs_data\")\n",
    "    df_VINs_data = df_VINs_data.toDF(\"VIN\", \"TOTAL_ROWS\")\n",
    "    df_VINs_data_p = df_VINs_data.toPandas()\n",
    "    for index, row in df_VINs_data_p.iterrows():\n",
    "        if row['VIN'] == thisVIN:\n",
    "          return 1\n",
    "    return 0\n",
    "\n",
    "# Function to submit a SLURM job and return the job ID\n",
    "def submit_job1(script_path, thisVIN, account='vuh14_dibbs_sc', partition='sla-prio'):\n",
    "    submit_command = f\"sbatch --account={account} --partition={partition} {script_path} {thisVIN}\"\n",
    "    print(\"The submit command for the submitted job is: \")\n",
    "    print(submit_command)\n",
    "    result = subprocess.run(submit_command, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        job_id = re.findall(r'\\d+', result.stdout)[0]\n",
    "        print(f\"Job {job_id} submitted successfully.\")\n",
    "        return job_id\n",
    "    else:\n",
    "        print(\"Job submission failed.\")\n",
    "        return None\n",
    "\n",
    "# Function to submit a SLURM job and return the job ID\n",
    "def submit_job2(script_path, thisVIN, calendar_day_from_where_it_left_off, account='vuh14_dibbs_sc', partition='sla-prio'):\n",
    "    submit_command = f\"sbatch --account={account} --partition={partition} {script_path} {thisVIN} {calendar_day_from_where_it_left_off}\"\n",
    "    print(\"The submit command for the submitted job is: \")\n",
    "    print(submit_command)\n",
    "    result = subprocess.run(submit_command, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        job_id = re.findall(r'\\d+', result.stdout)[0]\n",
    "        print(f\"Job {job_id} submitted successfully.\")\n",
    "        return job_id\n",
    "    else:\n",
    "        print(\"Job submission failed.\")\n",
    "        return None\n",
    " \n",
    "# Function to check the status of multiple jobs\n",
    "def check_jobs_status(job_ids):\n",
    "    jobs_running = set(job_ids)\n",
    "    while jobs_running:\n",
    "        for job_id in list(jobs_running):\n",
    "            status_command = f\"squeue -j {job_id}\"\n",
    "            result = subprocess.run(status_command, shell=True, capture_output=True, text=True)\n",
    "            if not result.stdout:  # If the job is no longer in the queue\n",
    "                print(f\"Job {job_id} completed.\")\n",
    "                jobs_running.remove(job_id)\n",
    "        if jobs_running:\n",
    "            print(\"Some jobs are still running...\")\n",
    "            print(jobs_running)\n",
    "            t.sleep(1)  # Check every 5 seconds\n",
    "\n",
    "# Function to retrieve job outputs\n",
    "def get_jobs_output(output_files):\n",
    "    for output_file in output_files:\n",
    "        try:\n",
    "            with open(output_file, \"r\") as file:\n",
    "                output = file.read()\n",
    "            print(f\"Output from {output_file}:\")\n",
    "            print(output)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Output file {output_file} not found.\")\n",
    "            \n",
    "            \n",
    "def update_VINs_data_total_rows_from_resulted_data(vins_data_path, resulted_data_path, headerList):\n",
    "\n",
    "    # Step 1: Read VINs_data.csv\n",
    "    df_vins = pd.read_csv(vins_data_path, names=[\"VIN\", \"TOTAL_ROWS\"], dtype=str, header=None)\n",
    "\n",
    "    # Step 2: Read resultedData.csv with headerList, skipping bad lines\n",
    "    df_resulted = pd.read_csv(resulted_data_path, names=headerList, header=None, on_bad_lines='skip')\n",
    "\n",
    "    # Step 3: Ensure calendar_day is numeric\n",
    "    df_resulted[\"calendar_day\"] = pd.to_numeric(df_resulted[\"calendar_day\"], errors='coerce')\n",
    "\n",
    "    # Step 4: Filter and update VINs\n",
    "    updated_rows = []\n",
    "\n",
    "    for index, row in df_vins.iterrows():\n",
    "        vin = row['VIN']\n",
    "        matching_rows = df_resulted[df_resulted['VIN'] == vin]\n",
    "\n",
    "        if not matching_rows.empty:\n",
    "            max_day = matching_rows['calendar_day'].max()\n",
    "            print(f\" VIN: {vin} → Max calendar_day: {max_day}, Updating TOTAL_ROWS to: {int(max_day)}\")\n",
    "            updated_rows.append([vin, int(max_day)])\n",
    "        else:\n",
    "            print(f\" VIN: {vin} → No match in resultedData → Setting TOTAL_ROWS to 0\")\n",
    "            updated_rows.append([vin, 0])  # Mark for later removal\n",
    "\n",
    "    # Step 5: Create DataFrame and remove TOTAL_ROWS == 0\n",
    "    updated_df = pd.DataFrame(updated_rows, columns=[\"VIN\", \"TOTAL_ROWS\"])\n",
    "    updated_df = updated_df[updated_df[\"TOTAL_ROWS\"] != 0]\n",
    "\n",
    "    # Step 6: Overwrite VINs_data.csv\n",
    "    updated_df.to_csv(vins_data_path, index=False, header=False)\n",
    "\n",
    "    print(f\"\\n VINs_data.csv updated. Final row count (excluding TOTAL_ROWS=0): {len(updated_df)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def is_valid_vin(vin: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns 1 if the VIN is valid, 0 if not.\n",
    "    A valid VIN is exactly 17 characters long and does not contain I, O, or Q.\n",
    "    \"\"\"\n",
    "    vin = vin.strip().upper()\n",
    "\n",
    "    # Rule 1: VIN must be exactly 17 characters\n",
    "    if len(vin) != 17:\n",
    "        return 0\n",
    "\n",
    "    # Rule 2: Must only contain A-H, J-N, P, R-Z, and 0-9 (excluding I, O, Q)\n",
    "    if not re.match(\"^[A-HJ-NPR-Z0-9]{17}$\", vin):\n",
    "        return 0\n",
    "\n",
    "    return 1  \n",
    "    \n",
    "    \n",
    "def update_VINs_data_from_resulted_data(vins_data_path, resulted_data_path, columnNames):\n",
    "\n",
    "   \n",
    "    #  Step 1: Load VINs_data.csv\n",
    "    df_vins = pd.read_csv(VINs_data_file_path, names=[\"VIN\", \"TOTAL_ROWS\"], dtype={\"VIN\": str, \"TOTAL_ROWS\": int}, header=None)\n",
    "\n",
    "    #  Step 2: Load resultedData.csv (skip bad lines)\n",
    "    df_resulted = pd.read_csv(resulted_data_path, names=columnNames, header=None, on_bad_lines='skip')\n",
    "\n",
    "    #  Step 3: Convert calendar_day to numeric\n",
    "    df_resulted[\"calendar_day\"] = pd.to_numeric(df_resulted[\"calendar_day\"], errors='coerce')\n",
    "\n",
    "    #  Step 4: Group by VIN → max calendar_day\n",
    "    grouped = df_resulted.groupby(\"VIN\")[\"calendar_day\"].max().reset_index()\n",
    "    grouped.columns = [\"VIN\", \"MAX_CALENDAR_DAY\"]\n",
    "\n",
    "    #  Step 5: Build dictionary of existing VINs\n",
    "    vins_dict = dict(zip(df_vins[\"VIN\"], df_vins[\"TOTAL_ROWS\"]))\n",
    "\n",
    "    #  Step 6: Update or add VINs\n",
    "    updated_rows = []\n",
    "\n",
    "    for _, row in grouped.iterrows():\n",
    "        vin = row[\"VIN\"]\n",
    "        max_day_val = row[\"MAX_CALENDAR_DAY\"]\n",
    "\n",
    "        if(is_valid_vin(vin)==0):\n",
    "            print(f\" Skipping VIN: {vin} because it is invalid.\")\n",
    "            continue\n",
    "\n",
    "        if pd.isna(max_day_val):\n",
    "            print(f\" Skipping VIN: {vin} → MAX_CALENDAR_DAY is NaN\")\n",
    "            continue\n",
    "\n",
    "        max_day = int(max_day_val)\n",
    "\n",
    "        if vin in vins_dict:\n",
    "            print(f\" Updating VIN: {vin} → TOTAL_ROWS: {vins_dict[vin]} → {max_day}\")\n",
    "        else:\n",
    "            print(f\" Adding new VIN: {vin} → TOTAL_ROWS: {max_day}\")\n",
    "\n",
    "        updated_rows.append([vin, max_day])\n",
    "\n",
    "    #  Step 7: Add VINs from original not in grouped\n",
    "    existing_vins_set = set(grouped[\"VIN\"])\n",
    "    for vin, total_rows in vins_dict.items():\n",
    "        if vin not in existing_vins_set:\n",
    "            updated_rows.append([vin, total_rows])\n",
    "            print(f\" Keeping existing VIN: {vin} → TOTAL_ROWS: {total_rows}\")\n",
    "\n",
    "    #  Step 8: Save updated VINs\n",
    "    df_final = pd.DataFrame(updated_rows, columns=[\"VIN\", \"TOTAL_ROWS\"])\n",
    "    df_final.to_csv(VINs_data_file_path, index=False, header=False)\n",
    "\n",
    "    print(f\"\\n VINs_data.csv successfully updated. Total VINs: {len(df_final)}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def remove_invalid_calendar_days_per_vin(population_path, resulted_data_path, columnNames, output_path=None):\n",
    "    # Read population to get VIN and INS_DATE\n",
    "    df_pop = pd.read_csv(population_path, usecols=[\"VIN\", \"INS_DATE\"])\n",
    "    df_pop[\"INS_DATE\"] = pd.to_datetime(df_pop[\"INS_DATE\"], errors=\"coerce\")\n",
    "    df_pop.dropna(subset=[\"INS_DATE\"], inplace=True)\n",
    "\n",
    "    # Read resulted data\n",
    "    df_resulted = pd.read_csv(resulted_data_path, names=columnNames, header=None, on_bad_lines='skip')\n",
    "\n",
    "    # Ensure calendar_day is numeric\n",
    "    df_resulted[\"calendar_day\"] = pd.to_numeric(df_resulted[\"calendar_day\"], errors='coerce')\n",
    "\n",
    "    # Reference date\n",
    "    reference_date = datetime(2021, 12, 31)\n",
    "\n",
    "    # Create dictionary of max allowed days (M) per VIN\n",
    "    vin_to_max_days = {\n",
    "        row[\"VIN\"]: (reference_date - row[\"INS_DATE\"]).days\n",
    "        for _, row in df_pop.iterrows()\n",
    "    }\n",
    "\n",
    "    print(\"Filtering rows with invalid calendar_day values...\")\n",
    "    original_count = len(df_resulted)\n",
    "\n",
    "    # Remove rows where calendar_day > M for each VIN\n",
    "    df_filtered = df_resulted[df_resulted.apply(\n",
    "        lambda row: (\n",
    "            row[\"VIN\"] in vin_to_max_days and\n",
    "            pd.notna(row[\"calendar_day\"]) and\n",
    "            row[\"calendar_day\"] <= vin_to_max_days[row[\"VIN\"]]\n",
    "        ),\n",
    "        axis=1\n",
    "    )]\n",
    "\n",
    "    filtered_count = len(df_filtered)\n",
    "    print(f\"Original rows: {original_count}\")\n",
    "    print(f\"Filtered rows: {filtered_count}\")\n",
    "    print(f\"Removed rows: {original_count - filtered_count}\")\n",
    "\n",
    "    # Save cleaned file\n",
    "    output_path = output_path if output_path else resulted_data_path\n",
    "    df_filtered.to_csv(output_path, index=False, header=False)\n",
    "\n",
    "    print(f\"Filtered resultedData saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943cb4ff-422c-4413-9f5e-89384093f5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updating VIN: 1M1AN07Y3GM021042 → TOTAL_ROWS: 0 → 1014\n",
      " Updating VIN: 1M1AN2AY5LM001812 → TOTAL_ROWS: 785 → 785\n",
      " Updating VIN: 1M1AN2GY3KM006268 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 1M1AN3GY4LM019358 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 1M1AN4GY0MM022961 → TOTAL_ROWS: 368 → 368\n",
      " Updating VIN: 1M1AN4GY7KM008343 → TOTAL_ROWS: 983 → 983\n",
      " Updating VIN: 1M1AN4GY8MM021279 → TOTAL_ROWS: 508 → 508\n",
      " Updating VIN: 1M1AN4GYXLM018348 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 1M1AW01Y2FM006911 → TOTAL_ROWS: 0 → 1433\n",
      " Updating VIN: 1M1AW01Y5JM010413 → TOTAL_ROWS: 24 → 24\n",
      " Updating VIN: 1M1AW02Y1FM050302 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 1M1AW02Y7FM051728 → TOTAL_ROWS: 764 → 764\n",
      " Updating VIN: 1M1AW07Y4FM045037 → TOTAL_ROWS: 0 → 2414\n",
      " Updating VIN: 1M2AU02C6GM010563 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 1M2AU04C5FM008959 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 1M2AV02C5JM018223 → TOTAL_ROWS: 0 → 1334\n",
      " Updating VIN: 1M2AV02C6HM016569 → TOTAL_ROWS: 0 → 1873\n",
      " Updating VIN: 1M2AX04C3GM030029 → TOTAL_ROWS: 0 → 1523\n",
      " Updating VIN: 1M2AX04C7GM027070 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 1M2AX04C9HM034281 → TOTAL_ROWS: 0 → 1948\n",
      " Updating VIN: 1M2AX07C0HM032978 → TOTAL_ROWS: 374 → 374\n",
      " Updating VIN: 1M2AX07C7FM024311 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 1M2AX13C1GM035012 → TOTAL_ROWS: 0 → 1889\n",
      " Updating VIN: 1M2AX13C3HM037068 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 1M2AX13C4GM031245 → TOTAL_ROWS: 0 → 2199\n",
      " Updating VIN: 1M2AX13C4JM040440 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 1M2AX13C9FM027402 → TOTAL_ROWS: 0 → 2108\n",
      " Updating VIN: 1M2GR2GC1LM015017 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 1M2GR2GC6LM012730 → TOTAL_ROWS: 973 → 973\n",
      " Updating VIN: 1M2GR4GC0LM014601 → TOTAL_ROWS: 694 → 694\n",
      " Updating VIN: 1M2GR4GC0LM018549 → TOTAL_ROWS: 685 → 685\n",
      " Updating VIN: 1M2GR4GCXKM004284 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 1M2LR05C3HM001875 → TOTAL_ROWS: 0 → 1919\n",
      " Updating VIN: 1M2TE2GC1LM003511 → TOTAL_ROWS: 977 → 977\n",
      " Updating VIN: 4V4M19EH8FN184640 → TOTAL_ROWS: 0 → 1479\n",
      " Updating VIN: 4V4MC9EG6FN178787 → TOTAL_ROWS: 0 → 2054\n",
      " Updating VIN: 4V4N19EH5LN221692 → TOTAL_ROWS: 883 → 883\n",
      " Updating VIN: 4V4N99EH3HN975934 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V4NC9DH1KN205824 → TOTAL_ROWS: 24 → 24\n",
      " Updating VIN: 4V4NC9EH0GN960694 → TOTAL_ROWS: 759 → 759\n",
      " Updating VIN: 4V4NC9EH0HN991459 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V4NC9EH0LN264590 → TOTAL_ROWS: 760 → 760\n",
      " Updating VIN: 4V4NC9EH1FN911535 → TOTAL_ROWS: 728 → 728\n",
      " Updating VIN: 4V4NC9EH1FN933289 → TOTAL_ROWS: 0 → 1013\n",
      " Updating VIN: 4V4NC9EH1GN938316 → TOTAL_ROWS: 0 → 2168\n",
      " Updating VIN: 4V4NC9EH1GN973065 → TOTAL_ROWS: 24 → 24\n",
      " Updating VIN: 4V4NC9EH2FN919420 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V4NC9EH2LN238489 → TOTAL_ROWS: 0 → 1054\n",
      " Updating VIN: 4V4NC9EH2NN289266 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V4NC9EH4GN952713 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V4NC9EH4HN977144 → TOTAL_ROWS: 0 → 2054\n",
      " Updating VIN: 4V4NC9EH4JN887837 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V4NC9EH4JN893072 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V4NC9EH4JN998906 → TOTAL_ROWS: 0 → 1306\n",
      " Updating VIN: 4V4NC9EH5HN990971 → TOTAL_ROWS: 578 → 578\n",
      " Updating VIN: 4V4NC9EH5JN894425 → TOTAL_ROWS: 893 → 893\n",
      " Updating VIN: 4V4NC9EH5LN205325 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V4NC9EH6GN942846 → TOTAL_ROWS: 0 → 1876\n",
      " Updating VIN: 4V4NC9EH7JN998821 → TOTAL_ROWS: 0 → 1259\n",
      " Updating VIN: 4V4NC9EH7LN210218 → TOTAL_ROWS: 891 → 891\n",
      " Updating VIN: 4V4NC9EH8FN139017 → TOTAL_ROWS: 2289 → 2289\n",
      " Updating VIN: 4V4NC9EH8GN944498 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V4NC9EH8GN961902 → TOTAL_ROWS: 759 → 759\n",
      " Updating VIN: 4V4NC9EH8JN894497 → TOTAL_ROWS: 1393 → 1393\n",
      " Updating VIN: 4V4NC9EH9FN916966 → TOTAL_ROWS: 1238 → 1238\n",
      " Updating VIN: 4V4NC9EH9GN946633 → TOTAL_ROWS: 584 → 584\n",
      " Updating VIN: 4V4NC9EH9LN243320 → TOTAL_ROWS: 966 → 966\n",
      " Updating VIN: 4V4NC9EHXKN208073 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V4NC9EJ3FN918604 → TOTAL_ROWS: 2378 → 2378\n",
      " Updating VIN: 4V4NC9EJ3FN937685 → TOTAL_ROWS: 923 → 923\n",
      " Updating VIN: 4V4NC9EJ5GN935339 → TOTAL_ROWS: 359 → 359\n",
      " Updating VIN: 4V4NC9EJ5HN982498 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V4NC9EJ8KN193057 → TOTAL_ROWS: 1183 → 1183\n",
      " Updating VIN: 4V4NC9EJXFN187833 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V4W19EG2KN216850 → TOTAL_ROWS: 1059 → 1059\n",
      " Updating VIN: 4V4WC9EG9MN295348 → TOTAL_ROWS: 318 → 318\n",
      " Updating VIN: 4V4WC9EH0MN600342 → TOTAL_ROWS: 422 → 422\n",
      " Updating VIN: 4V4WC9EH7MN276645 → TOTAL_ROWS: 29 → 29\n",
      " Updating VIN: 4V5KG9EH9LN251730 → TOTAL_ROWS: 764 → 764\n",
      " Updating VIN: 4V5KG9EJ5KN217477 → TOTAL_ROWS: 29 → 29\n",
      " Keeping existing VIN: 1M1AN4GY0MM022958 → TOTAL_ROWS: 0\n",
      " Keeping existing VIN: 1M2GR2GC2MM025105 → TOTAL_ROWS: 0\n",
      " Keeping existing VIN: 1M2GR3GC2MM022467 → TOTAL_ROWS: 29\n",
      " Keeping existing VIN: 1M2GR4GC8LM017763 → TOTAL_ROWS: 29\n",
      " Keeping existing VIN: 4V4NC9EH5NN289777 → TOTAL_ROWS: 2468\n",
      "\n",
      " VINs_data.csv successfully updated. Total VINs: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of df_all_pop_VINs=322178, df_generated_pop_VINs = 85\n",
      "VIN=1M1AN07Y3GM021042 has TOTAL_ROWS=1014, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 1M1AN07Y3GM021042 1014\n",
      "Job 36760211 submitted successfully.\n",
      "VIN=1M1AW01Y2FM006911 has TOTAL_ROWS=1433, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 1M1AW01Y2FM006911 1433\n",
      "Job 36760212 submitted successfully.\n",
      "VIN=1M1AW07Y4FM045037 has TOTAL_ROWS=2414, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 1M1AW07Y4FM045037 2414\n",
      "Job 36760213 submitted successfully.\n",
      "VIN=1M2AV02C5JM018223 has TOTAL_ROWS=1334, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 1M2AV02C5JM018223 1334\n",
      "Job 36760214 submitted successfully.\n",
      "VIN=1M2AV02C6HM016569 has TOTAL_ROWS=1873, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 1M2AV02C6HM016569 1873\n",
      "Job 36760215 submitted successfully.\n",
      "VIN=1M2AX04C3GM030029 has TOTAL_ROWS=1523, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 1M2AX04C3GM030029 1523\n",
      "Job 36760216 submitted successfully.\n",
      "VIN=1M2AX04C9HM034281 has TOTAL_ROWS=1948, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 1M2AX04C9HM034281 1948\n",
      "Job 36760217 submitted successfully.\n",
      "VIN=1M2AX13C1GM035012 has TOTAL_ROWS=1889, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 1M2AX13C1GM035012 1889\n",
      "Job 36760218 submitted successfully.\n",
      "VIN=1M2AX13C4GM031245 has TOTAL_ROWS=2199, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 1M2AX13C4GM031245 2199\n",
      "Job 36760219 submitted successfully.\n",
      "VIN=1M2AX13C9FM027402 has TOTAL_ROWS=2108, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 1M2AX13C9FM027402 2108\n",
      "Job 36760220 submitted successfully.\n",
      "VIN=1M2LR05C3HM001875 has TOTAL_ROWS=1919, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 1M2LR05C3HM001875 1919\n",
      "Job 36760221 submitted successfully.\n",
      "VIN=4V4M19EH8FN184640 has TOTAL_ROWS=1479, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 4V4M19EH8FN184640 1479\n",
      "Job 36760222 submitted successfully.\n",
      "VIN=4V4MC9EG6FN178787 has TOTAL_ROWS=2054, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 4V4MC9EG6FN178787 2054\n",
      "Job 36760223 submitted successfully.\n",
      "VIN=4V4NC9EH1FN933289 has TOTAL_ROWS=1013, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 4V4NC9EH1FN933289 1013\n",
      "Job 36760224 submitted successfully.\n",
      "VIN=4V4NC9EH1GN938316 has TOTAL_ROWS=2168, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 4V4NC9EH1GN938316 2168\n",
      "Job 36760225 submitted successfully.\n",
      "VIN=4V4NC9EH2LN238489 has TOTAL_ROWS=1054, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 4V4NC9EH2LN238489 1054\n",
      "Job 36760226 submitted successfully.\n",
      "VIN=4V4NC9EH4HN977144 has TOTAL_ROWS=2054, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 4V4NC9EH4HN977144 2054\n",
      "Job 36760227 submitted successfully.\n",
      "VIN=4V4NC9EH4JN998906 has TOTAL_ROWS=1306, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 4V4NC9EH4JN998906 1306\n",
      "Job 36760228 submitted successfully.\n",
      "VIN=4V4NC9EH6GN942846 has TOTAL_ROWS=1876, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 4V4NC9EH6GN942846 1876\n",
      "Job 36760229 submitted successfully.\n",
      "VIN=4V4NC9EH7JN998821 has TOTAL_ROWS=1259, which is < 2556 so partial...\n",
      "The submit command for the submitted job is: \n",
      "sbatch --account=vuh14_dibbs_sc --partition=sla-prio /storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh 4V4NC9EH7JN998821 1259\n",
      "Job 36760230 submitted successfully.\n",
      "Hit the job limit (20) while processing partial VINs.\n",
      "Some jobs are still running...\n",
      "{'36760216', '36760222', '36760229', '36760227', '36760223', '36760225', '36760221', '36760220', '36760218', '36760228', '36760224', '36760219', '36760211', '36760217', '36760226', '36760213', '36760212', '36760214', '36760215', '36760230'}\n",
      "Some jobs are still running...\n",
      "{'36760216', '36760222', '36760229', '36760227', '36760223', '36760225', '36760221', '36760220', '36760218', '36760228', '36760224', '36760219', '36760211', '36760217', '36760226', '36760213', '36760212', '36760214', '36760215', '36760230'}\n",
      "Some jobs are still running...\n",
      "{'36760216', '36760222', '36760229', '36760227', '36760223', '36760225', '36760221', '36760220', '36760218', '36760228', '36760224', '36760219', '36760211', '36760217', '36760226', '36760213', '36760212', '36760214', '36760215', '36760230'}\n"
     ]
    }
   ],
   "source": [
    "VINs_data_file_path = '/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv'\n",
    "resulted_data_path = '/storage/home/yqf5148/work/volvoPennState/data/dataset/resultedData.csv'\n",
    "\n",
    " # Columns: Keep selected + headers\n",
    "selected_features_from_population = ['VIN', 'ENGINE_SIZE', 'ENGINE_HP', 'VEH_TYPE'] + [s for s in df_population.columns if 'KOLA' in s]\n",
    "all_columns_names = selected_features_from_population + headerList\n",
    "    \n",
    "\n",
    "# remove_invalid_calendar_days_per_vin(\n",
    "#     population_path=\"/storage/home/yqf5148/work/volvoPennState/PopulationWithChassisId.csv\",\n",
    "#     resulted_data_path=\"/storage/home/yqf5148/work/volvoPennState/data/dataset/resultedData.csv\",\n",
    "#     columnNames = all_columns_names\n",
    "# )\n",
    "\n",
    "\n",
    "# update_VINs_data_total_rows_from_resulted_data(VINs_data_file_path, resulted_data_path, all_columns_names)\n",
    "update_VINs_data_from_resulted_data(VINs_data_file_path, resulted_data_path, all_columns_names)\n",
    "\n",
    "\n",
    "df_all_pop_VINs = spark.sql('select distinct VIN from population')\n",
    "df_all_pop_VINs_p = df_all_pop_VINs.toPandas()\n",
    "\n",
    "# Read the VINs_data.csv (with header row: VIN,TOTAL_ROWS)\n",
    "df_generated_pop_VINs = pd.read_csv(\n",
    "    VINs_data_file_path, sep=',', names=['VIN','TOTAL_ROWS'],\n",
    "    header=None, dtype=str\n",
    ")\n",
    "\n",
    "print(f\"size of df_all_pop_VINs={len(df_all_pop_VINs_p)}, df_generated_pop_VINs = {len(df_generated_pop_VINs)}\")\n",
    "\n",
    "# Only proceed if the population has more VINs than the generated set\n",
    "if len(df_all_pop_VINs_p) > len(df_generated_pop_VINs):   \n",
    "\n",
    "    # Let this be your overall job limit\n",
    "    number_of_jobs = 20\n",
    "    jobs_submitted = 0\n",
    "    job_ids = []\n",
    "    output_files = []\n",
    "\n",
    "    try:\n",
    "        if len(df_generated_pop_VINs) > 0:\n",
    "            df_generated_pop_VINs['TOTAL_ROWS'] = df_generated_pop_VINs['TOTAL_ROWS'].astype(int)\n",
    "            processed_VINs = set(df_generated_pop_VINs['VIN'].astype(str))\n",
    "        else:\n",
    "            processed_VINs = set()\n",
    "    except FileNotFoundError:\n",
    "        print(\"VINs_data.csv not found. Starting fresh.\")\n",
    "        processed_VINs = set()\n",
    "\n",
    "    # --- 1) Process partially generated VINs (TOTAL_ROWS < 2556) --- The second condition is temporary\n",
    "    df_incomplete = df_generated_pop_VINs[df_generated_pop_VINs['TOTAL_ROWS'].between(999, 2557)]\n",
    "\n",
    "    \n",
    "    \n",
    "    for idx, row in df_incomplete.iterrows():\n",
    "        # If we've hit the limit, no more submissions\n",
    "        if jobs_submitted >= number_of_jobs:\n",
    "            break\n",
    "\n",
    "        thisVIN = row['VIN']\n",
    "        rows_val = row['TOTAL_ROWS']\n",
    "        print(f\"VIN={thisVIN} has TOTAL_ROWS={rows_val}, which is < 2556 so partial...\")\n",
    "\n",
    "        # Example usage: pass the last day as param\n",
    "        calendar_day_from_where_it_left_off = rows_val\n",
    "\n",
    "        # Submit partial job\n",
    "        df_filtered_population_for_this_VIN = df_all_pop_VINs.filter(f.col('VIN') == thisVIN)\n",
    "        script_path = \"/storage/home/yqf5148/work/volvoPennState/Jobs/submit_partiallyCalculatedJobDone.sh\"\n",
    "        output_file = f\"/storage/home/yqf5148/work/volvoPennState/Jobs/log_test/result_{thisVIN}.txt\"\n",
    "        job_id = submit_job2(script_path, thisVIN, calendar_day_from_where_it_left_off)\n",
    "\n",
    "        if job_id:\n",
    "            job_ids.append(job_id)\n",
    "            output_files.append(output_file)\n",
    "            processed_VINs.add(thisVIN)\n",
    "\n",
    "            # Overwrite VINs_data.csv entry with 0\n",
    "            new_VIN_entry = pd.DataFrame([[thisVIN, 0]], columns=['VIN','TOTAL_ROWS'])\n",
    "            new_VIN_entry.to_csv(VINs_data_file_path, mode='a', index=False, header=False)\n",
    "\n",
    "            jobs_submitted += 1\n",
    "\n",
    "    # If we've used up all jobs in partial VINs alone, we're done\n",
    "    if jobs_submitted >= number_of_jobs:\n",
    "        print(f\"Hit the job limit ({number_of_jobs}) while processing partial VINs.\")\n",
    "        check_jobs_status(job_ids)\n",
    "        get_jobs_output(output_files)\n",
    "        sys.exit(0)  # Or just return if in a function\n",
    "\n",
    "    # --- 2) Now do random selection for ungenerated VINs ---\n",
    "    # The code below remains the same, but includes a check so we don’t exceed the limit.\n",
    "    \n",
    "    total_VINs = len(df_all_pop_VINs_p)\n",
    "    print(f\"total_VINs: {total_VINs}\")\n",
    "\n",
    "    while jobs_submitted < number_of_jobs and len(processed_VINs) < total_VINs:\n",
    "        random_index = random.randint(0, total_VINs - 1)\n",
    "        print(f\"Random generated index in overall {total_VINs} VINs is {random_index}\")\n",
    "        row = df_all_pop_VINs_p.iloc[random_index]\n",
    "        thisVIN = str(row['VIN'])\n",
    "\n",
    "        if thisVIN in processed_VINs or whether_thisVIN_data_already_generated(thisVIN) == 1:\n",
    "            print(f\"This VIN = {thisVIN}'s data is already generated. Selecting a new one...\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"This VIN = {thisVIN}'s data is not generated.\")\n",
    "            df_filtered_population_for_this_VIN = df_all_pop_VINs.filter(f.col('VIN') == thisVIN)\n",
    "            script_path = \"/storage/home/yqf5148/work/volvoPennState/Jobs/submit.sh\"\n",
    "            output_file = f\"/storage/home/yqf5148/work/volvoPennState/Jobs/log_test/result_{thisVIN}.txt\"\n",
    "            job_id = submit_job1(script_path, thisVIN)\n",
    "\n",
    "            if job_id:\n",
    "                job_ids.append(job_id)\n",
    "                output_files.append(output_file)\n",
    "                processed_VINs.add(thisVIN)\n",
    "\n",
    "                # Append a new entry\n",
    "                new_VIN_entry = pd.DataFrame([[thisVIN, 0]], columns=['VIN','TOTAL_ROWS'])\n",
    "                new_VIN_entry.to_csv(VINs_data_file_path, mode='a', index=False, header=False)\n",
    "\n",
    "                jobs_submitted += 1\n",
    "\n",
    "            # Stop if we hit the limit\n",
    "            if jobs_submitted >= number_of_jobs:\n",
    "                print(f\"Hit the job limit ({number_of_jobs}) while picking random VINs.\")\n",
    "                break\n",
    "\n",
    "    # --- 3) Check job status & retrieve outputs ---\n",
    "    check_jobs_status(job_ids)\n",
    "    get_jobs_output(output_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f2de50-a8dd-4a6b-8e2b-62139374f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_pop_VINs_p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volvopennstate_new_kernel",
   "language": "python",
   "name": "volvopennstate_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
