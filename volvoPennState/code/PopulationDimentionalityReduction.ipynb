{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d024bf3-3c33-4060-b854-07c3fd672b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
      "env: PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin\n"
     ]
    }
   ],
   "source": [
    "%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
    "%env PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7201403-8b2e-4405-83fc-082bde805089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ps: /storage/icds/RISE/sw8/anaconda/anaconda3/envs/tensorflow/lib/libuuid.so.1: no version information available (required by /usr/lib64/libblkid.so.1)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/22 16:54:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyspark as psk\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import time as t \n",
    "from datetime import date, datetime, timedelta\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import multiprocessing\n",
    "from functools import reduce  \n",
    "from math import modf\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from delta import * \n",
    "from delta.tables import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "# import covalent as ct\n",
    "\n",
    "import subprocess\n",
    "import re\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# import umap.umap_ as umap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "import gc  # import gorbage collector to resolve the problem of restarting kernel due to large table of population loading in RAM to append\n",
    "from pyspark.sql.functions import year, col\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import builtins\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# os.environ['PYDEVD_DISABLE_FILE_VALIDATION']=1\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "       .config(\"spark.ui.port\", \"4050\") \\\n",
    "       .master(\"local[2]\") \\\n",
    "       .appName(\"MyApp\") \\\n",
    "       .config(\"spark.driver.maxResultSize\", \"40g\")\\\n",
    "       .config(\"spark.driver.memory\", \"140g\")\\\n",
    "       .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "#both works\n",
    "# 1: \n",
    "# spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")    #To resolve the error for p1075_38 to_timestamp formating: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '1/2/2019 20:40:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n",
    "# Set Spark SQL legacy time parser policy to LEGACY to handle older date formats\n",
    "# 2:\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "# Increase the max fields in the string representation of a plan\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7747d09-4948-42bd-8374-4458d5761075",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data(problem).csv\") \\\n",
    "          .createOrReplaceTempView(\"VINs_data\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/PopulationWithChassisId.csv\") \\\n",
    "          .createOrReplaceTempView(\"population\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b090fc-e8e0-48a2-a54b-738bace16f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0af764d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all column names from the population table\n",
    "all_columns = spark.table(\"population\").columns\n",
    "\n",
    "# Select only columns that contain \"KOLA\" and exclude \"CHASSIS_ID\"\n",
    "kola_columns = [c for c in all_columns if \"KOLA\" in c and c != \"CHASSIS_ID\"]\n",
    "\n",
    "# Build the main DataFrame with selected columns\n",
    "df_population = spark.table(\"population\") \\\n",
    "    .select(\n",
    "        col(\"VIN\"),       \n",
    "        col(\"ENGINE_SIZE\"),\n",
    "        col(\"ENGINE_HP\"),\n",
    "        col(\"VEH_TYPE\"),\n",
    "        year(\"VEH_ASSEMB_DATE\").alias(\"VEH_ASSEMB_YEAR\"),\n",
    "        \n",
    "        *[col(c) for c in kola_columns]           # Include all KOLA-related columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ff3c49-e256-4cec-ba5b-c32bc4510f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The toPandas() finished!\n"
     ]
    }
   ],
   "source": [
    "selected_for_pca_cols = [c for c in df_population.columns if \"VIN\" not in c]\n",
    "not_selected_for_pca_cols = [c for c in df_population.columns if c not in selected_for_pca_cols]\n",
    "\n",
    "# print(non_kola_cols)\n",
    "df_non_selected_for_pca = df_population.select(*not_selected_for_pca_cols)\n",
    "df_selected_for_pca = df_population.select(*selected_for_pca_cols)\n",
    "\n",
    "df_selected_for_pca_pd = df_selected_for_pca.toPandas()  # convert from PySpark to Pandas\n",
    "print(f\"\\n The toPandas() finished!\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(f\"\\n The toPandas() finished!\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5199be3-b17a-4b2b-9549-539499525636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " üîÑ Converting to categorical data...\n",
      "\n",
      " The convert to category finished!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n üîÑ Converting to categorical data...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(\"\\n üîÑ Converting to categorical data...\")\n",
    "file.close()\n",
    "\n",
    "df_selected_for_pca_pd = df_selected_for_pca_pd.astype('category')\n",
    "\n",
    "print(f\"\\n The convert to category finished!\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(f\"\\n The convert to category finished!\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "919316b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " üöÄ Starting OneHotEncoding transformation...\n",
      "\n",
      " The OneHotEncoding finished!\n",
      "\n",
      " ‚úÖ onehot_df dataset shape: (322178, 7908)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n üöÄ Starting OneHotEncoding transformation...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(\"\\n üöÄ Starting OneHotEncoding transformation...\")\n",
    "file.close()\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "selected_for_pca_encoded = encoder.fit_transform(df_selected_for_pca_pd)\n",
    "\n",
    "print(f\"\\n The OneHotEncoding finished!\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(f\"\\n The OneHotEncoding finished!\")\n",
    "file.close()\n",
    "\n",
    "onehot_df = pd.DataFrame(selected_for_pca_encoded, columns=encoder.get_feature_names_out())\n",
    "print(f\"\\n ‚úÖ onehot_df dataset shape: {onehot_df.shape}\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(f\"\\n ‚úÖ onehot_df dataset shape: {onehot_df.shape}\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419e6ebe-b51f-459f-b0fe-53475b3484a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# This script reduces the dimensionality of a high-dimensional \n",
    "# one-hot encoded dataset by identifying and removing columns \n",
    "# that exhibit similar **value change patterns** across rows.\n",
    "#\n",
    "# Motivation:\n",
    "# In one-hot encoded data, many columns may carry redundant \n",
    "# information ‚Äî even if their exact values differ ‚Äî because they \n",
    "# follow similar change patterns across data rows. Comparing \n",
    "# columns directly (using `np.all`) is too strict, as it only \n",
    "# finds exact duplicates. Instead, we compare the **value \n",
    "# patterns** using Hamming similarity.\n",
    "#\n",
    "# Step-by-step Overview:\n",
    "# 1. Each column in the one-hot encoded DataFrame is label-encoded \n",
    "#    independently. This transforms categorical strings or binary \n",
    "#    encodings into integer codes that still preserve the pattern \n",
    "#    of value changes (e.g., A ‚Üí B ‚Üí A becomes 0 ‚Üí 1 ‚Üí 0).\n",
    "#\n",
    "# 2. Pairwise comparison of all columns is done using Hamming \n",
    "#    similarity ‚Äî calculated as the proportion of matching \n",
    "#    positions in the encoded column vectors.\n",
    "#\n",
    "# 3. If two columns have ‚â• 85% similarity in their encoded \n",
    "#    patterns, they are grouped together and considered redundant.\n",
    "#\n",
    "# 4. From each group of similar columns, only the first (as a \n",
    "#    representative) is retained, and the others are dropped.\n",
    "#\n",
    "# 5. All non-grouped (unique) columns are also retained.\n",
    "#\n",
    "# Final Output:\n",
    "# A reduced feature set with one representative column per \n",
    "# similarity group plus all ungrouped columns. This allows for \n",
    "# significant dimensionality reduction while preserving \n",
    "# meaningful variation across the dataset.\n",
    "# ==============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1536bc4-c83f-4167-a8da-cafc2af82105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " üîç Finding highly correlated feature columns...\n",
      "\n",
      "üîÑ Label encoding all columns in onehot_df...\n",
      "\n",
      " üîç Comparing columns for Hamming similarity = 0.8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7908/7908 [00:34<00:00, 230.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Representative columns = [0, 1, 3, 41, 52, 60, 67, 95, 136, 166, 168, 213, 239, 297, 315, 342, 346, 370, 384, 391, 399, 405, 408, 469, 474, 496, 500, 722, 773, 852, 855, 885, 919, 941, 945, 960, 962, 1109, 1146, 1170, 1193, 1328, 1425, 1785, 1998, 2000, 2019, 2035, 2073, 2082, 2104, 2267, 2272, 2352, 2354, 2426, 2443, 2518, 2538, 2812, 3077, 3585, 3598, 3609, 3863, 3864, 3965, 4088, 4155, 4165, 4166, 4219, 4220, 4279, 4474, 4555, 4562, 5248, 5358, 5360, 5478, 5996, 6494, 6585, 6586, 6720, 6976, 6978, 7363, 7376, 7399, 7482]\n",
      "\n",
      " Not grouped columns = [6658, 2051, 3078, 3593, 11, 14, 5646, 1552, 3603, 4630, 2583, 1560, 2584, 4631, 5656, 6683, 1565, 2599, 2093, 3630, 2613, 5173, 5174, 5176, 5177, 6718, 5698, 7235, 6223, 4178, 5207, 5215, 5220, 5223, 5226, 7799, 4216, 635, 2177, 1163, 140, 141, 1167, 2711, 1694, 6304, 6305, 1190, 167, 4268, 4783, 1200, 4784, 4790, 2236, 4291, 4805, 5833, 7890, 4314, 2786, 1256, 5353, 1259, 3824, 3825, 2303, 2818, 2307, 2823, 2318, 2319, 6944, 5413, 294, 7462, 296, 7478, 1851, 6985, 6987, 6988, 5455, 1360, 1363, 853, 6997, 6488, 6491, 2918, 2415, 7026, 883, 7028, 1397, 889, 890, 7546, 1404, 5502, 895, 897, 3971, 3972, 4484, 1417, 1929, 1419, 2444, 398, 1429, 1434, 3995, 1948, 3996, 6560, 4001, 4002, 6561, 7074, 4005, 7075, 947, 948, 7097, 954, 7098, 3004, 5570, 7106, 5574, 5575, 5577, 5586, 6613, 1494, 6615, 2521, 986, 988, 2542, 7157]\n",
      "\n",
      " Final selected columns = [0, 1, 3, 41, 52, 60, 67, 95, 136, 166, 168, 213, 239, 297, 315, 342, 346, 370, 384, 391, 399, 405, 408, 469, 474, 496, 500, 722, 773, 852, 855, 885, 919, 941, 945, 960, 962, 1109, 1146, 1170, 1193, 1328, 1425, 1785, 1998, 2000, 2019, 2035, 2073, 2082, 2104, 2267, 2272, 2352, 2354, 2426, 2443, 2518, 2538, 2812, 3077, 3585, 3598, 3609, 3863, 3864, 3965, 4088, 4155, 4165, 4166, 4219, 4220, 4279, 4474, 4555, 4562, 5248, 5358, 5360, 5478, 5996, 6494, 6585, 6586, 6720, 6976, 6978, 7363, 7376, 7399, 7482, 6658, 2051, 3078, 3593, 11, 14, 5646, 1552, 3603, 4630, 2583, 1560, 2584, 4631, 5656, 6683, 1565, 2599, 2093, 3630, 2613, 5173, 5174, 5176, 5177, 6718, 5698, 7235, 6223, 4178, 5207, 5215, 5220, 5223, 5226, 7799, 4216, 635, 2177, 1163, 140, 141, 1167, 2711, 1694, 6304, 6305, 1190, 167, 4268, 4783, 1200, 4784, 4790, 2236, 4291, 4805, 5833, 7890, 4314, 2786, 1256, 5353, 1259, 3824, 3825, 2303, 2818, 2307, 2823, 2318, 2319, 6944, 5413, 294, 7462, 296, 7478, 1851, 6985, 6987, 6988, 5455, 1360, 1363, 853, 6997, 6488, 6491, 2918, 2415, 7026, 883, 7028, 1397, 889, 890, 7546, 1404, 5502, 895, 897, 3971, 3972, 4484, 1417, 1929, 1419, 2444, 398, 1429, 1434, 3995, 1948, 3996, 6560, 4001, 4002, 6561, 7074, 4005, 7075, 947, 948, 7097, 954, 7098, 3004, 5570, 7106, 5574, 5575, 5577, 5586, 6613, 1494, 6615, 2521, 986, 988, 2542, 7157]\n",
      "\n",
      " ‚úÖ Reduced features from 7908 to 234\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n üîç Finding highly correlated feature columns...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(\"\\n üîç Finding highly correlated feature columns...\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "\n",
    "# Label encode each column individually\n",
    "print(\"\\nüîÑ Label encoding all columns in onehot_df...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(\"\\nüîÑ Label encoding all columns in onehot_df...\")\n",
    "file.close()\n",
    "\n",
    "label_encoded_df = onehot_df.copy()\n",
    "for col in label_encoded_df.columns:\n",
    "    encoder = LabelEncoder()\n",
    "    label_encoded_df[col] = encoder.fit_transform(label_encoded_df[col])\n",
    "\n",
    "# Initialize structures\n",
    "similar_cols = []\n",
    "grouped = set()\n",
    "groups = defaultdict(list)\n",
    "\n",
    "# threshold = 0.85  # 85% similarity threshold\n",
    "\n",
    "print(f\"\\n üîç Comparing columns for Hamming similarity = {threshold}...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(\"\\n üîç Comparing columns for Hamming similarity...\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "for i in tqdm(range(label_encoded_df.shape[1])):\n",
    "    if i in grouped:\n",
    "        continue\n",
    "    col_i = label_encoded_df.iloc[:, i].values\n",
    "    for j in range(i + 1, label_encoded_df.shape[1]):\n",
    "        if j in grouped:\n",
    "            continue\n",
    "        col_j = label_encoded_df.iloc[:, j].values\n",
    "        similarity = np.mean(col_i == col_j)\n",
    "        if similarity >= threshold:\n",
    "            grouped.add(j)\n",
    "            groups[i].append(j)\n",
    "    if len(groups[i]) > 0:\n",
    "        groups[i].insert(0, i)\n",
    "        similar_cols.append(groups[i])\n",
    "\n",
    "# Select representative columns\n",
    "representative_cols = [group[0] for group in similar_cols]\n",
    "all_grouped = set([item for group in similar_cols for item in group])\n",
    "all_features = set(range(label_encoded_df.shape[1]))\n",
    "not_grouped = list(all_features - all_grouped)\n",
    "\n",
    "# Combine representative and ungrouped features\n",
    "final_features_idx = representative_cols + not_grouped\n",
    "reduced_encoded = onehot_df.iloc[:, final_features_idx]  # keep original values for modeling\n",
    "\n",
    "# Output\n",
    "print(f\"\\n Representative columns = {representative_cols}\")\n",
    "print(f\"\\n Not grouped columns = {not_grouped}\")\n",
    "print(f\"\\n Final selected columns = {final_features_idx}\")\n",
    "print(f\"\\n ‚úÖ Reduced features from {onehot_df.shape[1]} to {reduced_encoded.shape[1]}\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(f\"\\n Representative columns = {representative_cols}\")\n",
    "file.writelines(f\"\\n Not grouped columns = {not_grouped}\")\n",
    "file.writelines(f\"\\n Final selected columns = {final_features_idx}\")\n",
    "file.writelines(f\"\\n ‚úÖ Reduced features from {onehot_df.shape[1]} to {reduced_encoded.shape[1]}\")\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd467d67-1dd8-4119-9559-9e30ffa3081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Similar column groups saved to: /storage/home/yqf5148/work/volvoPennState/code/similar_column_groups(80% similarity threshold).csv\n",
      "\n",
      "üî¢ Total Columns: 7908\n",
      "üìä Number of Similarity Groups Found: 92\n",
      "üîÅ Total Grouped Columns: 7766\n",
      "üîì Ungrouped Columns: 142\n",
      "‚úÖ Final Feature Count After Reduction: 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1528460/4028784883.py:56: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_1528460/4028784883.py:58: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(group_bar_plot_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " üìä Group size bar plot saved to: /storage/home/yqf5148/work/volvoPennState/code/similar_column_group_sizes(80% similarity threshold).png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1528460/4028784883.py:72: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_1528460/4028784883.py:74: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(group_hist_plot_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " üìà Group size histogram saved to: /storage/home/yqf5148/work/volvoPennState/code/group_size_histogram(80% similarity threshold).png\n"
     ]
    }
   ],
   "source": [
    "# Log group details to text output\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.write(\"\\n\\nüì¶ Similar Column Groups (Hamming similarity ‚â• 85%):\\n\")\n",
    "for group in similar_cols:\n",
    "    rep = group[0]\n",
    "    file.write(f\"Representative column {rep} represents columns {group}\\n\")\n",
    "file.close()\n",
    "\n",
    "# Save group details to CSV\n",
    "\n",
    "csv_output_path = f\"/storage/home/yqf5148/work/volvoPennState/code/similar_column_groups({int(threshold * 100)}% similarity threshold).csv\"\n",
    "with open(csv_output_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Representative_Column_Index\", \"Group_Column_Indices\"])\n",
    "    for group in similar_cols:\n",
    "        writer.writerow([group[0], group])\n",
    "        \n",
    "print(f\"\\nüìÅ Similar column groups saved to: {csv_output_path}\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(\"\\n üîç Comparing columns for Hamming similarity...\")\n",
    "file.close()\n",
    "\n",
    "# Summary stats\n",
    "total_columns = onehot_df.shape[1]\n",
    "num_groups = len(similar_cols)\n",
    "# grouped_columns = sum(len(g) for g in similar_cols) \n",
    "# grouped_columns = sum([len(g) for g in similar_cols])\n",
    "grouped_columns = builtins.sum([len(g) for g in similar_cols])\n",
    "  \n",
    "\n",
    "ungrouped_columns = len(not_grouped)\n",
    "final_columns = len(final_features_idx)\n",
    "\n",
    "summary_text = (\n",
    "    f\"\\nüî¢ Total Columns: {total_columns}\"\n",
    "    f\"\\nüìä Number of Similarity Groups Found: {num_groups}\"\n",
    "    f\"\\nüîÅ Total Grouped Columns: {grouped_columns}\"\n",
    "    f\"\\nüîì Ungrouped Columns: {ungrouped_columns}\"\n",
    "    f\"\\n‚úÖ Final Feature Count After Reduction: {final_columns}\"\n",
    ")\n",
    "\n",
    "print(summary_text)\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(summary_text)\n",
    "file.close()\n",
    "\n",
    "# Plot: Distribution of group sizes\n",
    "\n",
    "group_sizes = [len(group) for group in similar_cols]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(group_sizes)), group_sizes)\n",
    "plt.xlabel(\"Group Index\")\n",
    "plt.ylabel(\"Number of Columns in Group\")\n",
    "plt.title(\"üìä Similar Column Group Sizes\")\n",
    "plt.tight_layout()\n",
    "group_bar_plot_path = f\"/storage/home/yqf5148/work/volvoPennState/code/similar_column_group_sizes({int(threshold * 100)}% similarity threshold).png\"\n",
    "plt.savefig(group_bar_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n üìä Group size bar plot saved to: {group_bar_plot_path}\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(f\"\\n üìä Group size bar plot saved to: {group_bar_plot_path}\")\n",
    "file.close()\n",
    "\n",
    "# Plot: Histogram of group size frequencies\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(group_sizes, bins=range(1, builtins.max(group_sizes) + 2), edgecolor='black')\n",
    "plt.xlabel(\"Group Size\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"üìà Histogram of Similar Column Group Sizes\")\n",
    "plt.tight_layout()\n",
    "group_hist_plot_path = f\"/storage/home/yqf5148/work/volvoPennState/code/group_size_histogram({int(threshold * 100)}% similarity threshold).png\"\n",
    "plt.savefig(group_hist_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n üìà Group size histogram saved to: {group_hist_plot_path}\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(f\"\\n üìà Group size histogram saved to: {group_hist_plot_path}\")\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fb3612a-3247-4b3f-bb18-b69bfdf64bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " üîÑ Re-applying PCA on reduced features...\n",
      "\n",
      " ‚úÖ PCA complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n üîÑ Re-applying PCA on reduced features...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(\"\\n üîÑ Re-applying PCA on reduced features...\")\n",
    "file.close()\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "pca_applied = pca.fit_transform(reduced_encoded)\n",
    "\n",
    "print(\"\\n ‚úÖ PCA complete.\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(\"\\n ‚úÖ PCA complete.\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ba6e7ec-ef07-4995-8367-7b9bee902677",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_columns = [f\"PCA_{i+1}\" for i in range(pca_applied.shape[1])]\n",
    "df_pca_applied = pd.DataFrame(pca_applied, columns=pca_columns, index=df_selected_for_pca_pd.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b85c2911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ‚úÖ Final dataset shape after concatenation: (322178, 136)\n"
     ]
    }
   ],
   "source": [
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(\"\\n üîÑ Converting non-selected Spark DataFrame to Pandas...\")\n",
    "file.close()\n",
    "\n",
    "df_non_selected_for_pca_pd = df_non_selected_for_pca.toPandas()\n",
    "df_final = pd.concat([df_non_selected_for_pca_pd.reset_index(drop=True), df_pca_applied], axis=1)\n",
    "\n",
    "print(f\"\\n ‚úÖ Final dataset shape after concatenation: {df_final.shape}\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(f\"\\n ‚úÖ Final dataset shape after concatenation: {df_final.shape}\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67b162cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ‚úÖ Final dataset saved to: /storage/home/yqf5148/work/volvoPennState/data/dataset/final_features_with_pca(80% similarity threshold).csv\n"
     ]
    }
   ],
   "source": [
    "output_path = f\"/storage/home/yqf5148/work/volvoPennState/data/dataset/final_features_with_pca({int(threshold * 100)}% similarity threshold).csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n ‚úÖ Final dataset saved to: {output_path}\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput({int(threshold * 100)}% similarity threshold).txt\", \"a\")\n",
    "file.writelines(f\"\\n ‚úÖ Final dataset saved to: {output_path}\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ab775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volvopennstate_new_kernel",
   "language": "python",
   "name": "volvopennstate_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
