{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d024bf3-3c33-4060-b854-07c3fd672b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
      "env: PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin\n"
     ]
    }
   ],
   "source": [
    "%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
    "%env PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7201403-8b2e-4405-83fc-082bde805089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ps: /storage/icds/RISE/sw8/anaconda/anaconda3/envs/tensorflow/lib/libuuid.so.1: no version information available (required by /usr/lib64/libblkid.so.1)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/14 12:12:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/14 12:12:22 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyspark as psk\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import time as t \n",
    "from datetime import date, datetime, timedelta\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import multiprocessing\n",
    "from functools import reduce  \n",
    "from math import modf\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from delta import * \n",
    "from delta.tables import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "# import covalent as ct\n",
    "\n",
    "import subprocess\n",
    "import re\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# import umap.umap_ as umap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc  # import gorbage collector to resolve the problem of restarting kernel due to large table of population loading in RAM to append\n",
    "from pyspark.sql.functions import year, col\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# os.environ['PYDEVD_DISABLE_FILE_VALIDATION']=1\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "       .config(\"spark.ui.port\", \"4050\") \\\n",
    "       .master(\"local[2]\") \\\n",
    "       .appName(\"MyApp\") \\\n",
    "       .config(\"spark.driver.maxResultSize\", \"40g\")\\\n",
    "       .config(\"spark.driver.memory\", \"140g\")\\\n",
    "       .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "#both works\n",
    "# 1: \n",
    "# spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")    #To resolve the error for p1075_38 to_timestamp formating: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '1/2/2019 20:40:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n",
    "# Set Spark SQL legacy time parser policy to LEGACY to handle older date formats\n",
    "# 2:\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "# Increase the max fields in the string representation of a plan\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7747d09-4948-42bd-8374-4458d5761075",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data(problem).csv\") \\\n",
    "          .createOrReplaceTempView(\"VINs_data\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/PopulationWithChassisId.csv\") \\\n",
    "          .createOrReplaceTempView(\"population\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0af764d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all column names from the population table\n",
    "all_columns = spark.table(\"population\").columns\n",
    "\n",
    "# Select only columns that contain \"KOLA\" and exclude \"CHASSIS_ID\"\n",
    "kola_columns = [c for c in all_columns if \"KOLA\" in c and c != \"CHASSIS_ID\"]\n",
    "\n",
    "# Build the main DataFrame with selected columns\n",
    "df_population = spark.table(\"population\") \\\n",
    "    .select(\n",
    "        col(\"VIN\"),       \n",
    "        col(\"ENGINE_SIZE\"),\n",
    "        col(\"ENGINE_HP\"),\n",
    "        col(\"VEH_TYPE\"),\n",
    "        year(\"VEH_ASSEMB_DATE\").alias(\"VEH_ASSEMB_YEAR\"),\n",
    "        \n",
    "        *[col(c) for c in kola_columns]           # Include all KOLA-related columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff3c49-e256-4cec-ba5b-c32bc4510f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "selected_for_pca_cols = [c for c in df_population.columns if \"VIN\" not in c]\n",
    "not_selected_for_pca_cols = [c for c in df_population.columns if c not in selected_for_pca_cols]\n",
    "\n",
    "# print(non_kola_cols)\n",
    "df_non_selected_for_pca = df_population.select(*not_selected_for_pca_cols)\n",
    "df_selected_for_pca = df_population.select(*selected_for_pca_cols)\n",
    "\n",
    "df_selected_for_pca_pd = df_selected_for_pca.toPandas()  # convert from PySpark to Pandas\n",
    "print(f\"\\n The toPandas() finished!\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(f\"\\n The toPandas() finished!\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5199be3-b17a-4b2b-9549-539499525636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ðŸ”„ Converting to categorical data...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(\"\\n ðŸ”„ Converting to categorical data...\")\n",
    "file.close()\n",
    "\n",
    "df_selected_for_pca_pd = df_selected_for_pca_pd.astype('category')\n",
    "\n",
    "print(f\"\\n The convert to category finished!\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(f\"\\n The convert to category finished!\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919316b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ðŸš€ Starting OneHotEncoding transformation...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(\"\\n ðŸš€ Starting OneHotEncoding transformation...\")\n",
    "file.close()\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "selected_for_pca_encoded = encoder.fit_transform(df_selected_for_pca_pd)\n",
    "\n",
    "print(f\"\\n The OneHotEncoding finished!\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(f\"\\n The OneHotEncoding finished!\")\n",
    "file.close()\n",
    "\n",
    "onehot_df = pd.DataFrame(selected_for_pca_encoded, columns=encoder.get_feature_names_out())\n",
    "print(f\"\\n âœ… onehot_df dataset shape: {onehot_df.shape}\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(f\"\\n âœ… onehot_df dataset shape: {onehot_df.shape}\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e6ebe-b51f-459f-b0fe-53475b3484a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# This script reduces the dimensionality of a high-dimensional \n",
    "# one-hot encoded dataset by identifying and removing columns \n",
    "# that exhibit similar **value change patterns** across rows.\n",
    "#\n",
    "# Motivation:\n",
    "# In one-hot encoded data, many columns may carry redundant \n",
    "# information â€” even if their exact values differ â€” because they \n",
    "# follow similar change patterns across data rows. Comparing \n",
    "# columns directly (using `np.all`) is too strict, as it only \n",
    "# finds exact duplicates. Instead, we compare the **value \n",
    "# patterns** using Hamming similarity.\n",
    "#\n",
    "# Step-by-step Overview:\n",
    "# 1. Each column in the one-hot encoded DataFrame is label-encoded \n",
    "#    independently. This transforms categorical strings or binary \n",
    "#    encodings into integer codes that still preserve the pattern \n",
    "#    of value changes (e.g., A â†’ B â†’ A becomes 0 â†’ 1 â†’ 0).\n",
    "#\n",
    "# 2. Pairwise comparison of all columns is done using Hamming \n",
    "#    similarity â€” calculated as the proportion of matching \n",
    "#    positions in the encoded column vectors.\n",
    "#\n",
    "# 3. If two columns have â‰¥ 85% similarity in their encoded \n",
    "#    patterns, they are grouped together and considered redundant.\n",
    "#\n",
    "# 4. From each group of similar columns, only the first (as a \n",
    "#    representative) is retained, and the others are dropped.\n",
    "#\n",
    "# 5. All non-grouped (unique) columns are also retained.\n",
    "#\n",
    "# Final Output:\n",
    "# A reduced feature set with one representative column per \n",
    "# similarity group plus all ungrouped columns. This allows for \n",
    "# significant dimensionality reduction while preserving \n",
    "# meaningful variation across the dataset.\n",
    "# ==============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1536bc4-c83f-4167-a8da-cafc2af82105",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ðŸ” Finding highly correlated feature columns...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(\"\\n ðŸ” Finding highly correlated feature columns...\")\n",
    "file.close()\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label encode each column individually\n",
    "print(\"\\nðŸ”„ Label encoding all columns in onehot_df...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(\"\\nðŸ”„ Label encoding all columns in onehot_df...\")\n",
    "file.close()\n",
    "\n",
    "label_encoded_df = onehot_df.copy()\n",
    "for col in label_encoded_df.columns:\n",
    "    encoder = LabelEncoder()\n",
    "    label_encoded_df[col] = encoder.fit_transform(label_encoded_df[col])\n",
    "\n",
    "# Initialize structures\n",
    "similar_cols = []\n",
    "grouped = set()\n",
    "groups = defaultdict(list)\n",
    "\n",
    "threshold = 0.90  # 85% similarity threshold\n",
    "\n",
    "print(\"\\n ðŸ” Comparing columns for Hamming similarity...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(\"\\n ðŸ” Comparing columns for Hamming similarity...\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "for i in tqdm(range(label_encoded_df.shape[1])):\n",
    "    if i in grouped:\n",
    "        continue\n",
    "    col_i = label_encoded_df.iloc[:, i].values\n",
    "    for j in range(i + 1, label_encoded_df.shape[1]):\n",
    "        if j in grouped:\n",
    "            continue\n",
    "        col_j = label_encoded_df.iloc[:, j].values\n",
    "        similarity = np.mean(col_i == col_j)\n",
    "        if similarity >= threshold:\n",
    "            grouped.add(j)\n",
    "            groups[i].append(j)\n",
    "    if len(groups[i]) > 0:\n",
    "        groups[i].insert(0, i)\n",
    "        similar_cols.append(groups[i])\n",
    "\n",
    "# Select representative columns\n",
    "representative_cols = [group[0] for group in similar_cols]\n",
    "all_grouped = set([item for group in similar_cols for item in group])\n",
    "all_features = set(range(label_encoded_df.shape[1]))\n",
    "not_grouped = list(all_features - all_grouped)\n",
    "\n",
    "# Combine representative and ungrouped features\n",
    "final_features_idx = representative_cols + not_grouped\n",
    "reduced_encoded = onehot_df.iloc[:, final_features_idx]  # keep original values for modeling\n",
    "\n",
    "# Output\n",
    "print(f\"\\n Representative columns = {representative_cols}\")\n",
    "print(f\"\\n Not grouped columns = {not_grouped}\")\n",
    "print(f\"\\n Final selected columns = {final_features_idx}\")\n",
    "print(f\"\\n âœ… Reduced features from {onehot_df.shape[1]} to {reduced_encoded.shape[1]}\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(f\"\\n Representative columns = {representative_cols}\")\n",
    "file.writelines(f\"\\n Not grouped columns = {not_grouped}\")\n",
    "file.writelines(f\"\\n Final selected columns = {final_features_idx}\")\n",
    "file.writelines(f\"\\n âœ… Reduced features from {onehot_df.shape[1]} to {reduced_encoded.shape[1]}\")\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3612a-3247-4b3f-bb18-b69bfdf64bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ðŸ”„ Re-applying PCA on reduced features...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(\"\\n ðŸ”„ Re-applying PCA on reduced features...\")\n",
    "file.close()\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "pca_applied = pca.fit_transform(reduced_encoded)\n",
    "\n",
    "print(\"\\n âœ… PCA complete.\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(\"\\n âœ… PCA complete.\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6e7ec-ef07-4995-8367-7b9bee902677",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_columns = [f\"PCA_{i+1}\" for i in range(pca_applied.shape[1])]\n",
    "df_pca_applied = pd.DataFrame(pca_applied, columns=pca_columns, index=df_selected_for_pca_pd.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n ðŸ”„ Converting non-selected Spark DataFrame to Pandas...\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(\"\\n ðŸ”„ Converting non-selected Spark DataFrame to Pandas...\")\n",
    "file.close()\n",
    "\n",
    "df_non_selected_for_pca_pd = df_non_selected_for_pca.toPandas()\n",
    "df_final = pd.concat([df_non_selected_for_pca_pd.reset_index(drop=True), df_pca_applied], axis=1)\n",
    "\n",
    "print(f\"\\n âœ… Final dataset shape after concatenation: {df_final.shape}\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(f\"\\n âœ… Final dataset shape after concatenation: {df_final.shape}\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b162cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/storage/home/yqf5148/work/volvoPennState/data/dataset/final_features_with_pca.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n âœ… Final dataset saved to: {output_path}\")\n",
    "file = open(f\"/storage/home/yqf5148/work/volvoPennState/code/dimentionalityReductionOutput.txt\", \"a\")\n",
    "file.writelines(f\"\\n âœ… Final dataset saved to: {output_path}\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ab775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volvopennstate_new_kernel",
   "language": "python",
   "name": "volvopennstate_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
