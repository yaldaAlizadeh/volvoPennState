{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d024bf3-3c33-4060-b854-07c3fd672b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
      "env: PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin\n"
     ]
    }
   ],
   "source": [
    "%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
    "%env PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7201403-8b2e-4405-83fc-082bde805089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ps: /storage/icds/RISE/sw8/anaconda/anaconda3/envs/tensorflow/lib/libuuid.so.1: no version information available (required by /usr/lib64/libblkid.so.1)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/09 00:21:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/09 00:21:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/09 00:21:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/04/09 00:21:04 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark as psk\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import time as t \n",
    "from datetime import date, datetime, timedelta\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import multiprocessing\n",
    "from functools import reduce  \n",
    "from math import modf\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from delta import * \n",
    "from delta.tables import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "# import covalent as ct\n",
    "\n",
    "import subprocess\n",
    "import re\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "# import umap.umap_ as umap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# os.environ['PYDEVD_DISABLE_FILE_VALIDATION']=1\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "       .master(\"local[2]\") \\\n",
    "       .appName(\"MyApp\") \\\n",
    "       .config(\"spark.driver.maxResultSize\", \"20g\")\\\n",
    "       .config(\"spark.driver.memory\", \"100g\")\\\n",
    "       .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "#both works\n",
    "# 1: \n",
    "# spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")    #To resolve the error for p1075_38 to_timestamp formating: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '1/2/2019 20:40:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n",
    "# Set Spark SQL legacy time parser policy to LEGACY to handle older date formats\n",
    "# 2:\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "# Increase the max fields in the string representation of a plan\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7747d09-4948-42bd-8374-4458d5761075",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data.csv\") \\\n",
    "          .createOrReplaceTempView(\"VINs_data\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/PopulationWithChassisId.csv\") \\\n",
    "          .createOrReplaceTempView(\"population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f979ef-4646-4baf-a780-91b0f14c1337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_all_pop = spark.sql('select * from population')\n",
    "df_all_pop_p = df_all_pop.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48230f6b-740b-4c0c-8acc-f2a8d1adf9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 column names:\n",
      "VIN\n",
      "CHASSIS_ID\n",
      "ENGINE_ASSEMB_DATE\n",
      "ENGINE_ASSEMB_MONTH\n",
      "VEH_ASSEMB_DATE\n",
      "VEH_ASSEMB_MONTH\n",
      "INS_DATE\n",
      "INS_MONTH\n",
      "ENGINE_SIZE\n",
      "ENGINE_HP\n",
      "VEH_TYPE\n"
     ]
    }
   ],
   "source": [
    "# Get all column names\n",
    "all_columns = df_all_pop.columns\n",
    "\n",
    "# Print the first 10\n",
    "print(\"First 10 column names:\")\n",
    "for col in all_columns[:11]:\n",
    "    print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcd78ce1-02a6-409b-a2e5-e3bb1eaec518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all column names\n",
    "all_columns = df_all_pop.columns\n",
    "\n",
    "vin_column = [all_columns[0]]\n",
    "\n",
    "# Include column 2 (index 1)\n",
    "extra_column = [all_columns[2]]\n",
    "\n",
    "# Pick columns from index 8 to the end\n",
    "main_columns = all_columns[8:]\n",
    "\n",
    "# Combine them\n",
    "selected_columns = vin_column + extra_column + main_columns\n",
    "\n",
    "# # Print how many columns are selected\n",
    "# print(f\"Number of selected columns: {len(selected_columns)}\")\n",
    "\n",
    "# # Print their names\n",
    "# print(\"Selected column names:\")\n",
    "# for col in selected_columns:\n",
    "#     print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11325673-12be-4d43-90f3-b58dba3595f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make sure you only pass PCA-reduced KOLA feature columns\n",
    "# pca_columns = [col for col in df_final.columns if col.startswith(\"KOLA_PCA_\")]\n",
    "# df_kmeans_result = apply_kmeans_clustering(df_final[pca_columns], n_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce743651-29dc-40bf-9937-4aeafb4a9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pca_component_range(df, categorical_columns, non_kola_columns, k=10):\n",
    "    print(\" Step 1: Dropping NA rows...\")\n",
    "    df_filtered = df[non_kola_columns + categorical_columns].dropna()\n",
    "    print(f\"    - Filtered shape: {df_filtered.shape}\")\n",
    "\n",
    "    print(\" Step 2: One-hot encoding KOLA features...\")\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    kola_encoded = encoder.fit_transform(df_filtered[categorical_columns])\n",
    "    print(f\"    - Encoded shape: {kola_encoded.shape}\")\n",
    "\n",
    "    silhouette_scores = []\n",
    "    n_components = 6\n",
    "    for n_components in range(6, 16):\n",
    "        print(f\"\\n Testing PCA with {n_components} components...\")\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca_result = pca.fit_transform(kola_encoded)\n",
    "\n",
    "        print(\"    - Clustering with KMeans...\")\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(pca_result)\n",
    "\n",
    "        score = silhouette_score(pca_result, labels)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"    - Silhouette Score: {score:.4f}\")\n",
    "\n",
    "    print(\"\\n Plotting silhouette scores...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x_vals = list(range(6, 16))\n",
    "\n",
    "    plt.plot(x_vals, silhouette_scores, marker='o')\n",
    "    plt.title(\"Silhouette Score vs. Number of PCA Components\")\n",
    "    plt.xlabel(\"Number of PCA Components\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Annotate each point with its score\n",
    "    for x, y in zip(x_vals, silhouette_scores):\n",
    "        plt.text(x, y + 0.01, f\"{y:.3f}\", ha='center', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure as a PDF\n",
    "    plt.savefig(\"silhouette_score_vs_pca_components.pdf\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9381b1-a490-4d49-9c39-a4970b092fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1: Dropping NA rows...\n",
      "    - Filtered shape: (321746, 593)\n",
      " Step 2: One-hot encoding KOLA features...\n",
      "    - Encoded shape: (321746, 7845)\n",
      "\n",
      " Testing PCA with 6 components...\n",
      "    - Clustering with KMeans...\n",
      "    - Silhouette Score: 0.5588\n",
      "\n",
      " Testing PCA with 7 components...\n",
      "    - Clustering with KMeans...\n",
      "    - Silhouette Score: 0.5431\n",
      "\n",
      " Testing PCA with 8 components...\n",
      "    - Clustering with KMeans...\n"
     ]
    }
   ],
   "source": [
    "vin_column = \"VIN\"\n",
    "df = df_all_pop_p.copy()\n",
    "\n",
    "# Assuming df is your full dataset and selected_columns is already defined\n",
    "kola_columns = [col for col in selected_columns if \"KOLA\" in col]\n",
    "non_kola_columns = [col for col in selected_columns if col not in kola_columns or col == vin_column]\n",
    "\n",
    "evaluate_pca_component_range(df, categorical_columns=kola_columns, non_kola_columns=non_kola_columns, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75840a3f-8d69-4c36-bdb7-0e74f382f58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volvopennstate_new_kernel",
   "language": "python",
   "name": "volvopennstate_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
