{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d024bf3-3c33-4060-b854-07c3fd672b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
      "env: PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin\n"
     ]
    }
   ],
   "source": [
    "%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64\n",
    "%env PATH=/storage/home/yqf5148/work/anaconda3/envs/volvopennstate-env/bin:storage/icds/swst/deployed/production/20220813/apps/anaconda3/2021.05_gcc-8.5.0/bin:/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin:/storage/icds/tools/bin:/storage/sys/slurm/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7201403-8b2e-4405-83fc-082bde805089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ps: /storage/icds/RISE/sw8/anaconda/anaconda3/envs/tensorflow/lib/libuuid.so.1: no version information available (required by /usr/lib64/libblkid.so.1)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/23 22:39:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark as psk\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import time as t \n",
    "from datetime import date, datetime, timedelta\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import multiprocessing\n",
    "from functools import reduce  \n",
    "from math import modf\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from delta import * \n",
    "from delta.tables import *\n",
    "from delta import configure_spark_with_delta_pip\n",
    "# import covalent as ct\n",
    "\n",
    "import subprocess\n",
    "import re\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "# import umap.umap_ as umap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc  # import gorbage collector to resolve the problem of restarting kernel due to large table of population loading in RAM to append\n",
    "from pyspark.sql.functions import year, col\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# os.environ['PYDEVD_DISABLE_FILE_VALIDATION']=1\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "       .config(\"spark.ui.port\", \"4050\") \\\n",
    "       .master(\"local[2]\") \\\n",
    "       .appName(\"MyApp\") \\\n",
    "       .config(\"spark.driver.maxResultSize\", \"40g\")\\\n",
    "       .config(\"spark.driver.memory\", \"140g\")\\\n",
    "       .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "\n",
    "#both works\n",
    "# 1: \n",
    "# spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")    #To resolve the error for p1075_38 to_timestamp formating: You may get a different result due to the upgrading to Spark >= 3.0: Fail to parse '1/2/2019 20:40:00' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n",
    "# Set Spark SQL legacy time parser policy to LEGACY to handle older date formats\n",
    "# 2:\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "# Increase the max fields in the string representation of a plan\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7747d09-4948-42bd-8374-4458d5761075",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/data/dataset/VINs_data(problem).csv\") \\\n",
    "          .createOrReplaceTempView(\"VINs_data\")\n",
    "\n",
    "spark.read.option(\"header\",True) \\\n",
    "          .csv(\"/storage/home/yqf5148/work/volvoPennState/PopulationWithChassisId.csv\") \\\n",
    "          .createOrReplaceTempView(\"population\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0af764d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all column names from the population table\n",
    "all_columns = spark.table(\"population\").columns\n",
    "\n",
    "# Select only columns that contain \"KOLA\" and exclude \"CHASSIS_ID\"\n",
    "kola_columns = [c for c in all_columns if \"KOLA\" in c and c != \"CHASSIS_ID\"]\n",
    "\n",
    "# Build the main DataFrame with selected columns\n",
    "df_population = spark.table(\"population\") \\\n",
    "    .select(\n",
    "        col(\"VIN\"),                               # Include VIN\n",
    "        col(\"ENGINE_SIZE\"),                       # Include ENGINE_SIZE\n",
    "        col(\"ENGINE_HP\"),                         # Include ENGINE_HP\n",
    "        col(\"VEH_TYPE\"),                          # Include VEH_TYPE\n",
    "        year(\"VEH_ASSEMB_DATE\").alias(\"VEH_ASSEMB_YEAR\"),  # Extract year from VEH_ASSEMB_DATE\n",
    "        *[col(c) for c in kola_columns]           # Include all KOLA-related columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ff3c49-e256-4cec-ba5b-c32bc4510f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The toPandas() finished!\n"
     ]
    }
   ],
   "source": [
    "kola_cols = [c for c in df_population.columns if \"KOLA\" in c]\n",
    "non_kola_cols = [c for c in df_population.columns if c not in kola_cols]\n",
    "\n",
    "# print(non_kola_cols)\n",
    "df_non_kola = df_population.select(*non_kola_cols)\n",
    "df_kola = df_population.select(*kola_cols)\n",
    "\n",
    "df_kola_pd = df_kola.toPandas()  # convert from PySpark to Pandas\n",
    "print(f\"The toPandas() finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5199be3-b17a-4b2b-9549-539499525636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The convert to category finished!\n"
     ]
    }
   ],
   "source": [
    "df_kola_pd = df_kola_pd.astype('category')\n",
    "print(f\"The convert to category finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "919316b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OneHotEncoding finished!\n",
      "The PCA application on kola columns finished!\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "kola_encoded = encoder.fit_transform(df_kola_pd)\n",
    "print(f\"The OneHotEncoding finished!\")\n",
    "\n",
    "pca = PCA(n_components=0.95)\n",
    "kola_pca = pca.fit_transform(kola_encoded)\n",
    "print(f\"The PCA application on kola columns finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba6e7ec-ef07-4995-8367-7b9bee902677",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_columns = [f\"KOLA_PCA_{i+1}\" for i in range(kola_pca.shape[1])]\n",
    "df_kola_pca = pd.DataFrame(kola_pca, columns=pca_columns, index=df_kola_pd.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b85c2911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_non_kola_pd = df_non_kola.toPandas()\n",
    "df_final = pd.concat([df_non_kola_pd.reset_index(drop=True), df_kola_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67b162cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"/storage/home/yqf5148/work/volvoPennState/data/dataset/final_features_with_pca.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ab775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volvopennstate_new_kernel",
   "language": "python",
   "name": "volvopennstate_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
